# External GPT5-Pro Audit Snapshot
Generated at 2025-10-18T16:41:38.854474+00:00

## Repository Layout
./
  .agent/
    .agent/logs/
      .agent/logs/events.jsonl
      .agent/logs/monitor.port
  .codex_ci/
    .codex_ci/component_plan_hello_cli.json
    .codex_ci/events.jsonl
    .codex_ci/generator_console_hello_cli.log
    .codex_ci/generator_critic_prompt.txt
    .codex_ci/generator_critic_response.log
    .codex_ci/generator_patch.diff
    .codex_ci/generator_prompt.txt
    .codex_ci/generator_response.log
    .codex_ci/generator_tests.log
    .codex_ci/rex.lock
    .codex_ci/rex_generator.lock
  .git/ (contents omitted)
  .venv/ (gitignored; contents omitted)
  bin/
    bin/fake-codex
    bin/rex-codex
  documents/
    documents/assumption_ledgers/
      documents/assumption_ledgers/README.md
    documents/feature_cards/
      documents/feature_cards/hello_cli.md
    documents/design_review.md
  for_external_GPT5_pro_audit/
    for_external_GPT5_pro_audit/audit_20251015214523.md
    for_external_GPT5_pro_audit/audit_20251015220825.md
    for_external_GPT5_pro_audit/audit_20251016045436.md
    for_external_GPT5_pro_audit/audit_20251016051354.md
    for_external_GPT5_pro_audit/audit_20251016052416.md
    for_external_GPT5_pro_audit/audit_20251016181645.md
    for_external_GPT5_pro_audit/audit_20251016190447.md
    for_external_GPT5_pro_audit/audit_20251016203920.md
    for_external_GPT5_pro_audit/audit_20251016205948.md
    for_external_GPT5_pro_audit/audit_20251016213839.md
    for_external_GPT5_pro_audit/audit_20251016215502.md
    for_external_GPT5_pro_audit/audit_20251016231731.md
    for_external_GPT5_pro_audit/audit_20251016T181537Z.md
    for_external_GPT5_pro_audit/audit_20251017010320.md
    for_external_GPT5_pro_audit/audit_20251017014650.md
    for_external_GPT5_pro_audit/audit_20251017014831.md
    for_external_GPT5_pro_audit/audit_20251017015608.md
    for_external_GPT5_pro_audit/audit_20251017025135.md
    for_external_GPT5_pro_audit/audit_20251017042600.md
    for_external_GPT5_pro_audit/audit_20251017053839.md
    for_external_GPT5_pro_audit/audit_20251017060718.md
    for_external_GPT5_pro_audit/audit_20251017153730.md
    for_external_GPT5_pro_audit/audit_20251017160841.md
    for_external_GPT5_pro_audit/audit_20251017162954.md
    for_external_GPT5_pro_audit/audit_20251017173142.md
    ... (15 more entries)
  monitor/
    monitor/.agent/
      monitor/.agent/logs/ (depth limit)
    monitor/.codex_ci/
    monitor/agent/
      monitor/agent/launch-monitor.js
      monitor/agent/logger-node.js
      monitor/agent/logger-python.py
    monitor/node_modules/
      monitor/node_modules/.bin/ (depth limit)
      monitor/node_modules/accepts/ (depth limit)
      monitor/node_modules/array-flatten/ (depth limit)
      monitor/node_modules/body-parser/ (depth limit)
      monitor/node_modules/bytes/ (depth limit)
      monitor/node_modules/call-bind-apply-helpers/ (depth limit)
      monitor/node_modules/call-bound/ (depth limit)
      monitor/node_modules/compressible/ (depth limit)
      monitor/node_modules/compression/ (depth limit)
      monitor/node_modules/content-disposition/ (depth limit)
      monitor/node_modules/content-type/ (depth limit)
      monitor/node_modules/cookie/ (depth limit)
      monitor/node_modules/cookie-signature/ (depth limit)
      monitor/node_modules/cors/ (depth limit)
      monitor/node_modules/debug/ (depth limit)
      monitor/node_modules/depd/ (depth limit)
      monitor/node_modules/destroy/ (depth limit)
      monitor/node_modules/dunder-proto/ (depth limit)
      monitor/node_modules/ee-first/ (depth limit)
      monitor/node_modules/encodeurl/ (depth limit)
      monitor/node_modules/es-define-property/ (depth limit)
      monitor/node_modules/es-errors/ (depth limit)
      monitor/node_modules/es-object-atoms/ (depth limit)
      monitor/node_modules/escape-html/ (depth limit)
      monitor/node_modules/etag/ (depth limit)
      ... (49 more entries)
    monitor/public/
      monitor/public/app.js
      monitor/public/index.html
      monitor/public/style.css
    monitor/package-lock.json
    monitor/package.json
    monitor/server.js
  packaging/
    packaging/install.sh
    packaging/uninstall.sh
  project_runtime/
    project_runtime/hooks/
      project_runtime/hooks/README.md
    project_runtime/__init__.py
    project_runtime/agent.lock.schema.json
    project_runtime/bootstrap.py
    project_runtime/README.md
  scripts/
    scripts/install.sh
    scripts/selftest_loop.sh
    scripts/smoke_e2e.sh
    scripts/start_hud_popout.sh
  src/
    src/rex_codex/
      src/rex_codex/scope_global/ (depth limit)
      src/rex_codex/scope_project/ (depth limit)
      src/rex_codex/scope_sandbox/ (depth limit)
      src/rex_codex/__init__.py
      src/rex_codex/__main__.py
      src/rex_codex/_compat.py
      src/rex_codex/burn.py
      src/rex_codex/cards.py
      src/rex_codex/cli.py
      src/rex_codex/component_planner.py
      src/rex_codex/config.py
      src/rex_codex/discriminator.py
      src/rex_codex/doctor.py
      src/rex_codex/events.py
      src/rex_codex/generator.py
      src/rex_codex/generator_ui.py
      src/rex_codex/hermetic.py
      src/rex_codex/hud.py
      src/rex_codex/init.py
      src/rex_codex/install.py
      src/rex_codex/logs.py
      src/rex_codex/loop.py
      src/rex_codex/monitoring.py
      src/rex_codex/playbook.py
      src/rex_codex/self_update.py
      ... (3 more entries)
  templates/
    templates/documents/
      templates/documents/assumption_ledgers/ (depth limit)
      templates/documents/feature_cards/ (depth limit)
    templates/tests/
      templates/tests/enforcement/ (depth limit)
    templates/.flake8
    templates/AGENTS.local.md
    templates/AGENTS.md
    templates/conftest.py
    templates/mypy.ini
    templates/pyproject.toml
    templates/pytest.ini
    templates/requirements-dev.txt
  tests/
    tests/e2e/
      tests/e2e/__init__.py
      tests/e2e/test_cli_install.py
      tests/e2e/test_utils_audit.py
    tests/enforcement/
    tests/feature_specs/
    tests/fixtures/
      tests/fixtures/README.md
    tests/unit/
      tests/unit/__init__.py
      tests/unit/test_cards.py
      tests/unit/test_discriminator_parallel.py
      tests/unit/test_events.py
      tests/unit/test_generator_guard.py
      tests/unit/test_generator_spec_trace.py
      tests/unit/test_generator_ui.py
      tests/unit/test_hud.py
      tests/unit/test_init_requirements.py
      tests/unit/test_install_pruning.py
      tests/unit/test_playbook.py
    tests/utils/
      tests/utils/__init__.py
    tests/conftest.py
  tui/
    tui/dist/ (gitignored; contents omitted)
    tui/examples/
      tui/examples/sample_events.ndjson
    tui/node_modules/ (gitignored; contents omitted)
    tui/src/
      tui/src/App.tsx
      tui/src/index.tsx
      tui/src/model.ts
      tui/src/types.ts
    tui/package-lock.json
    tui/package.json
    tui/README.md
    tui/tsconfig.json
  .codex_ci_latest.log
  .gitignore
  AGENTS.md
  pytest.ini
  README.md
  requirements.txt
  rex-agent.json
  VERSION

## File Snapshots

=== /media/skynet3/8tb_a1/rex_codex_agent/.codex_ci/generator_console_hello_cli.log ===

Generator Dashboard
--------------------------------------------------------------
Feature: hello_cli (Hello CLI)
Status: proposed
Summary: Provide a simple command-line greeting that demonstrates the generator HUD.
Acceptance Criteria:
  - Run with default arguments and print `Hello World`.
  - Accept `--message` to override the greeting text.
  - Support `--quiet` to suppress output entirely.
Existing specs: (none yet)
Focus: default coverage guidance
Pass budget: 1 (continuous=False)
--------------------------------------------------------------
[generator] Iteration 1/1 (slug: hello_cli, status: proposed)
[generator] Calling Codex CLI…
[generator] Codex CLI running (pass 1/1)… 5s elapsed
[generator] Codex CLI running (pass 1/1)… 10s elapsed
[generator] Codex: new file mode 100644
[generator] Codex CLI finished in 12s.
[generator] Codex response saved to .codex_ci/generator_response.log
[generator] Applying diff from .codex_ci/generator_patch.diff:
diff --git a/tests/feature_specs/hello_cli/conftest.py b/tests/feature_specs/hello_cli/conftest.py
new file mode 100644
--- /dev/null
+++ b/tests/feature_specs/hello_cli/conftest.py
@@ -0,0 +1,63 @@
+from __future__ import annotations
+
+import importlib.util
+import os
+import runpy
+import sys
+from pathlib import Path
+
+import pytest
+
+
+def _project_root() -> Path:
+    env_root = os.environ.get("ROOT")
+    if env_root:
+        candidate = Path(env_root).resolve()
+        if (candidate / "src").exists():
+            return candidate
+    here = Path(__file__).resolve()
+    for parent in (here,) + tuple(here.parents):
+        if (parent / "src").exists():
+            return parent
+    return Path.cwd().resolve()
+
+
+def _import_from_src(pkg: str):
+    root = _project_root()
+    module_file = root / "src" / pkg / "__init__.py"
+    if not module_file.exists():
+        raise FileNotFoundError(f"{module_file} not found")
+    spec = importlib.util.spec_from_file_location(pkg, str(module_file))
+    module = importlib.util.module_from_spec(spec)
+    assert spec and spec.loader  # type: ignore[truthy-bool]
+    spec.loader.exec_module(module)  # type: ignore[attr-defined]
+    return module
+
+
+@pytest.fixture(scope="session")
+def hello():
+    return _import_from_src("hello")
+
+
+@pytest.fixture
+def run_app():
+    root = _project_root()
+
+    def _run(*args: str) -> None:
+        argv = ["hello", *args]
+        original_argv = sys.argv[:]
+        original_path = list(sys.path)
+        source_path = root / "src"
+        if str(source_path) not in sys.path:
+            sys.path.insert(0, str(source_path))
+        sys.argv = argv
+        try:
+            runpy.run_module("hello", run_name="__main__")
+        except SystemExit as exc:  # mimic CLI invocation
+            if exc.code not in (0, None):
+                raise
+        finally:
+            sys.argv = original_argv
+            sys.path[:] = original_path
+
+    return _run

diff --git a/tests/feature_specs/hello_cli/test_cli.py b/tests/feature_specs/hello_cli/test_cli.py
new file mode 100644
--- /dev/null
+++ b/tests/feature_specs/hello_cli/test_cli.py
@@ -0,0 +1,41 @@
+from __future__ import annotations
+
+import pytest
+
+
+def test_default_greeting(run_app, capsys):
+    '''AC#1 Run with default arguments and print `Hello World`.'''
+
+    run_app()
+    captured = capsys.readouterr()
+    assert captured.out == "Hello World\n"
+    assert captured.err == ""
+
+
+def test_message_override(run_app, capsys):
+    '''AC#2 Accept `--message` to override the greeting text.'''
+
+    run_app("--message", "Hi there")
+    captured = capsys.readouterr()
+    assert captured.out == "Hi there\n"
+    assert captured.err == ""
+
+
+@pytest.mark.parametrize("repeat", [2, 3])
+def test_repeat_behavior(run_app, capsys, repeat):
+    '''AC#2 Accept `--repeat` to control repetition.'''
+
+    run_app("--repeat", str(repeat))
+    captured = capsys.readouterr()
+    expected = ("Hello World\n" * repeat)
+    assert captured.out == expected
+    assert captured.err == ""
+
+
+def test_quiet_mode_suppresses_output(run_app, capsys):
+    '''AC#3 Support `--quiet` to suppress output entirely.'''
+
+    run_app("--message", "Muted", "--quiet")
+    captured = capsys.readouterr()
+    assert captured.out == ""
+    assert captured.err == ""
Diff summary: 2 files, +104 / -0 lines
  • tests/feature_specs/hello_cli/conftest.py (new, +63)
  • tests/feature_specs/hello_cli/test_cli.py (new, +41)
      added tests: test_default_greeting, test_message_override, test_quiet_mode_suppresses_output, test_repeat_behavior
[generator] git apply --index failed; retrying without --index
[generator] Diff applied successfully.
Spec Trace coverage:
  [AC#1] Run with default arguments and print `Hello World`.
      -> tests/feature_specs/hello_cli/test_cli.py::test_default_greeting
  [AC#2] Accept `--message` to override the greeting text.
      -> tests/feature_specs/hello_cli/test_cli.py::test_message_override
  [AC#3] Support `--quiet` to suppress output entirely.
      -> tests/feature_specs/hello_cli/test_cli.py::test_quiet_mode_suppresses_output
[generator] The following tests do not map to any acceptance bullet:
      - tests/feature_specs/hello_cli/test_cli.py::test_repeat_behavior (docstring: AC#2 Accept `--repeat` to control repetition.)
[generator] Specs updated from /media/skynet3/8tb_a1/rex_codex_agent/documents/feature_cards/hello_cli.md
[generator] Critic returned DONE after pass 1

=== /media/skynet3/8tb_a1/rex_codex_agent/.codex_ci/generator_critic_response.log ===
DONE


=== /media/skynet3/8tb_a1/rex_codex_agent/.codex_ci/generator_response.log ===


=== /media/skynet3/8tb_a1/rex_codex_agent/.codex_ci/generator_tests.log ===


=== /media/skynet3/8tb_a1/rex_codex_agent/.codex_ci_latest.log ===
## main...origin/main
 M AGENTS.md
 M README.md
 M VERSION
 M bin/rex-codex
 M lib/burn.sh
 M lib/discriminator.sh
 M lib/generator.sh
 M lib/init.sh
 M lib/loop.sh
 M lib/updater.sh
 M lib/util.sh
 M templates/AGENTS.md
 M templates/documents/feature_cards/README.md
?? .codex_ci/
?? lib/uninstall.sh
?? rex_codex/
?? templates/requirements-dev.txt
Python 3.13.6
Python 3.13.6
Traceback (most recent call last):
  File "<string>", line 1, in <module>
    import pytest; print(pytest.__version__)
    ^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'pytest'
FFFs                                                                     [100%]
=================================== FAILURES ===================================
_ test_generator_card_status_respects_whitespace_and_case[status: proposed-proposed] _

line = 'status: proposed', expected = 'proposed'

    @pytest.mark.unit
    @pytest.mark.parametrize(
        "line, expected",
        [
            ("status: proposed", "proposed"),
            (" Status: Accepted", "accepted"),
            ("\tstatus:REVIEW", "review"),
        ],
    )
    def test_generator_card_status_respects_whitespace_and_case(line: str, expected: str) -> None:
        rex_src = os.environ.get("REX_SRC")
        if not rex_src:
            pytest.skip("REX_SRC not exported; generator helper unavailable outside rex-codex executor")
    
        _write(f"{line}\n\nTitle: Demo\n")
        command = f"source \"{rex_src}/lib/generator.sh\"; generator_card_status {CARD}"
        result = _call(command)
>       assert result.returncode == 0, result.stderr
E       AssertionError: bash: line 1: /media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh: No such file or directory
E         bash: line 1: generator_card_status: command not found
E         
E       assert 127 == 0
E        +  where 127 = CompletedProcess(args=['bash', '-lc', 'source "/media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh";.../.rex_agent/src/lib/generator.sh: No such file or directory\nbash: line 1: generator_card_status: command not found\n').returncode

templates/tests/enforcement/test_feature_card_status_parsing.py:44: AssertionError
_ test_generator_card_status_respects_whitespace_and_case[ Status: Accepted-accepted] _

line = ' Status: Accepted', expected = 'accepted'

    @pytest.mark.unit
    @pytest.mark.parametrize(
        "line, expected",
        [
            ("status: proposed", "proposed"),
            (" Status: Accepted", "accepted"),
            ("\tstatus:REVIEW", "review"),
        ],
    )
    def test_generator_card_status_respects_whitespace_and_case(line: str, expected: str) -> None:
        rex_src = os.environ.get("REX_SRC")
        if not rex_src:
            pytest.skip("REX_SRC not exported; generator helper unavailable outside rex-codex executor")
    
        _write(f"{line}\n\nTitle: Demo\n")
        command = f"source \"{rex_src}/lib/generator.sh\"; generator_card_status {CARD}"
        result = _call(command)
>       assert result.returncode == 0, result.stderr
E       AssertionError: bash: line 1: /media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh: No such file or directory
E         bash: line 1: generator_card_status: command not found
E         
E       assert 127 == 0
E        +  where 127 = CompletedProcess(args=['bash', '-lc', 'source "/media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh";.../.rex_agent/src/lib/generator.sh: No such file or directory\nbash: line 1: generator_card_status: command not found\n').returncode

templates/tests/enforcement/test_feature_card_status_parsing.py:44: AssertionError
_ test_generator_card_status_respects_whitespace_and_case[\tstatus:REVIEW-review] _

line = '\tstatus:REVIEW', expected = 'review'

    @pytest.mark.unit
    @pytest.mark.parametrize(
        "line, expected",
        [
            ("status: proposed", "proposed"),
            (" Status: Accepted", "accepted"),
            ("\tstatus:REVIEW", "review"),
        ],
    )
    def test_generator_card_status_respects_whitespace_and_case(line: str, expected: str) -> None:
        rex_src = os.environ.get("REX_SRC")
        if not rex_src:
            pytest.skip("REX_SRC not exported; generator helper unavailable outside rex-codex executor")
    
        _write(f"{line}\n\nTitle: Demo\n")
        command = f"source \"{rex_src}/lib/generator.sh\"; generator_card_status {CARD}"
        result = _call(command)
>       assert result.returncode == 0, result.stderr
E       AssertionError: bash: line 1: /media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh: No such file or directory
E         bash: line 1: generator_card_status: command not found
E         
E       assert 127 == 0
E        +  where 127 = CompletedProcess(args=['bash', '-lc', 'source "/media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh";.../.rex_agent/src/lib/generator.sh: No such file or directory\nbash: line 1: generator_card_status: command not found\n').returncode

templates/tests/enforcement/test_feature_card_status_parsing.py:44: AssertionError
=============================== warnings summary ===============================
templates/tests/enforcement/test_docs_cli_sync.py:11
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_docs_cli_sync.py:11: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_docs_cli_sync.py:19
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_docs_cli_sync.py:19: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_feature_card_status_parsing.py:27
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_feature_card_status_parsing.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_runtime_boundary.py:25
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_runtime_boundary.py:25: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_specs.py:47
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_specs.py:47: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /media/skynet3/8tb_a1/rex_codex_agent/.codex_ci/discriminator_global_smoke.xml -
=========================== short test summary info ============================
SKIPPED [1] templates/tests/enforcement/test_specs.py:51: No runtime modules to audit yet
FAILED templates/tests/enforcement/test_feature_card_status_parsing.py::test_generator_card_status_respects_whitespace_and_case[status: proposed-proposed]
FAILED templates/tests/enforcement/test_feature_card_status_parsing.py::test_generator_card_status_respects_whitespace_and_case[ Status: Accepted-accepted]
FAILED templates/tests/enforcement/test_feature_card_status_parsing.py::test_generator_card_status_respects_whitespace_and_case[\tstatus:REVIEW-review]
3 failed, 1 skipped, 3 deselected, 5 warnings in 0.49s
F.FFFss                                                                  [100%]
=================================== FAILURES ===================================
______________ test_readme_mentions_loop_and_discriminator_flags _______________

    @pytest.mark.unit
    def test_readme_mentions_loop_and_discriminator_flags() -> None:
        text = README.read_text(encoding="utf-8", errors="replace")
        assert "./rex-codex loop" in text
>       assert "discriminator --feature-only" in text
E       AssertionError: assert 'discriminator --feature-only' in '# rex_codex_agent\n\nCodex-first automation scaffold for **Python projects on Linux**. Drop the wrapper into a repo, ...butions, open an issue/PR with the diff and attach the relevant `.codex_ci/` logs so reviewers can trace the run.***\n'

templates/tests/enforcement/test_docs_cli_sync.py:15: AssertionError
_ test_generator_card_status_respects_whitespace_and_case[status: proposed-proposed] _

line = 'status: proposed', expected = 'proposed'

    @pytest.mark.unit
    @pytest.mark.parametrize(
        "line, expected",
        [
            ("status: proposed", "proposed"),
            (" Status: Accepted", "accepted"),
            ("\tstatus:REVIEW", "review"),
        ],
    )
    def test_generator_card_status_respects_whitespace_and_case(line: str, expected: str) -> None:
        rex_src = os.environ.get("REX_SRC")
        if not rex_src:
            pytest.skip("REX_SRC not exported; generator helper unavailable outside rex-codex executor")
    
        _write(f"{line}\n\nTitle: Demo\n")
        command = f"source \"{rex_src}/lib/generator.sh\"; generator_card_status {CARD}"
        result = _call(command)
>       assert result.returncode == 0, result.stderr
E       AssertionError: bash: line 1: /media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh: No such file or directory
E         bash: line 1: generator_card_status: command not found
E         
E       assert 127 == 0
E        +  where 127 = CompletedProcess(args=['bash', '-lc', 'source "/media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh";.../.rex_agent/src/lib/generator.sh: No such file or directory\nbash: line 1: generator_card_status: command not found\n').returncode

templates/tests/enforcement/test_feature_card_status_parsing.py:44: AssertionError
_ test_generator_card_status_respects_whitespace_and_case[ Status: Accepted-accepted] _

line = ' Status: Accepted', expected = 'accepted'

    @pytest.mark.unit
    @pytest.mark.parametrize(
        "line, expected",
        [
            ("status: proposed", "proposed"),
            (" Status: Accepted", "accepted"),
            ("\tstatus:REVIEW", "review"),
        ],
    )
    def test_generator_card_status_respects_whitespace_and_case(line: str, expected: str) -> None:
        rex_src = os.environ.get("REX_SRC")
        if not rex_src:
            pytest.skip("REX_SRC not exported; generator helper unavailable outside rex-codex executor")
    
        _write(f"{line}\n\nTitle: Demo\n")
        command = f"source \"{rex_src}/lib/generator.sh\"; generator_card_status {CARD}"
        result = _call(command)
>       assert result.returncode == 0, result.stderr
E       AssertionError: bash: line 1: /media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh: No such file or directory
E         bash: line 1: generator_card_status: command not found
E         
E       assert 127 == 0
E        +  where 127 = CompletedProcess(args=['bash', '-lc', 'source "/media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh";.../.rex_agent/src/lib/generator.sh: No such file or directory\nbash: line 1: generator_card_status: command not found\n').returncode

templates/tests/enforcement/test_feature_card_status_parsing.py:44: AssertionError
_ test_generator_card_status_respects_whitespace_and_case[\tstatus:REVIEW-review] _

line = '\tstatus:REVIEW', expected = 'review'

    @pytest.mark.unit
    @pytest.mark.parametrize(
        "line, expected",
        [
            ("status: proposed", "proposed"),
            (" Status: Accepted", "accepted"),
            ("\tstatus:REVIEW", "review"),
        ],
    )
    def test_generator_card_status_respects_whitespace_and_case(line: str, expected: str) -> None:
        rex_src = os.environ.get("REX_SRC")
        if not rex_src:
            pytest.skip("REX_SRC not exported; generator helper unavailable outside rex-codex executor")
    
        _write(f"{line}\n\nTitle: Demo\n")
        command = f"source \"{rex_src}/lib/generator.sh\"; generator_card_status {CARD}"
        result = _call(command)
>       assert result.returncode == 0, result.stderr
E       AssertionError: bash: line 1: /media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh: No such file or directory
E         bash: line 1: generator_card_status: command not found
E         
E       assert 127 == 0
E        +  where 127 = CompletedProcess(args=['bash', '-lc', 'source "/media/skynet3/8tb_a1/rex_codex_agent/.rex_agent/src/lib/generator.sh";.../.rex_agent/src/lib/generator.sh: No such file or directory\nbash: line 1: generator_card_status: command not found\n').returncode

templates/tests/enforcement/test_feature_card_status_parsing.py:44: AssertionError
=============================== warnings summary ===============================
templates/tests/enforcement/test_docs_cli_sync.py:11
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_docs_cli_sync.py:11: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_docs_cli_sync.py:19
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_docs_cli_sync.py:19: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_feature_card_status_parsing.py:27
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_feature_card_status_parsing.py:27: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_runtime_boundary.py:25
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_runtime_boundary.py:25: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

templates/tests/enforcement/test_specs.py:47
  /media/skynet3/8tb_a1/rex_codex_agent/templates/tests/enforcement/test_specs.py:47: PytestUnknownMarkWarning: Unknown pytest.mark.unit - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.unit

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /media/skynet3/8tb_a1/rex_codex_agent/.codex_ci/discriminator_global_unit.xml -
=========================== short test summary info ============================
SKIPPED [1] templates/tests/enforcement/test_runtime_boundary.py:29: No runtime modules to audit yet
SKIPPED [1] templates/tests/enforcement/test_specs.py:51: No runtime modules to audit yet
FAILED templates/tests/enforcement/test_docs_cli_sync.py::test_readme_mentions_loop_and_discriminator_flags
FAILED templates/tests/enforcement/test_feature_card_status_parsing.py::test_generator_card_status_respects_whitespace_and_case[status: proposed-proposed]
FAILED templates/tests/enforcement/test_feature_card_status_parsing.py::test_generator_card_status_respects_whitespace_and_case[ Status: Accepted-accepted]
FAILED templates/tests/enforcement/test_feature_card_status_parsing.py::test_generator_card_status_respects_whitespace_and_case[\tstatus:REVIEW-review]
4 failed, 1 passed, 2 skipped, 5 warnings in 0.45s
/usr/lib/python3/dist-packages/_pytest/config/__init__.py:331: PluggyTeardownRaisedWarning: A plugin raised an exception during an old-style hookwrapper teardown.
Plugin: helpconfig, Hook: pytest_cmdline_parse
UsageError: usage: pytest [options] [file_or_dir] [file_or_dir] [...]
pytest: error: unrecognized arguments: --cov=. --cov-report=term --cov-fail-under=80
  inifile: None
  rootdir: /media/skynet3/8tb_a1/rex_codex_agent
For more information see https://pluggy.readthedocs.io/en/stable/api_reference.html#pluggy.PluggyTeardownRaisedWarning
  config = pluginmanager.hook.pytest_cmdline_parse(
ERROR: usage: pytest [options] [file_or_dir] [file_or_dir] [...]
pytest: error: unrecognized arguments: --cov=. --cov-report=term --cov-fail-under=80
  inifile: None
  rootdir: /media/skynet3/8tb_a1/rex_codex_agent

bash: line 1: black: command not found
bash: line 1: isort: command not found
bash: line 1: ruff: command not found
bash: line 1: flake8: command not found
bash: line 1: mypy: command not found

=== /media/skynet3/8tb_a1/rex_codex_agent/AGENTS.md ===
# rex_codex_agent - Operations Guide

This repository ships the **Codex-first automation scaffold** that installs via `./rex-codex`. It is deliberately opinionated:

- **Platform:** Linux shells (Bash 4+) or WSL.
- **Language/tooling:** Python projects (pytest, mypy, ruff, black, isort, flake8) with coverage >=80% by default.
- **LLM:** OpenAI Codex invoked through `npx @openai/codex` (Node 18+). Discriminator LLM edits are opt-in (`DISABLE_LLM=1` by default).
- **Audit goals:**
  1. Maintain a folder named `for_external_GPT5_pro_audit/` in each working repository.
  2. After every interaction, commit and push the current state of the repository and drop into that folder a concatenated snapshot of every important script/markdown/readme file, each prefixed with its absolute path.
  3. Snapshots **must** be generated via the built-in helper (`rex_codex.utils.create_audit_snapshot(RexContext.discover())` or an equivalent CLI hook); never hand-roll or trim the audit output.
  4. Treat audits as part of the conversational handshake-produce a fresh snapshot at the end of every operator interaction before yielding control.
5. Before producing the final snapshot, run `scripts/selftest_loop.sh` (with the bundled Codex stub) so the repo proves it can regenerate the `hello_greet` / `hello_cli` specs end-to-end. The script appends its logs, status, and generated source listings to the latest audit file—commit that updated audit so external GPT5-Pro review sees the full trace.
     - Set `REX_DISABLE_AUTO_COMMIT=1` while developing locally if you only want a snapshot without touching git state.
     - Set `REX_DISABLE_AUTO_PUSH=1` when you need the audit committed but do not want the helper to push.
     - The agent automatically detects when it is running inside its own source tree and defaults to testing mode (auto commit/push disabled). Export `REX_AGENT_FORCE_BUILD=1` to override when you genuinely intend to publish from this repo.
, run `scripts/selftest_loop.sh` (with the bundled Codex stub) so the repo proves it can regenerate the `hello_greet` / `hello_cli` specs end-to-end. The script appends its logs, status, and generated source listings to the latest audit file-commit that updated audit so external GPT5-Pro review sees the full trace.
- **Self-development loop:** `bin/fake-codex`, `scripts/selftest_loop.sh`, and `scripts/smoke_e2e.sh` must stay executable and green. We dogfood the agent by reinstalling it into clean workspaces and running the generator -> discriminator pipeline offline.

The Bash wrapper is now a shim; all orchestration lives in the Python package `rex_codex` so we can unit-test and extend behaviour without shell metaprogramming.

Keep these expectations visible-both docs and templates must reinforce them so future LLM audits stay aligned.

> Repository-specific guardrails belong in `AGENTS.local.md`. This template is
> seeded alongside the global doc and never overwritten, so you can keep
> project-specific notes, integrations, and tribal knowledge there.

---

## Scope Boundaries

- **S0 – Global shim** (`bin/rex-codex`, `packaging/`): installers, uninstallers, and the thin CLI wrapper that dispatches into Python.
- **S1 – Project runtime** (`src/rex_codex/`, `project_runtime/`): pinned Python modules, templates, and manifest helpers copied into each consumer repo.
- **S2 – Sandbox** (`tests/e2e/`, `tests/unit/`, `tests/fixtures/`): hermetic self-tests that exercise the agent in throwaway repositories.

Treat each scope as a separately versioned surface: upgrade the global shim without disturbing existing projects, and evolve the sandbox without touching published runtimes.

---

## Golden Path (from empty repo to green)

1. **Install + bootstrap (inside the target repo)**
   ```bash
   curl -fsSL https://raw.githubusercontent.com/rexdouglass/rex_codex_agent/main/packaging/install.sh | bash
   ```
   The installer strips audit/CI artefacts, always replaces any existing
   `.rex_agent/`, resets `.venv`, writes a pinned `requirements.txt`, and runs
   `./rex-codex init` then `./rex-codex doctor`. Re-run those commands manually
   later if you want to refresh guardrails/tooling checks.
3. **Author a Feature Card**
   - Use `./rex-codex card new` for a guided prompt (writes `documents/feature_cards/<slug>.md` with `status: proposed`).
   - If you hand-edit, keep the `status:` line intact and leave `## Links` / `## Spec Trace` empty-the generator appends to them.
4. **Generate deterministic specs**
   ```bash
   ./rex-codex generator            # loops with a critic until DONE (use --single-pass to exit early)
   ```
   The generator:
   - Keeps diffs under `tests/feature_specs/<slug>/...` (tests only) and appends links/trace in the card.
   - Prints a dashboard summarising the Feature Card (acceptance criteria, existing specs) and previews the diff with new/updated tests before applying patches so operators can follow along in one screen.
   - Enforces patch-size limits (default 6 files / 300 lines).
- Warns when cards exist but their `status:` values don't match the requested set (e.g. typos like `propsed`) so operators can repair metadata quickly.
- Runs an AST hermeticity scan that bans network, subprocess, clock, and entropy **calls** (`requests.get`, `subprocess.run`, `time.sleep`, `uuid.uuid4`, `os.urandom`, `secrets`, `numpy.random`...), plus unconditional skip/xfail.
- Tag every spec with its acceptance target using either `"""AC#<n> ..."""` docstrings or `@pytest.mark.ac(<n>)`. The Spec Trace, HUD coverage bar, and audit snapshots rely on these markers to keep acceptance -> tests -> pass/fail traceable.
5. **Run the discriminator ladder**
   ```bash
   ./rex-codex discriminator --feature-only   # smoke/unit on the spec shard (pytest -x --maxfail=1)
   ./rex-codex discriminator --global         # full ladder (xdist auto, coverage >=80%)
   ```
   Stages = health -> tooling -> smoke/unit -> coverage -> optional `pip-audit`/`bandit`/`build` -> style/type (`black`, `isort`, `ruff`, `flake8`, `mypy`). Each pass now ends with a color summary (stage, result, duration, first failing line) plus a "next command" hint if anything failed. Logs + JUnit land in `.codex_ci/`. Successful passes are recorded in `rex-agent.json`.
6. **Iterate via the loop**
   ```bash
  ./rex-codex loop                # generator -> feature -> global
  ./rex-codex loop --explain      # preview planned stages before execution
  ./rex-codex loop --discriminator-only   # implement runtime without re-triggering generator
  DISABLE_LLM=0 ./rex-codex loop --discriminator-only   # or add --enable-llm to discriminator/loop for guarded runtime edits
  ```
The loop finishes with a two-line scoreboard (generator vs discriminator) so operators immediately know which phase passed, warned, or failed.
Every invocation also generates `for_external_GPT5_pro_audit/audit_<timestamp>.md`, stages all changes, and pushes the repository so external GPT5-Pro audits can start from the latest state.
Monitor mode (`--ui monitor`, default) keeps the HUD in a single refreshed screen. When running inside VS Code we automatically spawn a companion terminal window for the HUD and keep it around for ~30 s after completion (`GENERATOR_UI_LINGER` tunes this). We also burn down the bundled `hello_*` spec shards before each generator run so the toy project is rebuilt from scratch every time; override with `--no-scrub-specs` or `GENERATOR_SCRUB_SPECS=0` if you need to preserve previous runs.
Need the latest frame without attaching to TTY? Call the single-shot helpers-or stream them live with `--follow` (generator only)-handy for `watch -d` in CI: `./bin/rex-codex hud generator --slug <slug> [--follow]` and `./bin/rex-codex hud discriminator --slug <slug>`.
- **Mandatory self-test:** Before landing major changes or handing off a session, run `scripts/selftest_loop.sh`. It rebuilds the toy `hello` project, regenerates both feature cards, drives the discriminator ladder, and appends the command log plus generated sources to the active audit file. Leave its output in place-external reviewers rely on that trace.
7. **Promote the Feature Card**
   - When the repo is green, edit the card to `status: accepted` (generator never changes statuses). Commit your changes.

> Reset sandbox? `./rex-codex burn --dry-run` -> `./rex-codex burn -y` -> `./rex-codex init`.

### Self-development loop (maintainers run this constantly)

- `bin/fake-codex` emulates Codex and emits deterministic, hermetic diffs under `tests/feature_specs/<slug>/`. Keep it executable and versioned with the agent.
- `scripts/selftest_loop.sh` resets `.selftest_workspace/`, installs the current checkout, runs two Feature Cards (`hello_greet`, `hello_cli`) through generator -> discriminator, appends logs/status/spec listings/runtime code to the latest audit file, then removes the workspace (`SELFTEST_KEEP=1` preserves it for debugging).
- `scripts/smoke_e2e.sh` spins up a temp repo, installs the current checkout via `packaging/install.sh`, scaffolds the `hello_greet` and `hello_cli` Feature Cards, runs `./rex-codex loop --feature-only`, then executes the global discriminator sweep. Export `KEEP=1` while debugging to retain the workspace.
- Run the selftest loop before accepting PRs, bumping `VERSION`, or cutting releases; use the broader smoke harness to cross-check longer flows. Treat failures as blockers-they signal the agent can no longer bootstrap itself offline.
- After both loops pass, repeat the Golden Path manually in a new repo (your target project-e.g. the practice Pong game) to confirm end-to-end behaviour beyond the stub.

---

## Guardrails & Defaults

- **Tests-first:** generator only writes specs; runtime changes must be manual or pass the discriminator's guarded LLM step.
- **Protected surfaces:** tests, Feature Cards, documents, CI configs, dependency manifests, tooling configs are hash-snapshotted before LLM edits-unauthorized changes are reverted.
- **Runtime allow-list:** discriminator LLM patches may only touch runtime directories (`src/`, detected packages). Non-runtime paths are rejected.
- **Patch-size budgets:** generator and discriminator enforce defaults of 6 files / 300 lines (override via `GENERATOR_MAX_FILES/LINES`, `DISCRIMINATOR_MAX_FILES/LINES`).
- **Determinism:** hermetic specs ban network/entropy/time/subprocess calls; `PYTHONHASHSEED=0` is exported for generator snapshots and discriminator runs; pytest stages use configurable timeouts.
- **Coverage-first:** `COVERAGE_MIN` defaults to 80%; targets default to `src/`. Optional gates activate with `PIP_AUDIT=1`, `BANDIT=1`, `PACKAGE_CHECK=1`.
- **Auto-style:** mechanical `ruff/black/isort` runs only on runtime targets (never tests/docs).
- **Mypy scope:** type checking defaults to runtime targets (`MYPY_TARGETS` or `COVERAGE_TARGETS`); set `MYPY_INCLUDE_TESTS=1` to include spec shards when required.
- **Concurrency:** generator, discriminator, and loop take `.codex_ci/*.lock` with Python advisory (`fcntl`) locks.
- **Telemetry:** `rex-agent.json` tracks active slug/card and discriminator success metadata for auditability.

---

## Command Reference (internal expectations)

| Command | Notes for maintainers |
|---------|----------------------|
| `init` | Must remain idempotent. Seeds templates, enforces deterministic tool versions (see `templates/requirements-dev.txt`). |
| `generator` | Keep prompt guardrails aligned with code filters. Never relax hermetic checks without updating docs/templates. |
| `discriminator` | Maintain stage banners, logging, and optional gate envs. Default LLM usage must stay disabled (`DISABLE_LLM=1`). |
| `loop` | Orchestrates generator -> discriminator. Ensure flag passthrough stays consistent with docs. |
| `card` | CLI helper for card creation/listing/validation-keep prompts aligned with template README. |
| `status` / `logs` | Surface rex-agent.json metadata and `.codex_ci` tails; `logs` supports `--generator/--discriminator/--lines`. |
| `doctor` | Emit versions/paths for python/node/docker; add tooling here before relying on it elsewhere. |
| `burn` | Preserve `.git`, warn loudly, honour `--dry-run` / `--purge-agent`. |
| `uninstall` | `--force` skips the prompt; `--keep-wrapper` leaves the shim in place. |
| `self-update` | Default is **offline** (`REX_AGENT_NO_UPDATE=1`). Respect release tags (`VERSION`) when enabling `stable`. |

---

## Quick Command Cheatsheet

- `./rex-codex init` - seed guardrails and tooling (idempotent).
- `./rex-codex card new` - scaffold a Feature Card; `card list` / `card validate` keep hygiene tight.
- `./rex-codex install --force` - refresh the agent sources in-place and automatically rerun `init`/`doctor` (use `--skip-init` / `--skip-doctor` to opt out).
- `curl -fsSL https://raw.githubusercontent.com/rexdouglass/rex_codex_agent/main/packaging/install.sh | bash -s -- --force --channel main` - reinstall the latest agent snapshot from anywhere.
- `./rex-codex generator --tail 120` - replay Codex diffs and tail logs when the generator fails (add `--quiet` to silence).
- `./rex-codex discriminator --feature-only` / `--global` - run the shard or full ladder; add `--tail 120` (and `--quiet` if you want silence) during debug sessions.
- `./rex-codex loop --tail 120` - generator -> feature shard -> global sweep with inline diff previews (use `--quiet` to suppress diff chatter).
- `./rex-codex logs --generator --lines 200` - dump the latest generator response/patch without hunting for files.
- `GENERATOR_PROGRESS_SECONDS=5 ./rex-codex loop` - tighten the Codex heartbeat interval (default 15s) for long generator passes.
- `./rex-codex status` - inspect the active slug/card and last discriminator success metadata.
- `./rex-codex burn --yes` - reset the working tree (keeps `.git`; add `--purge-agent` to drop `.rex_agent`).
- `./rex-codex uninstall --force` - remove the agent (use `--keep-wrapper` to leave the shim).
- `scripts/selftest_loop.sh` - fast offline selftest that resets `.selftest_workspace/`, exercises the `hello_greet` and `hello_cli` Feature Cards, and appends logs/status/spec listings to the latest audit file (`SELFTEST_KEEP=1` preserves the workspace).
- `scripts/smoke_e2e.sh` - run the self-development loop offline; export `KEEP=1` to keep the temp repo when investigating failures.

## Documentation Duties

- Update this file, `README.md`, and templates in `templates/` whenever behaviour, defaults, or guardrails change.
- Keep the docs explicit that the agent is Python/Linux/Codex-specific-LLMs reviewing the repo should never infer cross-language support.

---

## Release Conventions

- Bump `VERSION` and tag (`vX.Y.Z`) for every behavioural/template change.
- Ensure `bin/rex-codex --help` matches documented commands.
- Include `.codex_ci/` logs (or summaries) in PRs/notes for traceability.
- Verify templates (`templates/AGENTS.md`, `templates/documents/feature_cards/README.md`, enforcement tests) reflect new behaviour before cutting a release.

Keep the guardrails tight, prefer explicit documentation, and remember every change should reduce ambiguity for future Codex audits.***

## Codex Testing Playbook

This playbook is implemented in `rex_codex.playbook` and drives the automated
conversion of Feature Cards into traceable, deterministic specs. Treat it as a
contract for how Codex plans, measures, and evolves tests.

### 0) Objectives & Non-Negotiables

**Primary goal:** Convert feature cards into a traceable, executable test suite that
captures intended behaviour, survives refactors, scales across interacting features,
improves monotonically, and stays fast and reliable in CI.

**Non-negotiables**

- Determinism over speed and repeatability over cleverness.
- Prefer public contracts (APIs, UI semantics, domain invariants) over internals.
- Traceability from card -> capability -> scenario -> observable -> test.
- Monotonic improvement: do not delete or weaken passing tests without intent.
- Default to parallel-safe execution; isolate or mark serial outliers.

### 1) Canonical Data Model

#### 1.1 Feature Card Canonicalization

Codex normalises every input card into a canonical schema:

```yaml
# FeatureCard.v1
id: FC-1234
title: "User can pause/resume a recurring transfer"
epic: "Payments - Scheduled Transfers"
risk_level: medium
priority: P1
owner: "payments-team"
version: 3
dependencies: [FC-1200, FC-1192]
acceptance_criteria:
  - id: AC-1
    text: "Pausing a transfer prevents runs until resumed."
  - id: AC-2
    text: "Resuming schedules pick up from the original cadence."
non_goals:
  - "Editing transfer amount during pause"
open_questions:
  - "What if resume date falls on a holiday?"
constraints:
  domain_invariants:
    - "Transfers cannot schedule in the past"
    - "Currency is immutable once transfer is created"
observability_hints:
  logs:
    - event: "transfer_schedule.paused"
    - event: "transfer_schedule.resumed"
  metrics:
    - counter: "transfers.paused_total"
    - counter: "transfers.resumed_total"
notes: "Existing cron-like scheduler; DB table schedules_v2"
```

Free-form cards that omit fields are captured with `unknown` placeholders plus
entries in the assumption ledger (see 2.2).

#### 1.2 Derived TestSpec Graph

Every card maps to a TestSpec graph for traceability:

```yaml
# TestSpec.v1
feature_card_id: FC-1234
capabilities:
  - id: CAP-1
    source_ac: AC-1
    statement: "Pause prevents execution"
    preconditions:
      - user_has_active_recurring_transfer
    triggers:
      - user_clicks_pause OR api_call_pause
    observables:
      - no_job_enqueued_for_next_tick
      - emitted_event: transfer_schedule.paused
      - ui_state_shows "Paused"
    negative_space:
      - cannot_execute_immediately_after_pause
    measurement_strategy:
      - "Inspect scheduler queue for next due date >= resume_date"
      - "Listen for event; assert exactly-once semantics"
    test_types: [unit, integration, e2e, property, contract]
    edge_cases:
      - "Pause within 1s of scheduled tick"
      - "Pause on holiday"
      - "Pause when already paused"
    invariants:
      - "Currency remains unchanged"
      - "Idempotent: repeated pause is no-op"
```

### 2) Resolve Ambiguity & Make It Testable

1. Decompose hierarchically: epic -> feature -> acceptance criterion -> capability
   -> scenario -> steps -> assertions.
2. Capabilities must stand alone (unit/property) and compose (integration/e2e).
3. Use cause-effect graphs and equivalence classes to minimise scenario counts.

#### 2.2 Assumption Ledger

Ambiguity is codified as explicit assumptions, never brushed aside. Each entry uses
`assumption_id`, `text`, `rationale`, `risk`, `default_choice`, and
`ways_to_falsify`. Embed assumption IDs in tests, docstrings, and PR summaries.
Maintain an escalation list for human follow-up.

Example:

```yaml
assumptions:
  - id: A-001
    text: "If resume date lands on a non-business day, schedule to next business day."
    rationale: "Aligns with existing settlement policy"
    risk: medium
    default_choice: "roll-forward"
    ways_to_falsify:
      - "OpenAPI spec contradicts"
      - "Existing prod logs show roll-back behavior"
```

### 3) Repository Intelligence & Code Mapping

Inventory the repo before generating tests: languages, frameworks, layout,
fixtures, helpers, selectors, API schemas, migrations, feature markers, and event
emitters. Build a mapping table of `capability_id -> code locations` for reuse.

### 4) Test Strategy Selection

Use a portfolio mindset. Prefer a few surgical e2e flows plus many rich
unit/property tests. Integration fills the seams. Contracts guard public APIs.

### 5) Observables & Measurement

Assert stable seams: API status/shape, event name + version, DB state without
volatile fields, `data-testid` selectors, relative timing windows, message queue
side-effects. Seed clocks and randomness. Limit snapshot tests to structural
shapes and versioned golden files.

### 6) Scenario Synthesis Algorithm

For each capability derive inputs, boundaries, negatives, and dependency
interactions. Prioritise by risk and priority. De-dupe by observable. See
`rex_codex.playbook._build_scenarios_for_capability` for the implementation.

### 7) Generate Tests

Follow Arrange-Act-Assert, stable IDs (`FC-XXXX-CAP-YY-SC-##`), reusable helpers,
and docstrings summarising assumptions/observables. Examples span unit, contract,
and UI e2e patterns with fake time.

### 8) Multi-Card Interactions

Maintain a constraint/interaction matrix across dependent cards. Generate composed
scenarios where behaviour overlaps. For conflicts, produce dual tests tagged with
the relevant assumption IDs.

### 9) Iteration, Parallelisation, and Isolation

Shard by component, keep fixtures layered, seed deterministic identifiers, and use
fake clocks. Resort to serial execution only when unavoidable.

### 10) Consistency with Existing Code

Add `data-testid` selectors rather than brittle locators. Raise contract drift
when schemas diverge. Validate events against the registry before asserting.

### 11) Improve Without Breaking

Preserve immutable test IDs, use explicit deprecation markers, produce semantic
diffs for golden updates, and run previous + new suites to guarantee monotonic
improvements.

### 12) Debugging Bad Tests

Classify failures (impossible, incorrect, flaky, dumb), minimise repro cases, fix
determinism, revisit assumptions, and run mutation tests to ensure assertions add
signal. Tag fixes with `@fixed`, quarantines with `@flaky-guarded`, gaps with
`@spec-gap(A-xxx)`.

### 13) Quality Gates

Enforce coverage, mutation score, flake rate, traceability completeness, and suite
runtime budgets. Promote maintenance by pruning redundant e2e tests.

### 14) CI/CD Integration

Pipeline order: lint -> unit/property -> contract -> integration -> e2e, with
fail-fast. Surfacing includes coverage deltas, new tests, assumptions, flake
history, and traceability tables. Attach diagnostics (screenshots, HAR, DB diffs,
event streams) on failure.

### 15) Templates & Checklists

- Traceability table (CSV) mirrors `test_id, feature_card, capability, scenario,
  observables, assumptions, test_type, components`.
- Optional Gherkin scenarios align with capability/scenario IDs.
- Test file header docstrings restate capability, scenario, assumptions, and
  observables. Pre-merge checklist covers selectors, fake time, assumption ledger,
  contract drift, and mutation score.

### 16) Property Testing Patterns

Use Hypothesis (Python) or quickcheck-like tools (Go) to encode calendar and
idempotency properties. Property tests replace bloated scenario enumerations.

### 17) Handling Legacy & Refactors

Wrap legacy endpoints with contract tests before refactors, keep UI selectors in a
registry, and add migration tests for DB changes (forward/backward compatibility).

### 18) Governance & Naming

- Test IDs: `FC-<num>-CAP-<num>-SC-<num>` and files mirror the ID.
- Tags: `@feature(FC-1234)`, `@component(scheduler)`, `@risk(high)`, `@serial`.
- Commit style: `test(FC-1234): add CAP-1 SC-03 pause prevents run`.

### 19) Failing Test Triage SOP

Check whether code changed, whether behaviour drifted, whether timing/env issues
exist, whether assertions are brittle, or whether the scenario is impossible.
Deprecate with justification when invariants prove it cannot happen.

### 20) Codex Agent Loop (High Level)

```text
for each FeatureCard:
  parse -> canonicalize
  build capability graph
  reconcile with repo mapping (APIs/UI/events/db)
  synthesise scenarios (equivalence + boundaries + negatives + interactions)
  select test portfolio (unit/property/contract/integration/e2e)
  generate tests with stable IDs + observables
  run locally with isolated fixtures + fake time
  triage failures (classify/repair)
  emit artifacts: tests, helpers, traceability, assumptions, PR summary
  commit with test impact analysis
```

### 21) Anti-patterns to Avoid

- Asserting private internals or brittle selectors.
- Time-based sleeps in place of waits or fake clocks.
- Snapshot sprawl without structural filters.
- E2E overload for edge cases better suited to unit/property tests.
- Coupling tests to global state or seeded IDs that leak across tests.

### 22) Minimal End-to-End Trace

Example mapping:

1. Card FC-1234 "Pause/Resume Recurring Transfer".
2. Capability CAP-1 "Pausing prevents next execution".
3. Scenario SC-03 "Pause just before tick".
4. Observables: `no job enqueued`, event `transfer_schedule.paused`.
5. Tests: unit (scheduler honours pause), contract (POST /pause schema), integration
   (pause -> scheduler -> no enqueue), e2e (UI pause with fake time).

**When in doubt**: document assumptions, assert behaviour at seams, use property
tests for generalised logic, keep e2e coverage sharp, and leave the suite cleaner
and more informative than you found it.

=== /media/skynet3/8tb_a1/rex_codex_agent/README.md ===
# rex_codex_agent

Codex-first automation scaffold for **Python projects on Linux**. Drop the wrapper into a repo, describe work in Feature Cards, and the agent will:

- Generate **deterministic pytest specs** (tests only) from those cards.
- Canonicalise Feature Cards into assumption ledgers, capability graphs, and traceability artefacts before every generator pass.
- Auto-detect when it is running inside its own source tree and default to a non-pushing testing mode.
- Run a disciplined **discriminator ladder** (smoke/unit → coverage ≥80% → optional security/package checks → style/type).
- Optionally nibble at runtime code with **tight guardrails** (small, allowlisted patches only).
- Capture logs, JUnit, and state in-repo so every pass is auditable.
- Dogfood itself with deterministic **self-development loops** (`scripts/selftest_loop.sh`, `scripts/smoke_e2e.sh`, and `bin/fake-codex`) so every change proves the generator → discriminator pipeline still works in a fresh repo.

> 🛠️ The agent intentionally targets **Linux shells (Bash 4+)**, **Python tooling**, and **OpenAI Codex** via `npx @openai/codex`. Windows support is via WSL; other ecosystems are out-of-scope.

`./rex-codex` is now a thin Bash shim that delegates to `python -m rex_codex`, so the orchestration logic (generator, discriminator, loop, card helpers) lives in Python modules that we can test and evolve directly.

---

## Scope Boundaries

The repository is split into three explicit scopes so the agent can act as both
a published product and its own lab:

- **S0 – Global shim** (`bin/rex-codex`, `packaging/`): minimal entrypoints that
  install/upgrade/uninstall the agent on a user’s machine.
- **S1 – Project runtime** (`src/rex_codex/`, `project_runtime/`): the pinned
  Python package, templates, and helpers that live inside each target repo.
- **S2 – Sandbox** (`tests/e2e/`, `tests/unit/`, `tests/fixtures/`): hermetic
  self-tests that spin up throwaway repos, exercise the agent, and burn them
  down cleanly.

Each scope versiones independently so historical projects can stay pinned while
the global shim and sandbox continue evolving.

---

## Requirements

- Linux (or WSL) with Bash 4+, `git`, and GNU `timeout` (Python handles advisory locks via `fcntl`).
- `python3` on PATH (the agent bootstraps a `.venv` with pytest/ruff/black/isort/flake8/mypy/pytest-cov).
- `node` 18+ if you want LLM-assisted generator/discriminator flows (the discriminator runs offline by default via `DISABLE_LLM=1`).
- Outbound network is optional: self-update now defaults **off** (`REX_AGENT_NO_UPDATE=1`). Flip to `0` to pull newer agent versions.
- For dogfooding, keep `bin/fake-codex` executable and run `scripts/selftest_loop.sh` (fast two-card loop) plus `scripts/smoke_e2e.sh` regularly—these harnesses prove the agent can install itself into a clean repo and go green without network access.

---

## Monitoring UI (optional)

- Run the passive web UI from `monitor/` to tail `.agent/logs/events.jsonl` in your browser.
  ```bash
  cd monitor
  npm install
  npm start
  ```
- The helper script `node monitor/agent/launch-monitor.js --background` starts the server detached and records the port in `.agent/logs/monitor.port`; use this from your agent boot sequence.
- Logging helpers are provided for both Node (`monitor/agent/logger-node.js`) and Python (`monitor/agent/logger-python.py`) to emit JSONL events.
- The UI stays read-only: it streams Server-Sent Events (SSE) to render task summaries, recent errors, and the live log feed.
- Set `LOG_DIR`, `EVENTS_FILE`, or `MONITOR_PORT` env vars to customise paths/ports; the default log file is `.agent/logs/events.jsonl`.
- The launcher exports `REPO_ROOT` so the server can surface `.codex_ci/component_plan_<slug>.json`; override if you run the monitor separately.
- `./rex-codex init` now runs `npm install` inside `monitor/` when dependencies are missing, so the UI is ready post-install. Use `REX_DISABLE_MONITOR_UI=1` to skip launching.
- `./rex-codex loop`, `generator`, and `discriminator` automatically launch the monitor and open your browser; the landing view now includes a Feature Planner tab that breaks cards into components → subcomponents → test proposals. The legacy terminal HUD stays disabled unless you explicitly re-enable it (set `GENERATOR_UI_POPOUT=1` if you need the old popout).

---

## Day-One Walkthrough

1. **Install + bootstrap the agent inside your repo**
   ```bash
   curl -fsSL https://raw.githubusercontent.com/rexdouglass/rex_codex_agent/main/packaging/install.sh | bash
   ```
   The installer clones the pinned sources, removes development-only artefacts,
   wipes any existing `.rex_agent/`, regenerates `.venv`, and automatically runs
   `./rex-codex init` followed by `./rex-codex doctor`. The init step now writes
   a pinned `requirements.txt` into your repo and installs those versions into
   the freshly reset `.venv`. Re-run `init`/`doctor` whenever you want to refresh
   guardrails or re-check tooling.

2. **(Optional) rerun doctor/init later**
   ```bash
   ./rex-codex init
   ./rex-codex doctor   # confirm python/node/docker availability
   ```

3. **Author a Feature Card**
   ```bash
   ./rex-codex card new       # guided prompts (writes documents/feature_cards/<slug>.md)
   ```
   Prefer the helper above—if you hand-edit, keep `status: proposed` on its own line and leave `## Links` / `## Spec Trace` empty so the generator can append to them later.
   The template in `templates/documents/feature_cards/README.md` shows the full heading layout the generator expects.

4. **Generate specs → run the ladder**
   ```bash
   ./rex-codex loop
   ```
   - **Generator** converts the card into deterministic pytest specs under `tests/feature_specs/<slug>/`.
   - Each generator pass opens with a dashboard summarising the Feature Card (title, acceptance criteria, existing specs) and previews the proposed diff with per-test highlights before patches land.
- **Discriminator** executes the staged ladder (health, smoke/unit, coverage ≥80%, optional pip-audit/bandit/build, style/type).
  - Run just the feature shard: `./rex-codex discriminator --feature-only`
  - Run the full ladder: `./rex-codex discriminator --global`
- Runs now finish with a color-coded loop summary so you can see at a glance whether generator/discriminator passed, warned, or failed and why.
- After each run, an audit snapshot is written to `for_external_GPT5_pro_audit/` and committed/pushed automatically so GPT5-Pro reviews have the latest scripts and docs.
   - Add `--explain` to preview the planned generator/discriminator phases before they run; `--no-self-update` skips the preflight update check.
   - Need a targeted rerun? `./rex-codex discriminator --feature-only` handles the shard; `./rex-codex discriminator --global` runs the full ladder.
   - Monitor mode (`--ui monitor`, default) keeps a single refreshed HUD in the active terminal. When the command runs inside VS Code we also auto-launch a popout terminal so you can watch the HUD in a standalone window (override with `--ui popout`, `--no-popout`, or `GENERATOR_UI_POPOUT=0`). For the bundled `hello_…` specs we automatically scrub `tests/feature_specs/<slug>/` before each generator run so you always watch the toy project rebuilt from scratch; disable with `GENERATOR_SCRUB_SPECS=0` if you need to preserve prior artifacts. Prefer a static frame? Use `--ui snapshot`, or `--ui off` to silence HUD output entirely.
   - Popout HUD windows linger for ~30 s after completion so you can review the final state; tune via `GENERATOR_UI_LINGER`.
   - Popouts now boot the Ink-based generator HUD (powered by `npm --prefix tui run start`) so you get the structured outline/tests/diff view in a standalone terminal. The HUD tails `.codex_ci/events.jsonl`; disable the new skin with `GENERATOR_UI_TUI=0` or customise shortcuts via `tui/README.md`.
   - Grab the latest HUD frame without a TTY (perfect for `watch -d` or CI artifacts), or stream it live with `--follow`:
     ```bash
     ./bin/rex-codex hud generator --slug <slug>
     ./bin/rex-codex hud generator --slug <slug> --follow
     ./bin/rex-codex hud discriminator --slug <slug>
     ```
   - Need a snapshot without launching generator? Run `npm --prefix tui run start` manually (or point `TUI_EVENTS_FILE` at another log) for an on-demand dashboard.

5. **Implement runtime code until green**
   - Edit modules under `src/...` (or your package directories).
   - Re-run `./rex-codex loop --discriminator-only` for fast feedback.
   - Set `DISABLE_LLM=0` or add `--enable-llm` to allow the discriminator to propose tiny guarded runtime patches (requires `node`).

6. **Accept the feature**
   - When the discriminator is green, manually change the card to `status: accepted` and commit your work.

7. **Maintenance & lifecycle**
   - `./rex-codex status` – inspect the active slug/card and last discriminator success.
   - `./rex-codex logs` – tail the latest discriminator/generator output from `.codex_ci/`.
   - `./rex-codex card list` – list cards by status for quick triage.
   - `./rex-codex card rename <old> <new>` / `card split` / `card archive` / `card prune-specs` – keep Feature Cards and spec shards tidy without manual git plumbing.
   - `./rex-codex doctor` – diagnose env issues.
   - `./rex-codex install --force` – re-clone the agent and re-run `init`/`doctor` automatically (add `--skip-init` / `--skip-doctor` to opt out).
   - `./rex-codex burn --dry-run` then `--yes` – wipe repo contents (keeps `.git`, optionally `.rex_agent`).
   - `./rex-codex uninstall --force` – remove the agent (add `--keep-wrapper` to preserve the shim).

**Troubleshooting cheat sheet**
- `curl -fsSL https://raw.githubusercontent.com/rexdouglass/rex_codex_agent/main/packaging/install.sh | bash -s -- --force --channel main` – drop the latest agent into the current repo.
- `./rex-codex generator --tail 120` – replay Codex output and show the latest diff/log on failure (add `--quiet` to silence).
- `./rex-codex loop --tail 120` – run generator + discriminator with live diff previews and automatic log tails.
- `./rex-codex logs --generator --lines 200` – dump the most recent generator response/patch when you need manual inspection.
- `scripts/selftest_loop.sh` – fast offline selftest with two feature cards; export `SELFTEST_KEEP=1` to inspect `.selftest_workspace/`.
- `scripts/smoke_e2e.sh` – run the self-development loop end-to-end; set `KEEP=1` to preserve the temp repo for debugging.
- `GENERATOR_PROGRESS_SECONDS=5 ./rex-codex loop` – tighten the Codex heartbeat interval (default 15s) so long passes show more frequent progress updates.

**Focused troubleshooting**
- Tail without hunting: `./rex-codex logs --generator --lines 200` or `./rex-codex logs --discriminator --lines 200`.
- Follow logs live when debugging long runs: `./rex-codex logs --discriminator --follow`.
- Re-run a shard while iterating: `./rex-codex discriminator --feature-only --single-pass`.
- Promote to the full ladder when the shard is green: `./rex-codex discriminator --global`.
- Cap runaway stages when debugging: `./rex-codex discriminator --stage-timeout 180` (or pass via `loop --stage-timeout`).
- Keep LLM edits disabled by default (the loop exports `DISABLE_LLM=1`). Opt in with `./rex-codex discriminator --enable-llm --single-pass` or `DISABLE_LLM=0 ./rex-codex loop --discriminator-only` when ready.
- When a stage fails, read `.codex_ci_latest.log` for the first failing command and rerun the suggested “next command”.

---

## Command Overview

| Command | Purpose | Key Flags & Env |
|---------|---------|-----------------|
| `./rex-codex install` | Reinstall or refresh the agent in-place (auto-runs `init`/`doctor`). | `--force`, `--channel`, `--skip-init`, `--skip-doctor` |
| `./rex-codex init` | Seed `.venv`, guardrails, Feature Card scaffolding, and `rex-agent.json`. | — |
| `./rex-codex generator` | Generate deterministic pytest specs from the next `status: proposed` card. | `--single-pass`, `--max-passes`, `--focus`, `--status`, `--each`, `--tail`, `--quiet`, `--reconcile` |
| `./rex-codex discriminator` | Run the staged ladder (feature shard via `--feature-only`, full sweep by default). | `--feature-only`, `--global`, `--single-pass`, `--enable-llm`, `--disable-llm`, `DISCRIMINATOR_MAX_PASSES`, `COVERAGE_MIN`, `PIP_AUDIT`, `BANDIT`, `PACKAGE_CHECK`, `MYPY_TARGETS`, `MYPY_INCLUDE_TESTS`, `--tail`, `--quiet`, `--stage-timeout` |
| `./rex-codex loop` | Generator → feature shard → global sweep in one shot. | `--generator-only`, `--discriminator-only`, `--feature-only`, `--global-only`, `--each`, `--explain`, `--no-self-update`, `--enable-llm`, `--disable-llm`, `--tail`, `--quiet`, `--stage-timeout`, `--continue-on-fail` |
| `./rex-codex card` | Manage Feature Cards (`new`, `list`, `validate`, `rename`, `split`, `archive`, `prune-specs`). | `--status`, `--acceptance` (for `new`) |
| `./rex-codex status` | Show the active slug/card and last discriminator success. | `--json` |
| `./rex-codex logs` | Tail or follow the latest generator/discriminator logs from `.codex_ci/`. | `--generator`, `--discriminator`, `--lines`, `--follow` |
| `./rex-codex doctor` | Print versions/paths for `python3`, `node`, and `docker`. | — |
| `./rex-codex burn` | Wipe the repo (keeps `.git`; optional `--purge-agent`; supports `--dry-run`). | `--yes`, `--purge-agent`, `--dry-run` |
| `./rex-codex uninstall` | Remove `.rex_agent/` and optionally the wrapper. | `--force`, `--keep-wrapper` |
| `./rex-codex self-update` | Refresh the agent when `REX_AGENT_NO_UPDATE=0`. | `--channel`, `REX_AGENT_CHANNEL` |

### Exit codes at a glance

| Command | Exit | Meaning |
|---------|------|---------|
| `generator` | 0 | Specs updated successfully. |
| `generator` | 1 | No matching Feature Card (or card path missing). |
| `generator` | 2 | Codex CLI errored; inspect `.codex_ci/generator_response.log`. |
| `generator` | 3 | Diff rejected (paths or patch-size budget). |
| `generator` | 4 | Patch application failed; manual merge required. |
| `generator` | 5 | Critic returned empty guidance. |
| `generator` | 6 | Max passes reached without a `DONE`. |
| `generator` | 7 | Guardrail rollback (card edit or hermetic failure). |
| `discriminator` | 0 | Ladder passed. |
| `discriminator` | 1 | Stage failed or max passes reached. |
| `discriminator` | 2 | LLM disabled or runtime patch rejected (see latest log). |

Artifacts land in `.codex_ci/`:
- `latest_discriminator.log` / `.codex_ci_latest.log` – tail of the latest run.
- `generator_tests.log` – pytest snapshot of generated specs.
- `discriminator_feature_<slug>.xml`, `discriminator_global_smoke.xml`, `discriminator_global_unit.xml` – JUnit results.
The agent also tracks state in `rex-agent.json` (active slug/card, last discriminator success).

---

## Generator (tests only, never runtime)

- Discovers cards by status; prompt instructs the Codex CLI to output a **unified diff** limited to `tests/feature_specs/<slug>/…` and the matching card.
- Prints a concise dashboard before each pass (Feature Card summary, acceptance criteria, existing specs) and a diff summary that calls out new/updated tests so you can see the plan at a glance.
- Maintains a Spec Trace block linking each acceptance criterion to the generated tests and appends it to the card; use `./rex-codex generator --reconcile` to review coverage and orphaned specs without invoking the Codex CLI.
- Instrument spec files so coverage stays trustworthy: tag docstrings with `AC#<n>` or decorate tests with `@pytest.mark.ac(n)` to link them to acceptance bullets. Unmapped tests surface as orphans, and the HUD’s Feature Coverage Index (FCI) updates automatically as linked tests pass or fail.
- Warns when Feature Cards exist but their `status:` values miss the requested set (useful for catching typos like `propsed`).
- Before applying a diff it enforces:
  - Allowed-path filter.
  - Patch-size budget (`GENERATOR_MAX_FILES`, `GENERATOR_MAX_LINES`).
  - Hermeticity scan blocking network/clock/entropy/subprocess calls (e.g. `requests.get`, `subprocess.run`, `time.sleep`, `uuid.uuid4`, `secrets`, `numpy.random.*`).
  - Card guard: only appends in `## Links` / `## Spec Trace`, never mutates `status:`.
- After each pass it runs pytest on the spec shard and feeds logs to a “critic” loop until the card is marked `DONE` or max passes hit.
- Long Codex calls surface elapsed-time heartbeats (default every 15 seconds, configurable via `GENERATOR_PROGRESS_SECONDS`) so the loop never sits silent during a pass.
- Stores the last few pass durations and prints a quick ETA hint when recent iterations averaged ≥20 s, so slow Codex calls come with expectations.

---

## Discriminator (quality ladder + guarded fixes)

Stages (feature or global):
1. Repo/system health (`git status -sb`, interpreter versions).
2. Tooling sanity (`python -c 'import pytest'`).
3. Smoke/unit grids (`pytest …`, parallel via `-n auto` when xdist present).
4. Coverage (default `COVERAGE_MIN=80`, targets default to `src/`).
5. Optional security/build gates (`pip-audit`, `bandit`, `python -m build` + `twine check`) driven by env flags.
6. Style/type (`black --check`, `isort --check-only`, `ruff check`, `flake8`, `mypy`).

Guardrails:
- Mechanical fixes (ruff/black/isort) run on runtime code only and auto-commit if they change anything.
- LLM runtime edits are **opt-in** (`DISABLE_LLM=0`) and obey protected-path hashing, runtime allowlists, patch-size limits, and “no shrinking tests”. Non-compliant diffs are reverted automatically.
- Each successful pass records a timestamp/slug/test-count in `rex-agent.json` for auditability.
- Every discriminator sweep ends with a colorized summary table (stage, identifier, duration, pass/fail) that includes the first failing log line and a suggested next command when something fails, making it easy to resume locally.

---

## Lifecycle Utilities & State

- `.rex_agent/` holds the agent sources; `.codex_ci/` holds run artifacts; `.codex_ci/*.lock` prevents concurrent commands from colliding.
- Templates (copied during `init`):
  - `AGENTS.md` – guardrails and operating guidance.
  - `documents/feature_cards/README.md` – how to structure cards.
  - `tests/enforcement/` – enforcement specs for repo hygiene.
- Self-update defaults off; set `REX_AGENT_NO_UPDATE=0` if you want automatic pulls (channels: `stable`, `main`, `<tag>`).

---

## Self-development Loop

- `bin/fake-codex` emulates `npx @openai/codex` and emits hermetic diffs limited to `tests/feature_specs/<slug>/`. Keep it executable so offline runs remain available.
- `scripts/selftest_loop.sh` resets `.selftest_workspace/`, installs the current checkout, exercises two feature cards (`hello_greet`, `hello_cli`) covering the default greeting and CLI flags, appends the command log/status/spec listing/runtime code to the latest audit file, then removes the workspace (set `SELFTEST_KEEP=1` to inspect).
- `scripts/smoke_e2e.sh` creates a throwaway repo, installs the current checkout via `packaging/install.sh`, scaffolds the `hello_greet` and `hello_cli` Feature Cards, and runs `./rex-codex loop --feature-only` followed by the global discriminator sweep (`KEEP=1` preserves the temp repo).
- Run the selftest loop before landing changes, bumping `VERSION`, or publishing docs; treat failures as release blockers. Follow up with the broader smoke harness as needed to validate longer paths.
- Once both pass, repeat the documented Golden Path in a fresh repo (e.g. your practice Pong game) to validate real-world usage with or without the Codex stub.
- Every selftest run appends its command log, generated sources, and discriminator outcomes to the latest `for_external_GPT5_pro_audit/audit_*.md` file. Leave that audit update in your commit so downstream reviewers (human or GPT5-Pro) can replay the evidence.

---

## Safety Rails & Defaults

- **Tests-first**: generator only writes specs; runtime edits must happen manually (or via the tightly constrained discriminator LLM pass).
- **Hermetic specs**: bans network/clock/entropy/subprocess calls, `skip`/`xfail`, and unseeded randomness.
- **Deterministic runs**: `PYTHONHASHSEED` defaults to `0`; pytest snapshots and discriminator stages honour configurable timeouts.
- **Patch-size limits**: generator and discriminator reject oversized diffs (defaults 6 files / 300 lines).
- **Protected paths**: tests, docs, configs, dependency manifests, CI, and the feature card are hashed before/after; unauthorized edits are reverted.
- **Coverage-first**: 80% minimum out of the box, captured via `pytest-cov`.
- **Optional gates**: enable `PIP_AUDIT=1`, `BANDIT=1`, `PACKAGE_CHECK=1` to bring security/build checks into the ladder.
- **Mypy scope**: type checking defaults to runtime targets (`MYPY_TARGETS` or `COVERAGE_TARGETS`); set `MYPY_INCLUDE_TESTS=1` to include spec shards when needed.
- **Concurrency-safe**: commands take out `.codex_ci/*.lock` using Python advisory (`fcntl`) locks.
- **Observability**: logs, JUnit XML, and recent state written to disk for CI ingestion and human review.

---

## Staying in the Guardrails

- The agent is purpose-built for **Python projects on Linux** with Codex as the LLM backend. Keep runtimes/tools in that lane for best results.
- When introducing new workflows or altering command behaviour, update `AGENTS.md`, this README, and the relevant templates before cutting a release.
- Version Tagged releases via `VERSION` ensure `REX_AGENT_CHANNEL=stable` installations stay reproducible.

Happy test-first hacking! For questions or contributions, open an issue/PR with the diff and attach the relevant `.codex_ci/` logs so reviewers can trace the run.***

=== /media/skynet3/8tb_a1/rex_codex_agent/documents/assumption_ledgers/README.md ===
# Assumption Ledgers

The generator now persists card-specific assumption ledgers under this folder. Each
ledger records:

- `assumption_id`, `text`, `rationale`, `risk`, `default_choice`, and
  `ways_to_falsify`.
- `escalation_hints` for product or QA follow-up.

Ledgers are updated every generator pass via `rex_codex.playbook`. Update them
manually when ambiguity is resolved so future runs stop carrying redundant
assumptions. Tests should reference relevant assumption IDs (`A-###`) in their
docstrings or markers.

=== /media/skynet3/8tb_a1/rex_codex_agent/documents/design_review.md ===
# rex_codex_agent Design Review

This document captures the current architecture, the rationale for migrating orchestration logic from shell to Python, and the intended end-to-end user experience. It is meant to keep expectations visible for future audits.

## 1. Current Posture

- **Purposeful constraints:** Linux shells (Bash 4+ or WSL), Python projects and tooling (pytest, mypy, ruff, black, isort, flake8) with coverage >=80 percent, and Codex as the LLM backend via `npx @openai/codex`. The agent is intentionally Python/Linux/Codex-specific.
- **Golden Path** is already documented: install the wrapper, run `init` and `doctor`, author Feature Cards, generate deterministic specs, pass the discriminator ladder, iterate, and finally accept the card. Guardrails include hermetic tests, patch budgets, and deterministic defaults.
- **Shell commands** today: `init`, `generator`, `discriminator`, `loop`, `supervise`, `uninstall`, and a gated `self-update`.
  - `init` bootstraps `.venv`, seeds templates, writes `rex-agent.json`, and enforces deterministic tool versions.
  - `generator` produces deterministic pytest specs in `tests/feature_specs/<slug>/`, appends to the Feature Card links/trace sections, and enforces hermetic AST checks and patch budgets (defaults: 6 files, 300 lines).
  - `discriminator` runs a staged ladder (health, tooling, smoke/unit shards, coverage >=80 percent, optional pip-audit/bandit/build, then style/type). Mechanical fixes are limited to runtime paths, and LLM runtime edits are off by default (`DISABLE_LLM=1`).
  - `loop` orchestrates generator -> discriminator with Python advisory (`fcntl`) locking, and mirrors flag passthrough from the underlying commands.
  - `install` provides an in-place refresh path (`--force` re-clones the agent) and now re-runs `init`/`doctor` automatically so the repo is ready immediately (opt out via `--skip-init` / `--skip-doctor`).
  - `supervise` is a thin wrapper over `loop`.
  - `uninstall` requires typing "remove agent" and honors `--keep-wrapper`.
  - `self-update` is opt-in and respects release channels via environment flags.

**Bottom line:** the current stack already delivers a disciplined, tests-first workflow with strong safety rails and reproducibility.

## 2. Architecture Direction (Shell vs. Python)

### Decision

Keep a thin Bash wrapper for installation ergonomics, but migrate orchestration logic into a Python package (`rex_codex`). The wrapper should drop into any repo and dispatch to `python -m rex_codex.cli`, preserving existing flags and behavior.

### Trade-offs

#### Bash (current)

- ✅ Minimal bootstrap friction; ideal for `curl | bash` installers; historical wrapper leveraged `flock`/`timeout`, while the Python CLI now owns locking via `fcntl`.
- ❌ Complex parsing, state management, and error handling are brittle; unit-testing is limited.
- ❌ The shell scripts already embed sizeable Python snippets (AST scanning, JSON edits), signalling that core logic wants a proper Python home.

#### Python (proposed)

- ✅ First-class support for testing, typing, logging, and state management; easier to express nuanced guardrails (protected-path hashing, hermetic scans, patch budgets).
- ✅ Enables richer UX (guided card creation, structured status/log outputs).
- ✅ Keeps guardrails centralized and testable.
- ❌ Requires Python to be present, but `.venv` bootstrapping already assumes it; retain the Bash shim to keep the drop-in experience.

### Recommendation

Adopt the hybrid approach: retain `./rex-codex` as a shell shim, but keep generator, discriminator, loop, doctor, burn, uninstall, and self-update inside the Python CLI so behavior can be tested and evolved safely.

## 3. User Journey (Idea -> Specs -> Runtime -> Quality Gates -> Iteration)

### Phase A: Idea to Deterministic Specs

1. **Install and bootstrap**
   ```bash
   curl -fsSL https://raw.githubusercontent.com/rexdouglass/rex_codex_agent/main/packaging/install.sh | bash
   ./rex-codex init
   ./rex-codex doctor
   ```
   `init` creates `.venv`, installs dev tooling, copies templates (`AGENTS.md`, pytest/mypy configs, enforcement tests), and seeds `rex-agent.json`. Documentation highlights the Linux/Python/Codex scope and the Golden Path.
2. **Author a Feature Card**
   - Run `./rex-codex card new` or hand-edit `documents/feature_cards/<slug>.md`.
   - Keep `status: proposed` on its own line; leave `## Links` and `## Spec Trace` empty so the generator can append.
3. **Generate deterministic specs (tests only)**
   ```bash
   ./rex-codex generator
   ```
   - Writes tests to `tests/feature_specs/<slug>/`.
   - Enforces patch budgets (defaults: 6 files, 300 lines) and hermetic AST scan (blocks network/time/entropy/subprocess **calls**, yet allows deterministic imports; unconditional skip/xfail remain forbidden).
   - Appends references to the Feature Card but never modifies `status:`.

### Phase B: Implement Runtime and Pass the Ladder

4. **Run the discriminator ladder**
   ```bash
   ./rex-codex discriminator --feature-only
   ./rex-codex discriminator --global
   ```
   Stages: health -> tooling -> smoke/unit -> coverage (>=80 percent, targets default to `src/`) -> optional security/build gates -> style/type (`black`, `isort`, `ruff`, `flake8`, `mypy`). Artifacts (logs, JUnit) live under `.codex_ci/`.
5. **Iterate on runtime code**
   - Implement features inside runtime allowlists (`src/...` or detected packages).
   - Use `./rex-codex loop --discriminator-only` for tight feedback.
   - Mechanical formatters can auto-fix runtime files; LLM runtime edits remain opt-in (enable with `--enable-llm` or `DISABLE_LLM=0`) and heavily constrained.
   - Type checking defaults to runtime targets via `MYPY_TARGETS` / `COVERAGE_TARGETS`; set `MYPY_INCLUDE_TESTS=1` when you need to type-check generated specs.

### Phase C: Changing Scope or Refining Requirements

6. **Refine acceptance criteria**
   - Edit the card while it remains `status: proposed` (or include accepted cards via flags).
   - Re-run the generator; it updates specs within guardrails and appends card links/trace.
   - Run the discriminator to validate the new requirements.
7. **Split or merge scope**
   - Create additional cards as needed (`card new`).
   - Use generator/discriminator status filters (`--include-accepted`, `--status`) to revisit accepted work when needed.
8. **Reset sandbox or uninstall**
   - `./rex-codex burn --dry-run` -> `./rex-codex burn --yes` to reset (preserves `.git`, optional `--purge-agent`).
   - `./rex-codex uninstall --force` removes the agent without prompts; add `--keep-wrapper` to retain the shim for a reinstall.

## 4. Python CLI UX Enhancements

The Python CLI enables ergonomics that were cumbersome in shell:

1. **Guided Feature Card workflow** (`card new`, `card list`, `card validate`) with prompts and linting.
2. **Single "do the right thing" command** via `loop`, showing a summary of the planned generator/discriminator stages.
3. **Better observability** through `status` (renders `rex-agent.json`) and `logs` (tails `.codex_ci/` artifacts).
4. **Explicit self-update controls** (`self-update --channel`, `REX_AGENT_NO_UPDATE`, `REX_AGENT_CHANNEL`).
5. **Explain mode** (`loop --explain`) to preview guardrails, patch budgets, and planned stages before execution.
6. **Verbose/tail diagnostics** (`generator --tail`, `loop --tail`, `logs --generator/--discriminator`) so engineers can inspect Codex output without copying files manually (add `--quiet` to silence).

## 5. Migration Plan

1. **Introduce the Python CLI package** (`rex_codex`) mirroring existing shell commands; re-home embedded Python snippets (AST scan, patch metrics, JSON state) into modules with tests.
2. **Convert `./rex-codex` into a thin shim** that locates the repo root, ensures Python is available, exports `PYTHONPATH` for the vendored sources, and calls `python -m rex_codex`.
3. **Expand UX** once parity is achieved: card helpers, `status`, `logs`, `self-update` surface, all while keeping default guardrails untouched (LLM off by default, coverage >=80 percent, patch budgets, hermetic specs).

## 6. Day-in-the-Life Scenario

1. Reinstall/refresh the agent (`curl … install.sh | bash -s -- --force --channel main` or `./rex-codex install --force --channel main`)—this now runs `./rex-codex init` and `./rex-codex doctor` automatically so the repo is seeded.
2. Create a card (`card new`) describing acceptance criteria (leave `status: proposed`).
3. Run `loop` to generate specs and execute the feature shard of the discriminator. Logs land in `.codex_ci/`.
4. Implement runtime code, rerunning `loop --discriminator-only` until green on feature and global stages.
5. Promote the card to `status: accepted` once the ladder passes.
6. When requirements change, update the card, rerun the generator, and iterate through the discriminator ladder again.
7. Optionally enable LLM runtime assistance by passing `--enable-llm` (or exporting `DISABLE_LLM=0`); guardrails still enforce runtime allowlists, patch budgets, and protected-path hashing. If Node is missing, the flow continues offline.
8. Use burn/uninstall flows to reset the environment or remove the agent entirely (`install --force` is available for re-cloning without a full uninstall).

## 7. Final Recommendation

- Adopt the hybrid architecture (shell shim + Python CLI) to align with the Python-first ecosystem, improve testability, and simplify future evolution.
- Preserve current guardrails and defaults: hermetic specs, protected paths, patch budgets, coverage >=80 percent, LLM disabled by default. These are the backbone of the tests-first, deterministic CI story and are reflected across README, AGENTS.md, and templates.
- Continue updating documentation (`README.md`, `AGENTS.md`, templates) whenever behavior or defaults change so future audits remain frictionless.

=== /media/skynet3/8tb_a1/rex_codex_agent/documents/feature_cards/hello_cli.md ===
# Hello CLI

status: proposed

## Summary

Provide a simple command-line greeting that demonstrates the generator HUD.

## Acceptance Criteria

- Run with default arguments and print `Hello World`.
- Accept `--message` to override the greeting text.
- Support `--quiet` to suppress output entirely.

## Links

- (pending)

## Spec Trace

- [AC#1] "Run with default arguments and print `Hello World`."
  -> tests/feature_specs/hello_cli/test_cli.py::test_default_greeting
- [AC#2] "Accept `--message` to override the greeting text."
  -> tests/feature_specs/hello_cli/test_cli.py::test_message_override
  -> tests/feature_specs/hello_cli/test_cli.py::test_message_flag_requires_value
- [AC#3] "Support `--quiet` to suppress output entirely."
  -> tests/feature_specs/hello_cli/test_cli.py::test_quiet_mode_suppresses_output

=== /media/skynet3/8tb_a1/rex_codex_agent/scripts/install.sh ===
#!/usr/bin/env bash
set -Eeuo pipefail
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
exec "$ROOT/packaging/install.sh" "$@"

=== /media/skynet3/8tb_a1/rex_codex_agent/scripts/selftest_loop.sh ===
#!/usr/bin/env bash
# Deterministic self-development loop with two feature cards.
set -Eeuo pipefail

this_dir="$(dirname "${BASH_SOURCE[0]}")"
repo_root="$(git -C "$this_dir/.." rev-parse --show-toplevel 2>/dev/null || realpath "$this_dir/..")"
workspace="$repo_root/.selftest_workspace"
log_file="$workspace/selftest.log"
fake_codex="$repo_root/bin/fake-codex"

if [[ ! -x "$fake_codex" ]]; then
  echo "[!] Missing executable Codex stub at $fake_codex" >&2
  exit 1
fi

if [[ -d "$workspace" ]]; then
  rm -rf "$workspace"
fi
mkdir -p "$workspace"
cd "$workspace"
shim_dir="$workspace/.shim"
mkdir -p "$shim_dir"
ln -sf "$(command -v python3)" "$shim_dir/python"
export PATH="$shim_dir:$PATH"
if [[ -d "$workspace/src/src" ]]; then
  export PYTHONPATH="$workspace/src/src:${PYTHONPATH:-}"
else
  export PYTHONPATH="$workspace/src:${PYTHONPATH:-}"
fi
export ROOT="$workspace"
export PYTHONHASHSEED=0

last_status=0

run() {
  printf '\n[%s] %s\n' "$(date -Ins --utc)" "$*" | tee -a "$log_file"
  set +e
  "$@" 2>&1 | tee -a "$log_file"
  status=${PIPESTATUS[0]}
  set -e
  if [[ $status -ne 0 ]]; then
    printf '[!] Command failed (%s) with exit status %s\n' "$*" "$status" | tee -a "$log_file"
    exit "$status"
  fi
  last_status=$status
}

finalized=0
finalize() {
  local exit_status=$?
  if ((finalized)); then
    return
  fi
  finalized=1
  set +e

  local audit_dir="$repo_root/for_external_GPT5_pro_audit"
  mkdir -p "$audit_dir"
  local latest_audit
  latest_audit="$(ls -1 "$audit_dir"/audit_*.md 2>/dev/null | sort | tail -1)"
  if [[ -z "$latest_audit" ]]; then
    latest_audit="$audit_dir/audit_$(date -u +%Y%m%d%H%M%S)_selftest.md"
    : >"$latest_audit"
  fi

  local status_output=""
  if [[ -x "$workspace/rex-codex" ]]; then
    status_output="$(cd "$workspace" && ./rex-codex status 2>&1 || true)"
  fi

  local specs_output=""
  if [[ -d "$workspace/tests/feature_specs" ]]; then
    specs_output="$(cd "$workspace" && find tests/feature_specs -type f -print | sort || true)"
  fi

  {
    printf '\n## Local Selftest Loop (%s UTC)\n\n' "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
    printf -- '- Workspace: %s\n' "$workspace"
    printf -- '- Features exercised: %s\n' "${SLUGS[*]:-n/a}"
    if [[ -n "${global_status:-}" ]]; then
      printf -- '- Global discriminator exit code: %s\n' "$global_status"
    fi
    printf -- '- Last command exit code: %s\n' "${last_status:-n/a}"
    printf -- '- Script exit code: %s\n\n' "$exit_status"
    if [[ -n "$status_output" ]]; then
      printf '### rex-codex status\n\n```\n%s\n```\n' "$status_output"
    fi
    if [[ -n "$specs_output" ]]; then
      printf '### Generated spec files\n\n```\n%s\n```\n' "$specs_output"
    fi
    if [[ -f "$log_file" ]]; then
      printf '### Command log\n\n```\n'
      cat "$log_file"
      printf '```\n'
    fi
    if [[ -f "$workspace/src/hello/__init__.py" ]]; then
      printf '### Runtime module (src/hello/__init__.py)\n\n```python\n'
      cat "$workspace/src/hello/__init__.py"
      printf '```\n'
    fi
    if [[ -f "$workspace/src/hello/__main__.py" ]]; then
      printf '### CLI entry (src/hello/__main__.py)\n\n```python\n'
      cat "$workspace/src/hello/__main__.py"
      printf '```\n'
    fi
  } >>"$latest_audit"

  cd "$repo_root" 2>/dev/null || true
  if [[ "${SELFTEST_KEEP:-0}" == "1" ]]; then
    echo "[i] Preserved workspace at $workspace"
  else
    rm -rf "$workspace"
    echo "[*] Removed workspace at $workspace"
  fi

  set -e
}
trap finalize EXIT

run git init -q
run git config user.email "selftest@rex.codex"
run git config user.name "Rex Codex Selftest"
cat > README.md <<'MD'
# selftest workspace

This sandbox exercises `./rex-codex loop`, including `./rex-codex discriminator --feature-only`
and `./rex-codex discriminator --global`.
MD
mkdir -p src/hello
cat > src/hello/__init__.py <<'PY'
from __future__ import annotations

import argparse
from collections.abc import Sequence

DEFAULT_MESSAGE = "Hello World"


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Print a configurable greeting.")
    parser.add_argument(
        "--message",
        default=DEFAULT_MESSAGE,
        help="Override the greeting message.",
    )
    parser.add_argument(
        "--repeat",
        type=int,
        default=1,
        help="Number of times to print the greeting.",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress output.",
    )
    return parser


def main(argv: Sequence[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    repeats = args.repeat if args.repeat >= 0 else 0
    if not args.quiet and repeats > 0:
        for _ in range(repeats):
            print(args.message)
    return 0


def run() -> int:
    return main(None)
PY
cat > src/hello/__main__.py <<'PY'
from __future__ import annotations

from . import main


def entrypoint() -> int:
    return main(None)


if __name__ == "__main__":
    raise SystemExit(entrypoint())
PY
run git add -A
run git commit -q -m "chore: seed runtime"

export REPO_URL="$repo_root"
export REX_AGENT_CHANNEL=main
export REX_AGENT_FORCE=1
export REX_AGENT_SKIP_DOCTOR=1

run bash "$repo_root/packaging/install.sh" --force --channel main

export CODEX_BIN="$fake_codex"
export REX_AGENT_NO_UPDATE=1
export PYTHON=python3
export PYENV_VERSION="${PYENV_VERSION:-3.11.8}"

declare -a SLUGS=("hello_greet" "hello_cli")
declare -A TITLES
declare -A SUMMARIES
declare -A ACCEPTANCE

TITLES["hello_greet"]="Print a default greeting"
SUMMARIES["hello_greet"]="Ensure `python -m hello` prints 'Hello World'."
ACCEPTANCE["hello_greet"]="python -m hello outputs Hello World once"

TITLES["hello_cli"]="Configure greeting via CLI"
SUMMARIES["hello_cli"]="Support --message, --repeat, and --quiet flags for the hello app."
ACCEPTANCE["hello_cli"]="python -m hello --message 'Hi' --repeat 2 prints Hi twice"

for slug in "${SLUGS[@]}"; do
  run ./rex-codex card new "$slug" \
    --title "${TITLES[$slug]}" \
    --summary "${SUMMARIES[$slug]}" \
    --acceptance "${ACCEPTANCE[$slug]}"

  run ./rex-codex generator "documents/feature_cards/${slug}.md" --single-pass
  run ./rex-codex discriminator --feature-only --single-pass --disable-llm
done

run ./rex-codex discriminator --global --single-pass --disable-llm
global_status=$last_status

=== /media/skynet3/8tb_a1/rex_codex_agent/scripts/smoke_e2e.sh ===
#!/usr/bin/env bash
# Deterministic end-to-end smoke of rex_codex_agent using the local checkout and Codex stub.
set -Eeuo pipefail

this_dir="$(dirname "${BASH_SOURCE[0]}")"
repo_root="$(git -C "$this_dir/.." rev-parse --show-toplevel 2>/dev/null || realpath "$this_dir/..")"
fake_codex="$repo_root/bin/fake-codex"

if [[ ! -x "$fake_codex" ]]; then
  echo "[!] Missing executable Codex stub at $fake_codex" >&2
  exit 1
fi

workdir="$(mktemp -d -t rex-codex-smoke.XXXXXX)"
keep="${KEEP:-0}"
cleanup() {
  status=$?
  if [[ "$keep" == "1" ]]; then
    echo "[i] Kept workdir: $workdir"
  else
    rm -rf "$workdir"
  fi
  exit $status
}
trap cleanup EXIT

echo "[*] Workdir: $workdir"
cd "$workdir"

mkdir dummy && cd dummy
git init -q
git config user.email "smoke@test.local"
git config user.name "Rex Codex Smoke"
cat > README.md <<'MD'
# dummy project

This sandbox exercises `./rex-codex loop`, including `./rex-codex discriminator --feature-only`
and `./rex-codex discriminator --global`.
MD
shim_dir="$PWD/.shim"
mkdir -p "$shim_dir"
ln -sf "$(command -v python3)" "$shim_dir/python"
export PATH="$shim_dir:$PATH"
mkdir -p src/hello
cat > src/hello/__init__.py <<'PY'
from __future__ import annotations

import argparse
from collections.abc import Sequence

DEFAULT_MESSAGE = "Hello World"


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Print a configurable greeting.")
    parser.add_argument(
        "--message",
        default=DEFAULT_MESSAGE,
        help="Override the greeting message.",
    )
    parser.add_argument(
        "--repeat",
        type=int,
        default=1,
        help="Number of times to print the greeting.",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress output.",
    )
    return parser


def main(argv: Sequence[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    repeats = args.repeat if args.repeat >= 0 else 0
    if not args.quiet and repeats > 0:
        for _ in range(repeats):
            print(args.message)
    return 0


def run() -> int:
    return main(None)
PY
cat > src/hello/__main__.py <<'PY'
from __future__ import annotations

from . import main


def entrypoint() -> int:
    return main(None)


if __name__ == "__main__":
    raise SystemExit(entrypoint())
PY
git add -A
git commit -m "chore: seed dummy runtime" >/dev/null

export REPO_URL="$repo_root"
export REX_AGENT_CHANNEL=main
export REX_AGENT_FORCE=1
export REX_AGENT_SKIP_DOCTOR=1

bash "$repo_root/packaging/install.sh" --force --channel main

declare -a SLUGS=("hello_greet" "hello_cli")
declare -A TITLES
declare -A SUMMARIES
declare -A ACCEPTANCE

TITLES["hello_greet"]="Print a default greeting"
SUMMARIES["hello_greet"]="Ensure `python -m hello` prints 'Hello World'."
ACCEPTANCE["hello_greet"]="python -m hello outputs Hello World once"

TITLES["hello_cli"]="Configure greeting via CLI"
SUMMARIES["hello_cli"]="Support --message, --repeat, and --quiet flags for the hello app."
ACCEPTANCE["hello_cli"]="python -m hello --message 'Hi' --repeat 2 prints Hi twice"

for slug in "${SLUGS[@]}"; do
  ./rex-codex card new "$slug" \
    --title "${TITLES[$slug]}" \
    --summary "${SUMMARIES[$slug]}" \
    --acceptance "${ACCEPTANCE[$slug]}"
done

export PYTHON=python3
export CODEX_BIN="$fake_codex"
export REX_AGENT_NO_UPDATE=1
export PYENV_VERSION="${PYENV_VERSION:-3.11.8}"

./rex-codex loop --feature-only --no-self-update --tail 120
./rex-codex discriminator --global --single-pass --disable-llm

echo
echo "[✓] Smoke run complete."
echo "    Created spec files:"
find tests/feature_specs -maxdepth 3 -type f -print || true
echo
./rex-codex status

=== /media/skynet3/8tb_a1/rex_codex_agent/scripts/start_hud_popout.sh ===
#!/usr/bin/env bash
set -euo pipefail

if [[ -z "${1:-}" ]]; then
  echo "Usage: $0 <slug>" >&2
  exit 1
fi

slug="$1"
root="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
events_file="$root/.codex_ci/events.jsonl"
diff_file="$root/.codex_ci/generator_patch.diff"
build_entry="$root/tui/dist/index.js"

quote() {
  local value="${1//\'/\'\\\'\'}"
  printf "'%s'" "$value"
}

install_cmd="if [ ! -d tui/node_modules ]; then npm --prefix tui install --no-fund --no-audit >/dev/null 2>&1 || exit 1; fi"
build_cmd="if [ ! -f tui/dist/index.js ]; then npm --prefix tui run build >/dev/null 2>&1 || exit 1; fi"
env_prefix="FORCE_COLOR=1 TUI_SLUG=$(quote "$slug") TUI_REPO_ROOT=$(quote "$root") TUI_EVENTS_FILE=$(quote "$events_file") TUI_DIFF_FILE=$(quote "$diff_file")"
command="cd $(quote "$root") && $install_cmd && $build_cmd && $env_prefix node $(quote "$build_entry")"

exec gnome-terminal \
  --title "rex-codex HUD :: $slug" \
  -- bash -lc "$command"

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/__init__.py ===
"""rex_codex Python package.

This package hosts the primary CLI implementation for the rex-codex agent.
The legacy Bash entrypoints now delegate to these modules so behaviour can be
unit-tested and extended directly in Python.
"""

from __future__ import annotations

from importlib import import_module
from pathlib import Path


def _read_version() -> str:
    """Resolve the project VERSION file even when the package lives under src/."""
    for parent in Path(__file__).resolve().parents:
        version_file = parent / "VERSION"
        if version_file.is_file():
            return version_file.read_text(encoding="utf-8").strip()
    return "0.0.0"


__all__ = ["__version__", "scope_global", "scope_project", "scope_sandbox"]
__version__ = _read_version()


def __getattr__(name: str):
    if name in {"scope_global", "scope_project", "scope_sandbox"}:
        return import_module(f"{__name__}.{name}")
    raise AttributeError(name)

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/__main__.py ===
"""Enable `python -m rex_codex`."""

from __future__ import annotations

from .cli import app


def main() -> None:  # pragma: no cover - exercised via Typer
    app()


if __name__ == "__main__":  # pragma: no cover
    main()

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/_compat.py ===
"""Helpers for re-exporting modules across the new scope boundaries."""

from __future__ import annotations

import sys
from importlib import import_module
from types import ModuleType
from typing import Dict


def reexport(module_path: str, global_ns: Dict[str, object]) -> ModuleType:
    """Populate ``global_ns`` with attributes from ``module_path``.

    This preserves backwards compatibility for modules that used to live at the
    package root while allowing us to group implementations under
    ``scope_*`` packages.
    """

    module = import_module(module_path)
    exported = getattr(module, "__all__", None)
    if exported is None:
        names = [name for name in dir(module) if not name.startswith("__")]
    else:
        names = list(exported)
        extras = [name for name in dir(module) if name.startswith("_") and not name.startswith("__")]
        for extra in extras:
            if extra not in names:
                names.append(extra)

    for name in names:
        global_ns[name] = getattr(module, name)
    global_ns["__all__"] = names
    sys.modules[global_ns["__name__"]] = module
    return module

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/burn.py ===
"""Compatibility shim for project runtime burn helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.burn", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/cards.py ===
"""Compatibility shim for project runtime card helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.cards", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/cli.py ===
"""Compatibility shim for legacy imports of rex_codex.cli."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_global.cli", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/component_planner.py ===
"""Compatibility shim for project runtime component planner helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.component_planner", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/config.py ===
"""Compatibility shim for project runtime configuration helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.config", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/discriminator.py ===
"""Compatibility shim for project runtime discriminator helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.discriminator", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/doctor.py ===
"""Compatibility shim for project runtime doctor helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.doctor", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/events.py ===
"""Compatibility shim for project runtime event helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.events", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/generator.py ===
"""Compatibility shim for project runtime generator helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.generator", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/generator_ui.py ===
"""Compatibility shim for project runtime generator UI helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.generator_ui", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/hermetic.py ===
"""Compatibility shim for project runtime hermetic helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.hermetic", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/hud.py ===
"""Compatibility shim for project runtime HUD helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.hud", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/init.py ===
"""Compatibility shim for project runtime init helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.init", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/install.py ===
"""Compatibility shim exposing global install helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_global.install", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/logs.py ===
"""Compatibility shim for project runtime log helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.logs", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/loop.py ===
"""Compatibility shim for project runtime loop helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.loop", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/monitoring.py ===
"""Compatibility shim for project runtime monitoring helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.monitoring", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/playbook.py ===
"""Compatibility shim for project runtime playbook helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.playbook", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_global/__init__.py ===
"""Expose the global shim scope for external callers."""

from __future__ import annotations

from .cli import app, build_parser  # noqa: F401
from .install import run_install  # noqa: F401
from .self_update import self_update  # noqa: F401
from .uninstall import uninstall_agent  # noqa: F401

__all__ = [
    "app",
    "build_parser",
    "run_install",
    "self_update",
    "uninstall_agent",
]

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_global/cli.py ===
"""Command-line interface for rex-codex."""

from __future__ import annotations

import argparse
from pathlib import Path

from .. import __version__
from ..scope_project.burn import burn_repo
from ..scope_project.cards import (
    archive_card,
    create_card,
    discover_cards,
    lint_all_cards,
    prune_spec_directories,
    rename_card,
    sanitise_slug,
    spec_directory,
    split_card,
)
from ..scope_project.discriminator import DiscriminatorOptions, run_discriminator
from ..scope_project.doctor import run_doctor
from ..scope_project.generator import GeneratorOptions, parse_statuses, run_generator
from ..scope_project.init import run_init
from .install import run_install
from ..scope_project.logs import show_latest_logs
from ..scope_project.loop import LoopOptions, run_loop
from .self_update import self_update
from ..scope_project.status import render_status
from .uninstall import uninstall_agent
from ..scope_project.utils import RexContext, prompt


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="rex-codex", description="Codex automation scaffold"
    )
    parser.add_argument(
        "--version", action="version", version=f"rex-codex {__version__}"
    )
    sub = parser.add_subparsers(dest="command")

    # install / init
    install_parser = sub.add_parser(
        "install", help="Install or refresh the rex-codex agent"
    )
    install_parser.add_argument(
        "--force",
        action="store_true",
        help="Remove existing .rex_agent before installing",
    )
    install_parser.add_argument(
        "--channel", help="Install a specific channel/tag (e.g. stable, main)"
    )
    install_parser.add_argument(
        "--skip-init",
        action="store_true",
        help="Skip running ./rex-codex init after install",
    )
    install_parser.add_argument(
        "--skip-doctor",
        action="store_true",
        help="Skip running ./rex-codex doctor after install",
    )

    init_parser = sub.add_parser("init", help="Seed guardrails and tooling")
    init_parser.add_argument(
        "--no-self-update",
        action="store_true",
        help="Skip self-update before initializing",
    )

    # generator
    gen_parser = sub.add_parser(
        "generator", help="Generate deterministic specs for Feature Cards"
    )
    gen_parser.add_argument("card", nargs="?", help="Feature Card path to focus on")
    gen_parser.add_argument(
        "--single-pass", action="store_true", help="Run generator once and stop"
    )
    gen_parser.add_argument(
        "--max-passes", type=int, default=None, help="Maximum passes before giving up"
    )
    gen_parser.add_argument(
        "--focus", default="", help="Seed additional coverage focus"
    )
    gen_parser.add_argument(
        "--include-accepted",
        action="store_true",
        help="Consider cards with status: accepted",
    )
    gen_parser.add_argument(
        "--status",
        dest="statuses",
        default=None,
        help="Comma-separated statuses to include",
    )
    gen_parser.add_argument(
        "--each",
        action="store_true",
        help="Process each matching Feature Card sequentially",
    )
    gen_parser.add_argument(
        "--reconcile",
        action="store_true",
        help="Report Spec Trace coverage without writing diffs",
    )
    gen_parser.add_argument(
        "--tail",
        type=int,
        default=0,
        help="Tail log output (N lines) when the generator fails",
    )
    gen_parser.add_argument(
        "--ui",
        choices=["monitor", "snapshot", "off", "auto", "popout"],
        default=None,
        help="Generator HUD mode (default: monitor when attached to a TTY)",
    )
    gen_parser.add_argument(
        "--popout",
        dest="popout",
        action="store_true",
        help="Launch the generator HUD in a separate terminal window when possible",
    )
    gen_parser.add_argument(
        "--no-popout",
        dest="popout",
        action="store_false",
        help="Disable generator HUD popout behaviour",
    )
    gen_parser.add_argument(
        "--scrub-specs",
        dest="scrub_specs",
        action="store_true",
        help="Delete the spec shard before generating",
    )
    gen_parser.add_argument(
        "--no-scrub-specs",
        dest="scrub_specs",
        action="store_false",
        help="Retain existing spec files before generating",
    )
    gen_parser.set_defaults(popout=None, scrub_specs=None)
    gen_parser.add_argument(
        "--popout-linger",
        type=float,
        default=None,
        help="Seconds to keep the HUD popout open after completion (default 5s)",
    )
    gen_verbose = gen_parser.add_mutually_exclusive_group()
    gen_verbose.add_argument(
        "--verbose", action="store_true", help="Print Codex diffs (default)"
    )
    gen_verbose.add_argument(
        "--quiet", action="store_true", help="Suppress Codex diff output"
    )

    # discriminator
    disc_parser = sub.add_parser("discriminator", help="Run the automation ladder")
    disc_mode = disc_parser.add_mutually_exclusive_group()
    disc_mode.add_argument(
        "--feature-only", action="store_true", help="Run only the active feature shard"
    )
    disc_mode.add_argument(
        "--global", dest="global_only", action="store_true", help="Run the global sweep"
    )
    disc_llm = disc_parser.add_mutually_exclusive_group()
    disc_llm.add_argument(
        "--enable-llm", action="store_true", help="Allow guarded runtime edits via LLM"
    )
    disc_llm.add_argument(
        "--disable-llm", action="store_true", help="Disable LLM runtime edits (default)"
    )
    disc_parser.add_argument(
        "--single-pass", action="store_true", help="Run one pass and stop"
    )
    disc_parser.add_argument(
        "--max-passes", type=int, default=None, help="Maximum passes before giving up"
    )
    disc_parser.add_argument(
        "--feature", dest="feature_slug", help="Override feature slug"
    )
    disc_verbose = disc_parser.add_mutually_exclusive_group()
    disc_verbose.add_argument(
        "--verbose",
        action="store_true",
        help="Print discriminator debug output (default)",
    )
    disc_verbose.add_argument(
        "--quiet", action="store_true", help="Reduce discriminator verbosity"
    )
    disc_parser.add_argument(
        "--tail",
        type=int,
        default=0,
        help="Tail log output (N lines) when the discriminator fails",
    )
    disc_parser.add_argument(
        "--stage-timeout",
        type=int,
        default=None,
        help="Timeout (seconds) for each discriminator stage",
    )

    # loop
    loop_parser = sub.add_parser("loop", help="Generator → discriminator orchestration")
    loop_parser.add_argument("--skip-generator", action="store_true")
    loop_parser.add_argument("--generator-only", action="store_true")
    loop_parser.add_argument("--discriminator-only", action="store_true")
    loop_parser.add_argument("--skip-feature", action="store_true")
    loop_parser.add_argument("--skip-global", action="store_true")
    loop_parser.add_argument("--feature-only", action="store_true")
    loop_parser.add_argument("--global-only", action="store_true")
    loop_parser.add_argument("--include-accepted", action="store_true")
    loop_parser.add_argument("--status", dest="statuses", default=None)
    loop_parser.add_argument(
        "--each",
        action="store_true",
        help="Process each matching Feature Card sequentially",
    )
    loop_parser.add_argument(
        "--no-self-update", action="store_true", help="Skip self-update before running"
    )
    loop_parser.add_argument(
        "--explain", action="store_true", help="Describe planned actions and exit"
    )
    loop_parser.add_argument(
        "--ui",
        choices=["monitor", "snapshot", "off", "auto", "popout"],
        default=None,
        help="Override generator HUD mode (default inherits from environment)",
    )
    loop_verbose = loop_parser.add_mutually_exclusive_group()
    loop_verbose.add_argument(
        "--verbose",
        action="store_true",
        help="Print generator/discriminator debug output (default)",
    )
    loop_verbose.add_argument(
        "--quiet", action="store_true", help="Reduce generator/discriminator output"
    )
    loop_parser.add_argument(
        "--tail", type=int, default=0, help="Tail log output (N lines) after failures"
    )
    loop_parser.add_argument(
        "--popout",
        dest="popout",
        action="store_true",
        help="Launch generator HUD in a separate terminal window when possible",
    )
    loop_parser.add_argument(
        "--no-popout",
        dest="popout",
        action="store_false",
        help="Disable generator HUD popout behaviour",
    )
    loop_parser.add_argument(
        "--scrub-specs",
        dest="scrub_specs",
        action="store_true",
        help="Delete the spec shard before generating",
    )
    loop_parser.add_argument(
        "--no-scrub-specs",
        dest="scrub_specs",
        action="store_false",
        help="Retain existing spec files before generating",
    )
    loop_parser.set_defaults(popout=None, scrub_specs=None)
    loop_parser.add_argument(
        "--popout-linger",
        type=float,
        default=None,
        help="Seconds to keep the HUD popout open after completion (default 5s)",
    )
    loop_llm = loop_parser.add_mutually_exclusive_group()
    loop_llm.add_argument(
        "--enable-llm", action="store_true", help="Allow guarded runtime edits via LLM"
    )
    loop_llm.add_argument(
        "--disable-llm", action="store_true", help="Disable LLM runtime edits"
    )
    loop_parser.add_argument(
        "--stage-timeout",
        type=int,
        default=None,
        help="Timeout (seconds) applied to discriminator stages",
    )
    loop_parser.add_argument(
        "--continue-on-fail",
        action="store_true",
        help="Process remaining Feature Cards even if one fails",
    )

    # card commands
    card_parser = sub.add_parser("card", help="Feature Card helpers")
    card_sub = card_parser.add_subparsers(dest="card_command")
    card_new = card_sub.add_parser("new", help="Create a new Feature Card")
    card_new.add_argument(
        "slug", nargs="?", help="Slug for the card (derived from title if omitted)"
    )
    card_new.add_argument("--title", help="Human-friendly title")
    card_new.add_argument("--summary", help="Summary paragraph")
    card_new.add_argument(
        "--acceptance",
        action="append",
        default=[],
        help="Acceptance criteria bullet (use multiple times)",
    )

    card_list = card_sub.add_parser("list", help="List Feature Cards")
    card_list.add_argument(
        "--status",
        dest="statuses",
        default=None,
        help="Comma separated statuses to filter",
    )

    card_sub.add_parser("validate", help="Validate Feature Card formatting")

    card_rename = card_sub.add_parser(
        "rename", help="Rename a Feature Card and its spec shard"
    )
    card_rename.add_argument("old_slug")
    card_rename.add_argument("new_slug")

    card_split_parser = card_sub.add_parser(
        "split", help="Split a Feature Card into two new cards"
    )
    card_split_parser.add_argument("source_slug")
    card_split_parser.add_argument("slug_a")
    card_split_parser.add_argument("slug_b")

    card_archive_parser = card_sub.add_parser(
        "archive", help="Mark a Feature Card as archived (status: archived)"
    )
    card_archive_parser.add_argument("slug")

    card_prune = card_sub.add_parser(
        "prune-specs", help="Remove orphan or archived spec shards"
    )
    card_prune.add_argument(
        "--yes", action="store_true", help="Delete without confirmation prompts"
    )
    arch_group = card_prune.add_mutually_exclusive_group()
    arch_group.add_argument(
        "--archived",
        dest="include_archived",
        action="store_true",
        help="Include archived cards (default)",
    )
    arch_group.add_argument(
        "--no-archived",
        dest="include_archived",
        action="store_false",
        help="Skip archived cards",
    )
    card_prune.set_defaults(include_archived=True)

    # logs & status
    logs_parser = sub.add_parser(
        "logs", help="Tail recent discriminator/generator logs"
    )
    logs_parser.add_argument(
        "--generator", action="store_true", help="Show generator logs"
    )
    logs_parser.add_argument(
        "--discriminator", action="store_true", help="Show discriminator logs"
    )
    logs_parser.add_argument(
        "--lines", type=int, default=120, help="Number of log lines to display"
    )
    logs_parser.add_argument(
        "--follow", action="store_true", help="Stream log output until interrupted"
    )
    status_parser = sub.add_parser("status", help="Show rex-codex status summary")
    status_parser.add_argument(
        "--json", action="store_true", help="Emit raw JSON summary"
    )

    hud_parser = sub.add_parser(
        "hud", help="Render a one-shot HUD snapshot from event streams"
    )
    hud_parser.add_argument(
        "phase", choices=["generator", "discriminator"], help="HUD phase to render"
    )
    hud_parser.add_argument(
        "--slug", help="Feature slug to focus (defaults to active card)"
    )
    hud_parser.add_argument("--events", help="Override events JSONL path")
    hud_parser.add_argument(
        "--follow",
        action="store_true",
        help="Continuously refresh the HUD until completion (generator only)",
    )
    hud_parser.add_argument(
        "--refresh",
        type=float,
        default=1.0,
        help="Refresh interval in seconds when using --follow",
    )
    hud_parser.add_argument(
        "--linger",
        type=float,
        default=5.0,
        help="Seconds to keep rendering after completion when using --follow",
    )

    # doctor
    sub.add_parser("doctor", help="Print environment diagnostics")

    # burn/uninstall
    burn_parser = sub.add_parser(
        "burn", help="Reset repository contents (preserve .git)"
    )
    burn_parser.add_argument("--yes", action="store_true", help="Skip confirmation")
    burn_parser.add_argument(
        "--purge-agent", action="store_true", help="Remove .rex_agent as well"
    )
    burn_parser.add_argument(
        "--dry-run", action="store_true", help="Preview deletions without executing"
    )

    uninstall_parser = sub.add_parser("uninstall", help="Remove the rex-codex agent")
    uninstall_parser.add_argument(
        "--yes", "--force", action="store_true", dest="force", help="Skip confirmation"
    )
    uninstall_parser.add_argument(
        "--keep-wrapper", action="store_true", help="Preserve the ./rex-codex wrapper"
    )

    # self-update
    update_parser = sub.add_parser("self-update", help="Force an agent self-update")
    update_parser.add_argument(
        "--channel", choices=["stable", "main"], default=None, help="Update channel"
    )

    return parser


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    context = RexContext.discover()

    if args.command == "install":
        run_install(
            force=args.force,
            channel=args.channel,
            run_init_after=not args.skip_init,
            run_doctor_after=not args.skip_doctor,
            context=context,
        )
        return 0

    if args.command == "init":
        run_init(context=context, perform_self_update=not args.no_self_update)
        return 0

    if args.command == "generator":
        options = GeneratorOptions()
        if args.single_pass:
            options.continuous = False
        if args.max_passes is not None:
            options.max_passes = args.max_passes
        options.focus = args.focus
        if args.statuses:
            options.statuses = parse_statuses(args.statuses)
        elif args.include_accepted:
            options.statuses.append("accepted")
        if args.card:
            options.card_path = Path(args.card)
        options.iterate_all = args.each
        options.verbose = not args.quiet
        if args.verbose:
            options.verbose = True
        options.tail_lines = args.tail
        options.reconcile_only = args.reconcile
        if args.ui:
            options.ui_mode = "monitor" if args.ui == "auto" else args.ui
        if args.popout is not None:
            options.spawn_popout = args.popout
        if args.popout_linger is not None:
            options.popout_linger = args.popout_linger
        if args.scrub_specs is not None:
            options.scrub_specs = args.scrub_specs
        exit_code = run_generator(options, context=context)
        if exit_code != 0 and args.tail:
            show_latest_logs(context, lines=args.tail, generator=True)
        return exit_code

    if args.command == "discriminator":
        options = DiscriminatorOptions()
        if args.feature_only:
            options.mode = "feature"
        elif args.global_only:
            options.mode = "global"
        if args.single_pass:
            options.continuous = False
        if args.max_passes is not None:
            options.max_passes = args.max_passes
        if args.feature_slug:
            options.slug = args.feature_slug
        if args.enable_llm:
            options.disable_llm = False
        elif args.disable_llm:
            options.disable_llm = True
        options.verbose = not args.quiet
        if args.verbose:
            options.verbose = True
        if args.stage_timeout is not None:
            options.stage_timeout = args.stage_timeout
        exit_code = run_discriminator(options, context=context)
        if exit_code != 0 and args.tail:
            show_latest_logs(context, lines=args.tail, discriminator=True)
        return exit_code

    if args.command == "loop":
        loop_opts = LoopOptions()
        if args.skip_generator:
            loop_opts.run_generator = False
        if args.generator_only:
            loop_opts.run_discriminator = False
        if args.discriminator_only:
            loop_opts.run_generator = False
        if args.skip_feature:
            loop_opts.run_feature = False
        if args.skip_global:
            loop_opts.run_global = False
        if args.feature_only:
            loop_opts.run_feature = True
            loop_opts.run_global = False
        if args.global_only:
            loop_opts.run_feature = False
            loop_opts.run_global = True
        if args.statuses:
            loop_opts.generator_options.statuses = parse_statuses(args.statuses)
        elif args.include_accepted:
            statuses = loop_opts.generator_options.statuses
            if "accepted" not in statuses:
                statuses.append("accepted")
        loop_opts.each_features = args.each
        loop_opts.perform_self_update = not args.no_self_update
        loop_opts.explain = args.explain
        loop_opts.verbose = not args.quiet
        if args.verbose:
            loop_opts.verbose = True
        loop_opts.tail_lines = args.tail
        if args.enable_llm:
            loop_opts.discriminator_options.disable_llm = False
        elif args.disable_llm:
            loop_opts.discriminator_options.disable_llm = True
        if args.ui:
            loop_opts.generator_options.ui_mode = (
                "monitor" if args.ui == "auto" else args.ui
            )
        if args.popout is not None:
            loop_opts.generator_options.spawn_popout = args.popout
        if args.popout_linger is not None:
            loop_opts.generator_options.popout_linger = args.popout_linger
        if args.scrub_specs is not None:
            loop_opts.generator_options.scrub_specs = args.scrub_specs
        if args.stage_timeout is not None:
            loop_opts.discriminator_options.stage_timeout = args.stage_timeout
        loop_opts.continue_on_fail = args.continue_on_fail
        return run_loop(loop_opts, context=context)

    if args.command == "card":
        if args.card_command == "new":
            slug = args.slug
            title = args.title or prompt("Title: ")
            slug = slug or sanitise_slug(title)
            summary = args.summary or prompt("Summary: ")
            acceptance = args.acceptance or []
            if not acceptance:
                print("Enter acceptance criteria (blank line to finish):")
                while True:
                    item = prompt("- ")
                    if not item.strip():
                        break
                    acceptance.append(item.strip())
            card = create_card(
                context, slug=slug, title=title, summary=summary, acceptance=acceptance
            )
            print(f"[card] Created {card.path}")
            return 0
        if args.card_command == "list":
            statuses = parse_statuses(args.statuses) if args.statuses else None
            cards = discover_cards(statuses=statuses, context=context)
            if not cards:
                print("[card] No Feature Cards found.")
                return 0
            for card in cards:
                print(f"{card.status:>9}  {card.slug}  {card.path}")
            return 0
        if args.card_command == "validate":
            errors = lint_all_cards(context)
            if not errors:
                print("[card] All Feature Cards look good.")
                return 0
            for error in errors:
                print(error)
            return 1
        if args.card_command == "rename":
            card = rename_card(context, args.old_slug, args.new_slug)
            spec_dir = spec_directory(context, card.slug)
            print(f"[card] Renamed Feature Card → {card.slug} ({card.path})")
            if spec_dir.exists():
                print(f"[card] Spec shard relocated to {spec_dir}")
            return 0
        if args.card_command == "split":
            card_a, card_b = split_card(
                context, args.source_slug, args.slug_a, args.slug_b
            )
            print(f"[card] Created {card_a.slug} ({card_a.path})")
            print(f"[card] Created {card_b.slug} ({card_b.path})")
            print(
                "[card] Review acceptance criteria for each new card and adjust tests as needed."
            )
            return 0
        if args.card_command == "archive":
            card = archive_card(context, args.slug)
            print(f"[card] Updated {card.path} to status: {card.status}")
            return 0
        if args.card_command == "prune-specs":
            removed = prune_spec_directories(
                context,
                include_archived=args.include_archived,
                assume_yes=args.yes,
            )
            if not removed:
                print("[card prune-specs] No spec shards removed.")
            return 0
        parser.error(
            "card requires a sub-command (new/list/validate/rename/split/archive/prune-specs)"
        )

    if args.command == "logs":
        show_latest_logs(
            context,
            lines=args.lines,
            generator=args.generator,
            discriminator=args.discriminator,
            follow=args.follow,
        )
        return 0

    if args.command == "status":
        render_status(context, json_output=getattr(args, "json", False))
        return 0

    if args.command == "doctor":
        run_doctor()
        return 0

    if args.command == "hud":
        from .hud import render_hud

        render_hud(
            phase=args.phase,
            slug=args.slug,
            events_file=args.events,
            context=context,
            follow=args.follow,
            refresh=args.refresh,
            linger=args.linger,
        )
        return 0

    if args.command == "burn":
        burn_repo(
            force=args.yes,
            purge_agent=args.purge_agent,
            dry_run=args.dry_run,
            context=context,
        )
        return 0

    if args.command == "uninstall":
        uninstall_agent(
            force=args.force, keep_wrapper=args.keep_wrapper, context=context
        )
        return 0

    if args.command == "self-update":
        self_update(channel=args.channel)
        return 0

    parser.print_help()
    return 1


def app() -> None:  # pragma: no cover - Typer compatibility shim
    raise SystemExit(main())


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_global/install.py ===
"""Install or re-install the rex-codex agent."""

from __future__ import annotations

import os
import subprocess

from ..scope_project.config import AGENT_SRC
from ..scope_project.doctor import run_doctor
from ..scope_project.init import run_init
from ..scope_project.utils import RexContext, RexError


def run_install(
    *,
    force: bool = False,
    channel: str | None = None,
    run_init_after: bool = True,
    run_doctor_after: bool = True,
    context: RexContext | None = None,
) -> None:
    """Invoke the bundled install script to (re)install the agent."""
    context = context or RexContext.discover()
    script = AGENT_SRC / "packaging" / "install.sh"
    if not script.exists():
        raise RexError(f"Install script not found: {script}")

    cmd = ["bash", str(script)]
    if force:
        cmd.append("--force")
    if channel:
        cmd.extend(["--channel", channel])

    env = os.environ.copy()
    if channel:
        env["REX_AGENT_CHANNEL"] = channel
    env["REX_AGENT_SKIP_INIT"] = "1"
    env["REX_AGENT_SKIP_DOCTOR"] = "1"
    completed = subprocess.run(cmd, cwd=context.root, env=env)
    if completed.returncode != 0:
        raise RexError(f"Install command failed with exit code {completed.returncode}")

    if run_init_after:
        run_init(context=context, perform_self_update=False)
    if run_doctor_after:
        run_doctor()

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_global/self_update.py ===
"""Agent self-update helpers."""

from __future__ import annotations

import os

from ..scope_project.config import AGENT_SRC
from ..scope_project.utils import run


def self_update(channel: str | None = None) -> None:
    """Mirror the legacy Bash self-update strategy."""
    if os.environ.get("REX_AGENT_NO_UPDATE", "1") == "1" and channel is None:
        return

    src = AGENT_SRC
    if not (src / ".git").exists():
        # Nothing to update; installation likely incomplete.
        return

    run(
        ["git", "-C", str(src), "fetch", "--all", "--tags", "--prune", "--force"],
        check=False,
    )

    channel = channel or os.environ.get("REX_AGENT_CHANNEL", "stable")
    if channel == "stable":
        completed = run(
            ["git", "-C", str(src), "tag", "--sort=-v:refname"],
            capture_output=True,
            check=False,
        )
        tags = [line.strip() for line in completed.stdout.splitlines() if line.strip()]
        target = tags[0] if tags else "main"
        run(["git", "-C", str(src), "checkout", "-q", target], check=False)
    elif channel == "main":
        run(["git", "-C", str(src), "checkout", "-q", "main"], check=False)
        run(["git", "-C", str(src), "pull", "--ff-only"], check=False)
    else:
        run(["git", "-C", str(src), "checkout", "-q", channel], check=False)

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_global/uninstall.py ===
"""Implementation of `rex-codex uninstall`."""

from __future__ import annotations

import shutil

from ..scope_project.utils import RexContext, ask_confirmation


def uninstall_agent(
    *, force: bool, keep_wrapper: bool, context: RexContext | None = None
) -> None:
    context = context or RexContext.discover()
    root = context.root
    agent_dir = root / ".rex_agent"
    wrapper = root / "rex-codex"

    if not force:
        print("This will remove the Codex agent from:")
        print(f"  - {agent_dir}")
        if keep_wrapper:
            print("  - (wrapper preserved due to --keep-wrapper)")
        else:
            print(f"  - {wrapper}")
        if not ask_confirmation(
            "Type 'remove agent' to continue: ", expected="remove agent"
        ):
            print("[uninstall] Aborted.")
            return

    if agent_dir.exists():
        shutil.rmtree(agent_dir)
        print(f"[uninstall] Removed {agent_dir}")
    else:
        print("[uninstall] No .rex_agent directory found; nothing to remove.")

    if not keep_wrapper and wrapper.exists():
        wrapper.unlink()
        print(f"[uninstall] Removed {wrapper}")

    print(
        "[uninstall] Agent uninstalled. Remove guardrail artefacts manually if desired."
    )

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/__init__.py ===
"""Per-project runtime scope exports."""

from __future__ import annotations

from importlib import import_module

__all__ = [
    "burn",
    "cards",
    "component_planner",
    "config",
    "discriminator",
    "doctor",
    "events",
    "generator",
    "generator_ui",
    "hermetic",
    "hud",
    "init",
    "logs",
    "loop",
    "monitoring",
    "playbook",
    "self_update",
    "status",
    "utils",
]


def __getattr__(name: str):
    if name in __all__:
        module = import_module(f"{__name__}.{name}")
        globals()[name] = module
        return module
    raise AttributeError(name)

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/burn.py ===
"""Implementation of `rex-codex burn`."""

from __future__ import annotations

import shutil

from .utils import RexContext, ask_confirmation, ensure_dir


def burn_repo(
    *,
    force: bool,
    purge_agent: bool,
    dry_run: bool,
    context: RexContext | None = None,
) -> None:
    context = context or RexContext.discover()
    root = context.root
    print(f"WARNING: This will delete repository files in {root}")
    if purge_agent:
        print("  - .rex_agent will be removed")
    else:
        print("  - .rex_agent will be preserved")
    print("  - .git directory is always preserved")

    if dry_run:
        print("[burn] Dry-run mode: no files will be deleted.")
    elif not force:
        if not ask_confirmation(
            "Type 'burn it down' to continue: ", expected="burn it down"
        ):
            print("[burn] Aborted.")
            return

    entries = list(root.iterdir())
    for entry in entries:
        name = entry.name
        if name in {".", ".."}:
            continue
        if name in {".git", "rex-codex"}:
            continue
        if name == ".rex_agent" and not purge_agent:
            continue
        if dry_run:
            print(f"[dry-run] would remove: {entry}")
            continue
        if entry.is_dir():
            shutil.rmtree(entry)
        else:
            entry.unlink()

    if dry_run:
        print("[✓] Dry-run complete. No files were removed.")
        return

    ensure_dir(root)
    print("[✓] Repository reset. Re-run ./rex-codex init to seed fresh scaffolding.")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/cards.py ===
"""Feature card helpers."""

from __future__ import annotations

import hashlib
import re
import shutil
import sys
from dataclasses import dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

from .utils import RexContext, dump_json, ensure_dir, load_json, prompt, repo_root, run

CARD_DIR = Path("documents/feature_cards")
CARD_FILENAME_RE = re.compile(r"^[a-z0-9][a-z0-9_-]*$")
STATUS_RE = re.compile(r"^[ \t]*status:[ \t]*([A-Za-z0-9_.-]+)", re.IGNORECASE)
SPEC_ROOT = Path("tests/feature_specs")


def card_path_for(context: RexContext, slug: str) -> Path:
    return card_directory(context) / f"{slug}.md"


def read_card_sections(path: Path) -> Dict[str, object]:
    text = path.read_text(encoding="utf-8")
    lines = text.splitlines()
    title = path.stem.replace("-", " ").title()
    for line in lines:
        if line.startswith("# "):
            title = line[2:].strip()
            break
    current_section: Optional[str] = None
    summary_lines: List[str] = []
    acceptance: List[str] = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith("## "):
            current_section = stripped.lower()
            continue
        if current_section == "## summary":
            summary_lines.append(line.rstrip())
        elif current_section == "## acceptance criteria":
            if stripped.startswith("- "):
                acceptance.append(stripped[2:].strip())
    summary = "\n".join([line for line in summary_lines if line.strip()]).strip()
    return {"title": title, "summary": summary, "acceptance": acceptance}


def spec_directory(context: RexContext, slug: str) -> Path:
    return context.root / SPEC_ROOT / slug


def card_content_hash(path: Path) -> Optional[str]:
    if not path.exists():
        return None
    digest = hashlib.sha256()
    with path.open("rb") as fh:
        for chunk in iter(lambda: fh.read(8192), b""):
            digest.update(chunk)
    return digest.hexdigest()


def _list_test_functions(path: Path) -> List[str]:
    try:
        source = path.read_text(encoding="utf-8")
    except OSError:
        return []
    import ast

    try:
        tree = ast.parse(source, filename=str(path))
    except SyntaxError:
        return []
    names: List[str] = []
    for node in tree.body:
        if isinstance(
            node, (ast.FunctionDef, ast.AsyncFunctionDef)
        ) and node.name.startswith("test"):
            names.append(node.name)
    return names


@dataclass
class FeatureCard:
    path: Path
    slug: str
    status: str

    @property
    def relative_path(self) -> Path:
        root = repo_root()
        try:
            return self.path.relative_to(root)
        except ValueError:
            return self.path


def card_directory(context: RexContext | None = None) -> Path:
    context = context or RexContext.discover()
    return context.root / CARD_DIR


def slug_from_filename(path: Path) -> str:
    stem = path.stem.lower()
    return stem


def read_status(path: Path) -> str:
    try:
        text = path.read_text(encoding="utf-8")
    except FileNotFoundError:
        return "missing"
    for line in text.splitlines():
        match = STATUS_RE.match(line)
        if match:
            return match.group(1).lower()
    return "unknown"


def discover_cards(
    statuses: Iterable[str] | None = None,
    *,
    context: RexContext | None = None,
) -> List[FeatureCard]:
    context = context or RexContext.discover()
    directory = card_directory(context)
    if not directory.exists():
        return []
    normalized_statuses = {s.lower() for s in (statuses or [])}
    matches: List[FeatureCard] = []
    for path in sorted(directory.glob("*.md")):
        slug = slug_from_filename(path)
        status = read_status(path)
        if normalized_statuses and status not in normalized_statuses:
            continue
        matches.append(FeatureCard(path, slug, status))
    return matches


def latest_card(statuses: Sequence[str] | None = None) -> Optional[FeatureCard]:
    cards = discover_cards(statuses)
    return cards[0] if cards else None


def load_rex_agent(context: RexContext | None = None) -> dict:
    context = context or RexContext.discover()
    return load_json(context.rex_agent_file)


def update_active_card(context: RexContext, *, card: FeatureCard | None) -> None:
    data = load_json(context.rex_agent_file)
    feature = data.setdefault("feature", {})
    if card:
        feature["active_card"] = str(card.relative_path)
        feature["active_slug"] = card.slug
    else:
        feature["active_card"] = None
        feature["active_slug"] = None
    dump_json(context.rex_agent_file, data)


def sanitise_slug(raw: str) -> str:
    slug = re.sub(r"[^a-z0-9_-]+", "-", raw.lower())
    slug = re.sub(r"-{2,}", "-", slug)
    slug = slug.strip("-_")
    slug = re.sub(r"^[^a-z0-9]+", "", slug)
    if not slug:
        slug = f"feature-{datetime.now(UTC):%Y%m%d%H%M%S}"
    return slug


def validate_slug(slug: str) -> None:
    if not slug:
        raise ValueError("slug cannot be empty")
    if not CARD_FILENAME_RE.match(slug):
        raise ValueError(
            "slug must contain lowercase letters, digits, hyphen, or underscore; "
            f"got {slug!r}"
        )


def create_card(
    context: RexContext,
    *,
    slug: str,
    title: str,
    summary: str,
    acceptance: Sequence[str],
) -> FeatureCard:
    validate_slug(slug)
    directory = card_directory(context)
    directory.mkdir(parents=True, exist_ok=True)
    path = directory / f"{slug}.md"
    if path.exists():
        raise FileExistsError(f"Feature Card already exists: {path}")
    body_lines = [
        "status: proposed",
        "",
        f"# {title.strip()}",
        "",
        "## Summary",
        "",
        summary.strip(),
        "",
        "## Acceptance Criteria",
    ]
    if acceptance:
        body_lines.append("")
        for item in acceptance:
            item = item.strip()
            if not item:
                continue
            if not item.startswith("- "):
                body_lines.append(f"- {item}")
            else:
                body_lines.append(item)
    else:
        body_lines.append("")
        body_lines.append("- TBD")
    body_lines.extend(
        [
            "",
            "## Links",
            "",
            "## Spec Trace",
            "",
        ]
    )
    path.write_text("\n".join(body_lines) + "\n", encoding="utf-8")
    card = FeatureCard(path=path, slug=slug, status="proposed")
    update_active_card(context, card=card)
    return card


def lint_card(path: Path) -> List[str]:
    errors: List[str] = []
    if not path.exists():
        return [f"{path}: missing file"]
    text = path.read_text(encoding="utf-8").splitlines()
    status_lines = [ln for ln in text if ln.lower().startswith("status:")]
    if not status_lines:
        errors.append(f"{path}: missing `status:` line")
    elif len(status_lines) > 1:
        errors.append(f"{path}: more than one `status:` line detected")
    headers = [ln.strip() for ln in text if ln.strip().startswith("## ")]
    expected = {"## Summary", "## Acceptance Criteria", "## Links", "## Spec Trace"}
    missing = expected.difference(headers)
    for header in sorted(missing):
        errors.append(f"{path}: missing header {header!r}")
    return errors


def lint_all_cards(context: RexContext | None = None) -> List[str]:
    context = context or RexContext.discover()
    directory = card_directory(context)
    errors: List[str] = []
    if not directory.exists():
        return ["No Feature Cards found; run `rex-codex card new` first."]
    for card in discover_cards(context=context):
        errors.extend(lint_card(card.path))
    return errors


def rename_card(context: RexContext, old_slug: str, new_slug: str) -> FeatureCard:
    validate_slug(new_slug)
    directory = card_directory(context)
    old_path = directory / f"{old_slug}.md"
    if not old_path.exists():
        raise FileNotFoundError(f"Feature Card not found: {old_path}")
    new_path = directory / f"{new_slug}.md"
    if new_path.exists():
        raise FileExistsError(f"Target Feature Card already exists: {new_path}")

    ensure_dir(new_path.parent)
    old_path.rename(new_path)

    old_spec = spec_directory(context, old_slug)
    new_spec = spec_directory(context, new_slug)
    if old_spec.exists():
        ensure_dir(new_spec.parent)
        if new_spec.exists():
            raise FileExistsError(f"Target spec directory already exists: {new_spec}")
        old_spec.rename(new_spec)

    data = load_json(context.rex_agent_file)
    feature = data.setdefault("feature", {})
    if feature.get("active_slug") == old_slug:
        feature["active_slug"] = new_slug
        feature["active_card"] = str(new_path.relative_to(context.root))
    dump_json(context.rex_agent_file, data)

    return FeatureCard(path=new_path, slug=new_slug, status=read_status(new_path))


def archive_card(
    context: RexContext, slug: str, *, status: str = "archived"
) -> FeatureCard:
    path = card_path_for(context, slug)
    if not path.exists():
        raise FileNotFoundError(f"Feature Card not found: {path}")
    lines = path.read_text(encoding="utf-8").splitlines()
    replaced = False
    new_lines: List[str] = []
    for line in lines:
        if STATUS_RE.match(line):
            new_lines.append(f"status: {status}")
            replaced = True
        else:
            new_lines.append(line)
    if not replaced:
        raise ValueError(f"{path} does not contain a status line")
    path.write_text("\n".join(new_lines) + "\n", encoding="utf-8")
    return FeatureCard(path=path, slug=slug, status=status)


def split_card(
    context: RexContext,
    source_slug: str,
    slug_a: str,
    slug_b: str,
) -> Tuple[FeatureCard, FeatureCard]:
    directory = card_directory(context)
    source_path = directory / f"{source_slug}.md"
    if not source_path.exists():
        raise FileNotFoundError(f"Feature Card not found: {source_path}")
    validate_slug(slug_a)
    validate_slug(slug_b)
    meta = read_card_sections(source_path)
    title = meta.get("title", slug_a.replace("-", " ").title())
    summary = meta.get("summary", "")
    acceptance = [str(item) for item in meta.get("acceptance", [])]

    card_a = create_card(
        context, slug=slug_a, title=title, summary=summary, acceptance=acceptance
    )
    card_b = create_card(
        context, slug=slug_b, title=title, summary=summary, acceptance=acceptance
    )

    source_spec = spec_directory(context, source_slug)
    if source_spec.exists():
        ensure_dir(spec_directory(context, slug_a))
        ensure_dir(spec_directory(context, slug_b))
        for path in sorted(source_spec.glob("*.py")):
            tests = _list_test_functions(path)
            test_display = ", ".join(tests) if tests else "(no tests discovered)"
            if not sys.stdin.isatty():
                choice = "k"
            else:
                prompt_msg = (
                    f"[card split] Move {path.relative_to(context.root)} "
                    f"(tests: {test_display}) to (a/b/k[eep]): "
                )
                raw = prompt(prompt_msg)
                choice = raw.strip().lower()[:1] if raw else "k"
                if choice not in {"a", "b"}:
                    choice = "k"
            if choice == "a":
                dest = spec_directory(context, slug_a) / path.name
            elif choice == "b":
                dest = spec_directory(context, slug_b) / path.name
            else:
                continue
            if dest.exists():
                raise FileExistsError(f"Destination already contains {dest}")
            shutil.move(str(path), str(dest))
            print(
                f"[card split] Moved {path.relative_to(context.root)} → {dest.relative_to(context.root)}"
            )
        # Remove the source directory if empty after moves
        if not any(source_spec.iterdir()):
            source_spec.rmdir()

    return card_a, card_b


def _git_path_dirty(context: RexContext, path: Path) -> bool:
    completed = run(
        ["git", "status", "--short", "--", str(path)],
        cwd=context.root,
        capture_output=True,
        check=False,
    )
    return bool((completed.stdout or "").strip())


def prune_spec_directories(
    context: RexContext,
    *,
    include_archived: bool = True,
    assume_yes: bool = False,
) -> List[Path]:
    specs_root = context.root / SPEC_ROOT
    if not specs_root.exists():
        return []
    cards = {card.slug: card.status for card in discover_cards(context=context)}
    targets: List[Path] = []
    for path in sorted(specs_root.iterdir()):
        if not path.is_dir():
            continue
        slug = path.name
        if slug not in cards:
            targets.append(path)
            continue
        if include_archived and cards[slug].lower() == "archived":
            targets.append(path)
    removed: List[Path] = []
    for path in targets:
        rel = path.relative_to(context.root)
        if _git_path_dirty(context, path):
            print(f"[card prune-specs] Skipping {rel} (git reports modifications).")
            continue
        if not assume_yes:
            response = (
                prompt(f"[card prune-specs] Delete {rel}? [y/N]: ").strip().lower()
            )
            if response not in {"y", "yes"}:
                continue
        shutil.rmtree(path)
        removed.append(path)
        print(f"[card prune-specs] Removed {rel}")
    return removed


def find_orphan_spec_slugs(context: RexContext) -> List[str]:
    specs_root = context.root / SPEC_ROOT
    if not specs_root.exists():
        return []
    existing = {card.slug for card in discover_cards(context=context)}
    orphans: List[str] = []
    for path in sorted(specs_root.iterdir()):
        if not path.is_dir():
            continue
        slug = path.name
        if slug not in existing:
            orphans.append(slug)
    return orphans

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/component_planner.py ===
"""Structured component planning for Feature Cards prior to spec generation."""

from __future__ import annotations

import hashlib
import json
import shlex
import subprocess
import time
import uuid
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Mapping, Optional, Sequence, Tuple

from .cards import FeatureCard
from .events import emit_event
from .utils import RexContext


@dataclass
class PlannerResult:
    plan: Dict[str, Any]
    path: Path


def ensure_component_plan(
    *,
    card: FeatureCard,
    context: RexContext,
    codex_bin: str,
    codex_flags: str,
    codex_model: str,
    verbose: bool = True,
) -> PlannerResult:
    """Build (or reuse) the component/subcomponent/test map for a Feature Card."""

    card_path = card.path
    slug = card.slug
    card_hash = _hash_path(card_path)
    plan_path = context.codex_ci_dir / f"component_plan_{slug}.json"

    if plan_path.exists():
        try:
            cached = json.loads(plan_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            cached = None
        if cached and cached.get("card_hash") == card_hash:
            return PlannerResult(plan=cached, path=plan_path)

    if verbose:
        print(f"[planner] Generating component plan for {slug}")
    emit_event(
        "generator",
        "component_plan_started",
        slug=slug,
        task=f"plan/{slug}",
        card_path=str(card_path),
    )

    base_plan: Dict[str, Any] = {
        "card_path": str(card_path),
        "card_hash": card_hash,
        "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "status": "in_progress",
        "components": [],
    }

    playbook_json = context.codex_ci_dir / f"playbook_{slug}.json"
    if playbook_json.exists():
        try:
            playbook_payload = json.loads(playbook_json.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            playbook_payload = {"error": "invalid_playbook_json"}
        base_plan["playbook_snapshot"] = playbook_payload

    _emit_plan_snapshot(slug, base_plan)

    card_text = card_path.read_text(encoding="utf-8")
    other_cards = _collect_other_cards(card.path.parent, exclude=card_path)

    components_payload = _run_codex_json(
        label="component-overview",
        prompt=_component_prompt(slug, card_text, other_cards),
        slug=slug,
        context=context,
        codex_bin=codex_bin,
        codex_flags=codex_flags,
        codex_model=codex_model,
        verbose=verbose,
    )
    components = components_payload.get("components") or []

    for index, component in enumerate(components, start=1):
        comp_name = component.get("name") or f"Component {index}"
        comp_uid = component.get("id") or f"{slug}-c{index}-{uuid.uuid4().hex[:6]}"
        comp_entry: Dict[str, Any] = {
            "id": comp_uid,
            "name": comp_name,
            "summary": component.get("summary") or "",
            "rationale": component.get("rationale") or "",
            "notes": component.get("notes") or "",
            "subcomponents": [],
        }
        base_plan["components"].append(comp_entry)
        _emit_plan_snapshot(slug, base_plan)
        emit_event(
            "generator",
            "component_plan_component_started",
            slug=slug,
            task=f"plan/{slug}",
            component=comp_name,
            component_index=index,
        )

        sub_payload = _run_codex_json(
            label=f"subcomponents::{comp_name}",
            prompt=_subcomponent_prompt(
                slug=slug,
                card_text=card_text,
                component=comp_entry,
            ),
            slug=slug,
            context=context,
            codex_bin=codex_bin,
            codex_flags=codex_flags,
            codex_model=codex_model,
            verbose=verbose,
        )
        subcomponents = sub_payload.get("subcomponents") or []
        for sub_index, sub in enumerate(subcomponents, start=1):
            sub_name = sub.get("name") or f"{comp_name} :: Subcomponent {sub_index}"
            sub_uid = sub.get("id") or f"{comp_uid}-s{sub_index}-{uuid.uuid4().hex[:6]}"
            sub_entry: Dict[str, Any] = {
                "id": sub_uid,
                "name": sub_name,
                "summary": sub.get("summary") or "",
                "dependencies": sub.get("dependencies") or [],
                "risks": sub.get("risks") or [],
                "tests": [],
            }
            comp_entry["subcomponents"].append(sub_entry)
            _emit_plan_snapshot(slug, base_plan)
            emit_event(
                "generator",
                "component_plan_subcomponent_started",
                slug=slug,
                task=f"plan/{slug}",
                component=comp_name,
                subcomponent=sub_name,
                component_index=index,
                subcomponent_index=sub_index,
            )

            tests_payload = _run_codex_json(
                label=f"tests::{comp_name}::{sub_name}",
                prompt=_test_prompt(
                    slug=slug,
                    card_text=card_text,
                    component=comp_entry,
                    subcomponent=sub_entry,
                ),
                slug=slug,
                context=context,
                codex_bin=codex_bin,
                codex_flags=codex_flags,
                codex_model=codex_model,
                verbose=verbose,
            )
            tests = tests_payload.get("tests") or []
            for test_index, test in enumerate(tests, start=1):
                question = _extract_question(test, test_index)
                measurement = _extract_measurement(test)
                context_note = test.get("context") or test.get("description") or ""
                test_entry = {
                    "id": test.get("id")
                    or f"{sub_uid}-t{test_index}-{uuid.uuid4().hex[:6]}",
                    "question": question,
                    "measurement": measurement,
                    "context": context_note,
                    "status": test.get("status") or "proposed",
                    "tags": test.get("tags") or [],
                }
                sub_entry["tests"].append(test_entry)
                _emit_plan_snapshot(slug, base_plan)

            emit_event(
                "generator",
                "component_plan_subcomponent_completed",
                slug=slug,
                task=f"plan/{slug}",
                component=comp_name,
                subcomponent=sub_name,
                total_tests=len(sub_entry["tests"]),
            )

        emit_event(
            "generator",
            "component_plan_component_completed",
            slug=slug,
            task=f"plan/{slug}",
            component=comp_name,
            subcomponents=len(comp_entry["subcomponents"]),
        )
        _emit_plan_snapshot(slug, base_plan)

    base_plan["status"] = "completed"
    base_plan["generated_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    plan_path.write_text(
        json.dumps(base_plan, indent=2, ensure_ascii=False) + "\n", encoding="utf-8"
    )
    emit_event(
        "generator",
        "component_plan_completed",
        slug=slug,
        task=f"plan/{slug}",
        plan_path=str(plan_path),
    )
    _emit_plan_snapshot(slug, base_plan, plan_path=plan_path)
    if verbose:
        print(f"[planner] Component plan written to {plan_path}")
    return PlannerResult(plan=base_plan, path=plan_path)


def _emit_plan_snapshot(
    slug: str, plan: Dict[str, Any], *, plan_path: Path | None = None
) -> None:
    meta: Dict[str, Any] = {"plan": plan, "plan_slug": slug}
    if plan_path is not None:
        meta["plan_path"] = str(plan_path)
    emit_event(
        "generator",
        "component_plan_snapshot",
        slug=slug,
        task=f"plan/{slug}",
        **meta,
    )


def _hash_path(path: Path) -> str:
    return hashlib.sha256(path.read_bytes()).hexdigest()


def _collect_other_cards(cards_dir: Path, exclude: Path) -> List[Dict[str, str]]:
    payload: List[Dict[str, str]] = []
    if not cards_dir.exists():
        return payload
    for item in sorted(cards_dir.glob("*.md")):
        if item == exclude:
            continue
        try:
            payload.append(
                {
                    "path": str(item),
                    "name": item.stem.replace("-", " ").title(),
                }
            )
        except OSError:
            continue
    return payload


def _component_prompt(
    slug: str, card_text: str, other_cards: List[Dict[str, str]]
) -> str:
    extras = (
        "\n".join(f"- {card['name']} ({card['path']})" for card in other_cards)
        or "None"
    )
    return f"""
You are an engineering planner. Analyse the Feature Card below for slug `{slug}` and produce a JSON object
with this shape:
{{
  "components": [
    {{
      "id": "<stable-id>",
      "name": "<concise component name>",
      "summary": "<what this component does>",
      "rationale": "<why it exists / business value>",
      "notes": "<implementation hints or constraints>"
    }}
  ]
}}

Guidelines:
- Focus on end-user behaviours and supporting systems implied by the Feature Card.
- Components should be coarse-grained areas of responsibility that we can later split into subcomponents.
- Return STRICT JSON (no markdown or explanations).

Existing Feature Cards in the repository:
{extras}

--- FEATURE CARD START ---
{card_text}
--- FEATURE CARD END ---
""".strip()


def _subcomponent_prompt(
    *,
    slug: str,
    card_text: str,
    component: Dict[str, Any],
) -> str:
    summary = component.get("summary", "")
    rationale = component.get("rationale", "")
    return f"""
You are breaking down component `{component.get('name')}` (slug `{slug}`) into subcomponents.
Return STRICT JSON object:
{{
  "subcomponents": [
    {{
      "id": "<stable-id>",
      "name": "<subcomponent name>",
      "summary": "<scope and responsibilities>",
      "dependencies": ["<optional external dependency>", "..."],
      "risks": ["<optional risk>", "..."]
    }}
  ]
}}

Guidelines:
- Subcomponents should be testable slices (e.g., CLI parsing, config validation, logging).
- Include dependencies/risks only if they are truly relevant.
- Base your reasoning on the component summary, rationale, and Feature Card.

Component summary: {summary}
Component rationale: {rationale}

--- FEATURE CARD START ---
{card_text}
--- FEATURE CARD END ---
""".strip()


def _test_prompt(
    *,
    slug: str,
    card_text: str,
    component: Dict[str, Any],
    subcomponent: Dict[str, Any],
) -> str:
    summary = subcomponent.get("summary", "")
    deps = ", ".join(subcomponent.get("dependencies") or []) or "None stated"
    return f"""
You are proposing deterministic pytest scenarios for slug `{slug}`.
Component: {component.get('name')}
Subcomponent: {subcomponent.get('name')}

Return STRICT JSON object:
{{
  "tests": [
    {{
      "id": "<stable-id>",
      "question": "Does the CLI print Hello World by default?",
      "measurement": "Invoke the CLI with no arguments and assert stdout equals 'Hello World' and exit code is 0.",
      "context": "Optional extra notes or setup details",
      "status": "proposed",
      "tags": ["happy-path", "cli"]
    }}
  ]
}}

Guidelines:
- Every `question` must be a concrete yes/no style question framed from an observer's perspective (e.g. "Does quiet suppress output?").
- `measurement` must describe the exact deterministic procedure used to answer the question (inputs, command, and assertions).
- Use `context` for additional setup hints only when necessary; otherwise omit or keep short.
- Tests must remain offline, hermetic, and avoid randomness or time-based assertions.
- Cover happy path, edge cases, and failure behaviours implied by the Feature Card.
- Prefer status "proposed" unless guidance indicates otherwise.

Subcomponent summary: {summary}
Dependencies: {deps}

--- FEATURE CARD START ---
{card_text}
--- FEATURE CARD END ---
""".strip()


def _run_codex_json(
    *,
    label: str,
    prompt: str,
    slug: str,
    context: RexContext,
    codex_bin: str,
    codex_flags: str,
    codex_model: str,
    verbose: bool,
) -> Dict[str, Any]:
    if verbose:
        print(f"[planner] Calling Codex ({label})…")
    emit_event(
        "generator",
        "component_plan_stage_started",
        slug=slug,
        task=f"plan/{slug}",
        stage=label,
    )
    cmd = shlex.split(codex_bin) + ["exec"]
    if codex_flags.strip():
        cmd += shlex.split(codex_flags)
    if codex_model:
        cmd += ["--model", codex_model]
    cmd += ["--cd", str(context.root), "--", prompt]

    completed = subprocess.run(
        cmd,
        cwd=context.root,
        text=True,
        capture_output=True,
    )
    stdout = completed.stdout or ""
    stderr = completed.stderr or ""
    if completed.returncode != 0:
        emit_event(
            "generator",
            "component_plan_stage_failed",
            slug=slug,
            task=f"plan/{slug}",
            stage=label,
            stderr=stderr.strip(),
        )
        raise RuntimeError(
            f"Codex ({label}) failed with exit code {completed.returncode}: {stderr.strip()}"
        )

    payload = _extract_json(stdout)
    emit_event(
        "generator",
        "component_plan_stage_completed",
        slug=slug,
        task=f"plan/{slug}",
        stage=label,
    )
    return payload


def _extract_json(text: str) -> Dict[str, Any]:
    stripped = text.strip()
    if not stripped:
        return {}
    try:
        return json.loads(stripped)
    except json.JSONDecodeError:
        pass
    start_candidates = [stripped.find("{"), stripped.find("[")]
    start_candidates = [idx for idx in start_candidates if idx != -1]
    if not start_candidates:
        raise json.JSONDecodeError("No JSON object found", stripped, 0)
    start = min(start_candidates)
    for end in range(len(stripped), start, -1):
        fragment = stripped[start:end]
        try:
            data = json.loads(fragment)
            return data
        except json.JSONDecodeError:
            continue
    raise json.JSONDecodeError("Unable to decode JSON response", stripped, start)


def _extract_question(test: Mapping[str, Any], index: int) -> str:
    if not isinstance(test, Mapping):
        return f"Test {index}?"
    for key in ("question", "name", "title"):
        value = test.get(key)
        if isinstance(value, str) and value.strip():
            return _ensure_question(value)
    return f"Test {index}?"


def _extract_measurement(test: Mapping[str, Any]) -> str:
    if not isinstance(test, Mapping):
        return ""
    for key in ("measurement", "verification", "how_to_verify", "strategy"):
        value = test.get(key)
        if isinstance(value, str) and value.strip():
            return value.strip()
    desc = test.get("description")
    if isinstance(desc, str) and desc.strip():
        return desc.strip()
    return ""


def _ensure_question(text: str) -> str:
    cleaned = (text or "").strip()
    if not cleaned:
        return "Question?"
    if cleaned.endswith("?"):
        return cleaned
    if cleaned[-1:] in ".!;":
        cleaned = cleaned[:-1].strip()
    lowered = cleaned.lower()
    prefixes = ("does ", "is ", "can ", "will ", "should ", "did ")
    if any(lowered.startswith(prefix) for prefix in prefixes):
        base = cleaned
    else:
        base = (
            f"Does {cleaned[0].lower() + cleaned[1:]}" if len(cleaned) > 1 else cleaned
        )
    return f"{base}?"

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/config.py ===
"""Centralised configuration defaults for rex-codex."""

from __future__ import annotations

from .utils import agent_src, repo_root

REPO_ROOT = repo_root()
AGENT_SRC = agent_src(REPO_ROOT)
CODENAME = "rex-codex"
DEFAULT_GENERATOR_MAX_FILES = 6
DEFAULT_GENERATOR_MAX_LINES = 300
DEFAULT_DISCRIMINATOR_MAX_FILES = 6
DEFAULT_DISCRIMINATOR_MAX_LINES = 300
DEFAULT_COVERAGE_MIN = 80
DEFAULT_RUNTIME_ALLOWLIST = ("src",)
DEFAULT_PROTECTED_PATHS = [
    "tests",
    "documents",
    "pytest.ini",
    "pyproject.toml",
    "mypy.ini",
    ".flake8",
    ".ruff.toml",
    "ruff.toml",
    "conftest.py",
    "tox.ini",
    "setup.cfg",
    ".coveragerc",
    ".pre-commit-config.yaml",
    "requirements.txt",
    "requirements-dev.txt",
    "requirements",
    "constraints.txt",
    "constraints-*.txt",
    "constraints",
    "Pipfile",
    "Pipfile.lock",
    "poetry.lock",
    "Dockerfile",
    "Dockerfile.*",
    ".github",
    ".gitlab-ci.yml",
    ".gitlab",
    "Makefile",
    "noxfile.py",
]

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/discriminator.py ===
"""Staged automation ladder (discriminator) implemented in Python."""

from __future__ import annotations

import glob
import hashlib
import os
import re
import shlex
import shutil
import subprocess
import sys
import threading
import time
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import nullcontext
from dataclasses import dataclass
from pathlib import Path
from types import SimpleNamespace
from typing import Iterable, List, Mapping, Optional, Sequence, Tuple

from .cards import discover_cards, find_orphan_spec_slugs, load_rex_agent
from .config import (
    AGENT_SRC,
    DEFAULT_COVERAGE_MIN,
    DEFAULT_DISCRIMINATOR_MAX_FILES,
    DEFAULT_DISCRIMINATOR_MAX_LINES,
    DEFAULT_PROTECTED_PATHS,
    DEFAULT_RUNTIME_ALLOWLIST,
)
from .events import emit_event
from .generator import _split_command
from .monitoring import ensure_monitor_server
from .self_update import self_update
from .utils import (
    RexContext,
    activate_venv,
    dump_json,
    ensure_dir,
    ensure_python,
    ensure_requirements_installed,
    load_json,
    lock_file,
    run,
)


@dataclass
class DiscriminatorOptions:
    mode: str = "global"  # "feature" or "global"
    slug: Optional[str] = None
    continuous: bool = True
    max_passes: int = int(os.environ.get("DISCRIMINATOR_MAX_PASSES", "25"))
    disable_llm: bool = os.environ.get("DISABLE_LLM", "1") == "1"
    codex_bin: str = os.environ.get("CODEX_BIN", "npx --yes @openai/codex")
    codex_flags: str = os.environ.get("CODEX_FLAGS", "--yolo")
    codex_model: str = os.environ.get("MODEL", "")
    verbose: bool = True
    stage_timeout: Optional[int] = None


@dataclass
class Stage:
    identifier: str
    description: str
    command: str


@dataclass
class StageGroup:
    title: str
    stages: List[Stage]


def _write_discriminator_result(
    context: RexContext, payload: Mapping[str, object]
) -> None:
    path = context.codex_ci_dir / "discriminator_result.json"
    dump_json(path, payload)


def _ansi_palette() -> SimpleNamespace:
    disable = bool(os.environ.get("NO_COLOR")) or not sys.stdout.isatty()
    if disable:
        return SimpleNamespace(
            green="",
            red="",
            yellow="",
            blue="",
            cyan="",
            magenta="",
            dim="",
            reset="",
            bold="",
            error="",
        )
    return SimpleNamespace(
        green="\x1b[32m",
        red="\x1b[31m",
        yellow="\x1b[33m",
        blue="\x1b[34m",
        cyan="\x1b[36m",
        magenta="\x1b[35m",
        dim="\x1b[2m",
        reset="\x1b[0m",
        bold="\x1b[1m",
        error="\x1b[31m",
    )


def run_discriminator(
    options: DiscriminatorOptions, *, context: RexContext | None = None
) -> int:
    context = context or RexContext.discover()
    ensure_monitor_server(context, open_browser=True)
    self_update()
    ensure_dir(context.codex_ci_dir)
    lock_path = context.codex_ci_dir / "rex_discriminator.lock"
    with lock_file(lock_path):
        return _run_locked(options, context)


def _run_locked(options: DiscriminatorOptions, context: RexContext) -> int:
    ensure_python(context, quiet=True)
    requirements_template = AGENT_SRC / "templates" / "requirements-dev.txt"
    ensure_requirements_installed(context, requirements_template)
    env = activate_venv(context)
    env.setdefault("PYTHONHASHSEED", "0")
    if "COVERAGE_TARGETS" not in env and (context.root / "src").exists():
        env["COVERAGE_TARGETS"] = "src"
    env.setdefault("COVERAGE_MIN", str(DEFAULT_COVERAGE_MIN))
    if options.stage_timeout:
        env["DISCRIMINATOR_STAGE_TIMEOUT"] = str(options.stage_timeout)

    slug = options.slug or _discover_active_slug(context)
    mode = options.mode
    if mode == "feature" and not slug:
        print("[discriminator] No active feature slug; falling back to global sweep")
        mode = "global"

    log_path = context.codex_ci_dir / "latest_discriminator.log"
    latest_log_path = context.root / ".codex_ci_latest.log"
    if options.verbose:
        print(f"[discriminator] Logs will be written to {context.relative(log_path)}")

    passes = 0
    run_counter = 0
    while passes < options.max_passes:
        passes += 1
        attempt = 1
        run_counter += 1
        print(
            f"=== rex-codex discriminator ({mode}) pass {passes}/{options.max_passes} ==="
        )
        log_path.write_text("", encoding="utf-8")
        latest_log_path.write_text("", encoding="utf-8")

        ok = _run_stage_plan(
            mode=mode,
            slug=slug,
            env=env,
            context=context,
            log_path=log_path,
            latest_log_path=latest_log_path,
            pass_number=passes,
            run_id=run_counter,
            attempt=attempt,
        )
        if ok:
            print(f"✅ Green: {mode} suite passed")
            _record_success(mode, slug, context, env)
            return 0

        if not options.continuous:
            print("[discriminator] Stopping after first failing pass (--single-pass).")
            return 1

        # Mechanical fixes
        attempt += 1
        next_run_id = run_counter + 1
        if _apply_mechanical_fixes(
            mode,
            slug,
            context,
            env,
            pass_number=passes,
            attempt=attempt,
            run_id=next_run_id,
        ):
            run_counter = next_run_id
            if _run_stage_plan(
                mode=mode,
                slug=slug,
                env=env,
                context=context,
                log_path=log_path,
                latest_log_path=latest_log_path,
                pass_number=passes,
                run_id=run_counter,
                attempt=attempt,
            ):
                print("✅ Green after mechanical fixes")
                _record_success(mode, slug, context, env)
                return 0
            attempt += 1

        future_run_id = run_counter + 1
        llm_event_context = {
            "slug": slug,
            "mode": mode,
            "pass_number": passes,
            "run_id": run_counter,
            "next_run_id": future_run_id,
            "attempt": attempt,
        }
        if options.disable_llm:
            emit_event(
                "discriminator",
                "llm_patch_decision",
                accepted=False,
                reason="llm_disabled",
                **llm_event_context,
            )
            print("LLM disabled; stopping after mechanical fixes.")
            return 2

        if not _ensure_node_present():
            emit_event(
                "discriminator",
                "llm_patch_decision",
                accepted=False,
                reason="node_missing",
                **llm_event_context,
            )
            print("[discriminator] Node.js not found; forcing DISABLE_LLM=1.")
            return 2

        test_count_before = _collect_test_count(mode, slug, context, env)
        snapshot = _snapshot_protected_paths(context)
        _invoke_llm_once(options, mode, slug, context, env, log_path, latest_log_path)

        changed = _detect_protected_changes(snapshot, context)
        if changed:
            emit_event(
                "discriminator",
                "llm_patch_decision",
                accepted=False,
                reason="protected_paths_modified",
                paths=changed,
                **llm_event_context,
            )
            print("[discriminator] Aborting pass; LLM patch touched protected paths.")
            _revert_paths(changed, context)
            return 2

        if not _reject_non_runtime_changes(context):
            emit_event(
                "discriminator",
                "llm_patch_decision",
                accepted=False,
                reason="non_runtime_changes",
                **llm_event_context,
            )
            print("[discriminator] Aborting pass; LLM patch touched non-runtime paths.")
            _revert_all_changes(context)
            return 2

        if _git_diff_is_empty(context):
            emit_event(
                "discriminator",
                "llm_patch_decision",
                accepted=False,
                reason="no_diff",
                **llm_event_context,
            )
            print("No diff from LLM; aborting.")
            return 2

        test_count_after = _collect_test_count(mode, slug, context, env)
        if (
            test_count_before is not None
            and test_count_after is not None
            and test_count_after < test_count_before
        ):
            emit_event(
                "discriminator",
                "llm_patch_decision",
                accepted=False,
                reason="test_count_decreased",
                before=test_count_before,
                after=test_count_after,
                **llm_event_context,
            )
            print(
                f"[discriminator] Test collection decreased ({test_count_before} -> {test_count_after}); rejecting LLM patch."
            )
            _revert_all_changes(context)
            return 2

        if not _enforce_patch_size(context):
            emit_event(
                "discriminator",
                "llm_patch_decision",
                accepted=False,
                reason="patch_size_exceeded",
                **llm_event_context,
            )
            print("[discriminator] Aborting pass; LLM patch exceeded size limits.")
            return 2

        run(["git", "add", "-A"], cwd=context.root, check=False)
        commit_message = f"chore(rex-codex): discriminator {mode} pass {passes}"
        run(
            ["git", "commit", "-m", commit_message],
            cwd=context.root,
            check=False,
        )
        emit_event(
            "discriminator",
            "llm_patch_decision",
            accepted=True,
            reason="committed",
            commit_message=commit_message,
            **llm_event_context,
        )
        _record_success(mode, slug, context, env)
    print(f"Hit max passes ({options.max_passes}) without going green")
    return 1


def _discover_active_slug(context: RexContext) -> Optional[str]:
    data = load_rex_agent(context)
    feature = data.get("feature", {})
    slug = feature.get("active_slug")
    if slug:
        return slug
    cards = discover_cards(statuses=["proposed"], context=context)
    return cards[0].slug if cards else None


def _run_stage_plan(
    *,
    mode: str,
    slug: Optional[str],
    env: dict[str, str],
    context: RexContext,
    log_path: Path,
    latest_log_path: Path,
    pass_number: int,
    run_id: int,
    attempt: int,
) -> bool:
    pytest_flags = _configure_pytest_flags(mode, env, context)
    specs_dir = context.root / "tests" / "feature_specs" / (slug or "")
    palette = _ansi_palette()
    if mode == "feature":
        if not slug:
            print("[discriminator] Feature mode requested but no slug provided.")
            _write_discriminator_result(
                context,
                {
                    "mode": mode,
                    "slug": slug,
                    "ok": False,
                    "coverage_failed": False,
                    "coverage_targets": None,
                    "coverage_threshold": None,
                    "first_failure": {
                        "identifier": None,
                        "description": "Feature slug missing",
                        "command": "",
                    },
                },
            )
            return False
        if not specs_dir.is_dir():
            msg = f"[discriminator] Feature specs directory {specs_dir} missing"
            print(msg)
            with open(latest_log_path, "a", encoding="utf-8") as fh:
                fh.write(msg + "\n")
            _write_discriminator_result(
                context,
                {
                    "mode": mode,
                    "slug": slug,
                    "ok": False,
                    "coverage_failed": False,
                    "coverage_targets": None,
                    "coverage_threshold": None,
                    "first_failure": {
                        "identifier": None,
                        "description": "Feature specs directory missing",
                        "command": "",
                    },
                },
            )
            return False

    groups = _build_stage_groups(mode, slug, pytest_flags, env, context)
    overall_ok = True
    summary: List[dict[str, object]] = []
    first_failure: Optional[dict[str, object]] = None
    coverage_failed = False
    coverage_min = env.get("COVERAGE_MIN")
    coverage_targets_config = env.get("COVERAGE_TARGETS")
    coverage_targets = coverage_targets_config or "."
    coverage_targets_display: Optional[str] = coverage_targets_config or (
        coverage_targets if coverage_min else None
    )
    coverage_threshold = coverage_min
    emit_event(
        "discriminator",
        "run_started",
        slug=slug,
        mode=mode,
        pass_number=pass_number,
        run_id=run_id,
        attempt=attempt,
        stage_groups=[group.title for group in groups],
    )
    print_lock = threading.Lock()

    for group in groups:
        print("------------------------------------------------------------")
        print(f"{palette.blue}Stage: {group.title}{palette.reset}")
        executable_stages = [stage for stage in group.stages if stage.command.strip()]
        if not executable_stages:
            continue

        def _handle_stage_result(
            stage: Stage, ok: bool, elapsed: float, tail: str
        ) -> None:
            nonlocal overall_ok, first_failure, coverage_failed
            status = (
                f"{palette.green}PASS{palette.reset}"
                if ok
                else f"{palette.red}FAIL{palette.reset}"
            )
            timing = f"{palette.dim}({elapsed:.2f}s){palette.reset}"
            print(
                f"    {palette.dim}[{stage.identifier}]{palette.reset} {status} {timing}"
            )
            record = {
                "group": group.title,
                "identifier": stage.identifier,
                "description": stage.description,
                "command": stage.command,
                "elapsed": elapsed,
                "ok": ok,
                "tail": tail,
            }
            summary.append(record)
            failure_reason = _summarize_failure_reason(tail) if not ok else ""
            emit_event(
                "discriminator",
                "stage_end",
                slug=slug,
                mode=mode,
                pass_number=pass_number,
                run_id=run_id,
                attempt=attempt,
                identifier=stage.identifier,
                description=stage.description,
                command=stage.command,
                group=group.title,
                ok=ok,
                elapsed=elapsed,
                tail=tail,
                failure_reason=failure_reason,
            )
            if stage.identifier.startswith("04.") or "Coverage" in group.title:
                percent = _parse_coverage_percent(tail)
                if percent is not None:
                    emit_event(
                        "discriminator",
                        "coverage_update",
                        slug=slug,
                        mode=mode,
                        pass_number=pass_number,
                        run_id=run_id,
                        attempt=attempt,
                        identifier=stage.identifier,
                        percent=percent,
                        threshold=coverage_threshold,
                        targets=_split_targets_for_events(coverage_targets),
                    )
            if not ok:
                overall_ok = False
                if first_failure is None:
                    first_failure = record
                    banner = f"[discriminator] First failure: [{stage.identifier}] {stage.description}"
                    print(f"{palette.error}{banner}{palette.reset}")
                    with open(latest_log_path, "a", encoding="utf-8") as fh:
                        fh.write(banner + "\n")
                if stage.identifier.startswith("04.") or "Coverage" in group.title:
                    coverage_failed = True

        if group.title.startswith("Level 06"):
            for stage in executable_stages:
                emit_event(
                    "discriminator",
                    "stage_start",
                    slug=slug,
                    mode=mode,
                    pass_number=pass_number,
                    run_id=run_id,
                    attempt=attempt,
                    identifier=stage.identifier,
                    description=stage.description,
                    command=stage.command,
                    group=group.title,
                )
            for stage, ok, elapsed, tail in _run_parallel_stage_group(
                executable_stages,
                env,
                context,
                log_path,
                latest_log_path,
                print_lock,
            ):
                _handle_stage_result(stage, ok, elapsed, tail)
        else:
            for stage in executable_stages:
                emit_event(
                    "discriminator",
                    "stage_start",
                    slug=slug,
                    mode=mode,
                    pass_number=pass_number,
                    run_id=run_id,
                    attempt=attempt,
                    identifier=stage.identifier,
                    description=stage.description,
                    command=stage.command,
                    group=group.title,
                )
                ok, elapsed, tail = _execute_stage(
                    stage,
                    env,
                    context,
                    log_path,
                    latest_log_path,
                )
                _handle_stage_result(stage, ok, elapsed, tail)
    result = (
        f"{palette.green}PASS{palette.reset}"
        if overall_ok
        else f"{palette.red}FAIL{palette.reset}"
    )
    print(f"  Result: [{result}]")
    _render_stage_summary(summary, overall_ok, first_failure, palette, context, mode)
    payload: dict[str, object] = {
        "mode": mode,
        "slug": slug,
        "ok": overall_ok,
        "coverage_failed": coverage_failed,
        "coverage_targets": coverage_targets_display,
        "coverage_threshold": coverage_threshold,
        "first_failure": None,
    }
    if first_failure is not None:
        payload["first_failure"] = {
            "identifier": first_failure.get("identifier"),
            "description": first_failure.get("description"),
            "command": first_failure.get("command"),
        }
    _write_discriminator_result(context, payload)
    emit_event(
        "discriminator",
        "run_completed",
        slug=slug,
        mode=mode,
        pass_number=pass_number,
        run_id=run_id,
        attempt=attempt,
        ok=overall_ok,
        coverage_failed=coverage_failed,
        first_failure_identifier=(
            first_failure.get("identifier") if first_failure else None
        ),
        first_failure_description=(
            first_failure.get("description") if first_failure else None
        ),
    )
    return overall_ok


def _configure_pytest_flags(
    mode: str, env: dict[str, str], context: RexContext
) -> List[str]:
    flags = ["-q", "-ra"]
    if mode == "feature":
        flags += ["-x", "--maxfail=1"]
        return flags
    probe = run(
        [
            "python",
            "-c",
            "import importlib.util, sys; sys.exit(0 if importlib.util.find_spec('xdist') else 1)",
        ],
        env=env,
        cwd=context.root,
        check=False,
        capture_output=True,
    )
    if probe.returncode == 0:
        flags += ["-n", "auto", "--dist", "loadscope"]
    return flags


def _build_stage_groups(
    mode: str,
    slug: Optional[str],
    pytest_flags: Sequence[str],
    env: dict[str, str],
    context: RexContext,
) -> List[StageGroup]:
    pytest_flags_str = " ".join(shlex.quote(flag) for flag in pytest_flags)
    specs_dir = f"tests/feature_specs/{slug}" if slug else ""
    coverage_min = env.get("COVERAGE_MIN")
    coverage_targets = env.get("COVERAGE_TARGETS", ".")

    def _format_targets(raw: str) -> str:
        tokens = [token for token in re.split(r"[,\s]+", raw.strip()) if token]
        return " ".join(shlex.quote(token) for token in tokens) or "."

    if env.get("MYPY_TARGETS"):
        mypy_raw = env["MYPY_TARGETS"]
    elif env.get("COVERAGE_TARGETS"):
        mypy_raw = env["COVERAGE_TARGETS"]
    elif (context.root / "src").exists():
        mypy_raw = "src"
    else:
        mypy_raw = "."
    mypy_targets = _format_targets(mypy_raw)
    groups: List[StageGroup] = []

    level00 = StageGroup(
        title="Level 00 - Repo & System Health",
        stages=[
            Stage("00.1", "Git status", "git status -sb"),
            Stage("00.2", "Python version", "python3 --version"),
        ],
    )
    if (context.root / ".venv" / "bin" / "python").exists():
        level00.stages.append(
            Stage("00.3", "Venv Python", ".venv/bin/python --version")
        )
    groups.append(level00)

    groups.append(
        StageGroup(
            title="Level 01 - Tooling & Dependencies",
            stages=[
                Stage(
                    "01.1",
                    "pytest importable?",
                    "python -c 'import pytest; print(pytest.__version__)'",
                ),
            ],
        )
    )

    if mode == "feature":
        groups.append(
            StageGroup(
                title=f"Level 02 - Feature Spec Smoke ({slug})",
                stages=[
                    Stage(
                        "02.1",
                        "Run feature specs",
                        f"pytest {pytest_flags_str} {shlex.quote(specs_dir)} --junitxml .codex_ci/discriminator_feature_{slug}.xml",
                    )
                ],
            )
        )
        groups.append(
            StageGroup(
                title=f"Level 03 - Feature Unit Grid ({slug})",
                stages=[
                    Stage(
                        "03.1",
                        "Run feature specs (no DB markers)",
                        f"pytest {pytest_flags_str} {shlex.quote(specs_dir)} -m 'not django_db'",
                    )
                ],
            )
        )
        if coverage_min:
            groups.append(
                StageGroup(
                    title=f"Level 04 - Feature Coverage ({slug})",
                    stages=[
                        Stage(
                            "04.1",
                            "Coverage threshold",
                            f"pytest {pytest_flags_str} {shlex.quote(specs_dir)} --cov={coverage_targets} --cov-report=term --cov-fail-under={coverage_min}",
                        )
                    ],
                )
            )
    else:
        groups.append(
            StageGroup(
                title="Level 02 - Inline Spec Smoke",
                stages=[
                    Stage(
                        "02.1",
                        "Do doctests/specs pass?",
                        f"pytest {pytest_flags_str} -k 'spec or doctest' --junitxml .codex_ci/discriminator_global_smoke.xml",
                    )
                ],
            )
        )
        groups.append(
            StageGroup(
                title="Level 03 - Unit Test Grid",
                stages=[
                    Stage(
                        "03.1",
                        "Run unit tests (no DB markers)",
                        f"pytest {pytest_flags_str} -m 'not django_db' --junitxml .codex_ci/discriminator_global_unit.xml",
                    )
                ],
            )
        )
        if coverage_min:
            groups.append(
                StageGroup(
                    title="Level 04 - Coverage",
                    stages=[
                        Stage(
                            "04.1",
                            "Coverage threshold",
                            f"pytest {pytest_flags_str} --cov={coverage_targets} --cov-report=term --cov-fail-under={coverage_min}",
                        )
                    ],
                )
            )

    level05_stages: List[Stage] = []
    if env.get("PIP_AUDIT") == "1":
        level05_stages.append(
            Stage(
                "05.1",
                "pip-audit (dependencies)",
                "python -m pip install -q pip-audit >/dev/null 2>&1 && pip-audit",
            )
        )
    if env.get("BANDIT") == "1":
        bandit_targets = (
            env.get("BANDIT_TARGETS") or env.get("COVERAGE_TARGETS") or "src"
        )
        if not (context.root / bandit_targets).exists():
            bandit_targets = "."
        level05_stages.append(
            Stage(
                "05.2",
                "bandit (static security)",
                f"python -m pip install -q bandit >/dev/null 2>&1 && bandit -q -r {bandit_targets}",
            )
        )
    if env.get("PACKAGE_CHECK") == "1":
        level05_stages.extend(
            [
                Stage(
                    "05.3",
                    "Build distribution artifacts",
                    "python -m pip install -q build twine >/dev/null 2>&1 && python -m build",
                ),
                Stage(
                    "05.4",
                    "twine check dist/*",
                    "python -m pip install -q build twine >/dev/null 2>&1 && twine check dist/*",
                ),
            ]
        )
    if level05_stages:
        groups.append(
            StageGroup(title="Level 05 - Security & Build", stages=level05_stages)
        )

    if mode == "feature":
        target = shlex.quote(specs_dir)
        feature_style_stages = [
            Stage("06.1", "black --check (feature)", f"black {target} --check"),
            Stage(
                "06.2", "isort --check-only (feature)", f"isort {target} --check-only"
            ),
            Stage("06.3", "ruff check (feature)", f"ruff check {target}"),
            Stage("06.4", "flake8 (feature)", f"flake8 {target}"),
        ]
        if env.get("MYPY_INCLUDE_TESTS") == "1":
            feature_style_stages.append(
                Stage("06.5", "mypy (feature)", f"mypy {target}")
            )
        groups.append(
            StageGroup(
                title=f"Level 06 - Feature Style & Type Gates ({slug})",
                stages=feature_style_stages,
            )
        )
    else:
        groups.append(
            StageGroup(
                title="Level 06 - Style & Type Gates",
                stages=[
                    Stage("06.1", "black --check", "black . --check"),
                    Stage("06.2", "isort --check-only", "isort . --check-only"),
                    Stage("06.3", "ruff check", "ruff check ."),
                    Stage("06.4", "flake8", "flake8 ."),
                    Stage("06.5", "mypy", f"mypy {mypy_targets}"),
                ],
            )
        )
    return groups


def _execute_stage(
    stage: Stage,
    env: dict[str, str],
    context: RexContext,
    log_path: Path,
    latest_log_path: Path,
    print_lock: Optional[threading.Lock] = None,
) -> Tuple[bool, float, str]:
    with print_lock or nullcontext():
        print(f"\n  Question {stage.identifier}: {stage.description}")
        print(f"    Command: {stage.command}")
    stage_timeout = int(os.environ.get("DISCRIMINATOR_STAGE_TIMEOUT", "0") or "0")
    timeout_seconds = stage_timeout if stage_timeout > 0 else None
    cmd = ["bash", "-lc", stage.command]
    start = time.perf_counter()
    try:
        completed = subprocess.run(
            cmd,
            cwd=context.root,
            env=env,
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
        )
        output = (completed.stdout or "") + (completed.stderr or "")
    except subprocess.TimeoutExpired:
        message = (
            f"[discriminator] Stage {stage.identifier} timed out after {stage_timeout}s"
        )
        output = message + "\n"
        completed = subprocess.CompletedProcess(
            cmd, returncode=124, stdout="", stderr=message
        )
    elapsed = time.perf_counter() - start
    with open(log_path, "a", encoding="utf-8") as fh:
        fh.write(f"\n[{stage.identifier}] {stage.description}\n")
        fh.write(output)
    with open(latest_log_path, "a", encoding="utf-8") as fh:
        fh.write(output)
    with print_lock or nullcontext():
        print(output, end="")
        if completed.returncode == 124:
            print(
                f"[discriminator] Stage {stage.identifier} timed out after {stage_timeout}s"
            )
    ok = completed.returncode == 0
    tail_lines = "\n".join((output or "").splitlines()[-8:])
    return ok, elapsed, tail_lines


def _run_parallel_stage_group(
    stages: Sequence[Stage],
    env: dict[str, str],
    context: RexContext,
    log_path: Path,
    latest_log_path: Path,
    print_lock: threading.Lock,
) -> Iterable[Tuple[Stage, bool, float, str]]:
    if not stages:
        return
    max_workers = min(5, len(stages))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_map = {
            executor.submit(
                _execute_stage,
                stage,
                env,
                context,
                log_path,
                latest_log_path,
                print_lock,
            ): stage
            for stage in stages
        }
        for future in as_completed(future_map):
            stage = future_map[future]
            ok, elapsed, tail = future.result()
            yield stage, ok, elapsed, tail


_COVERAGE_TOTAL_RE = re.compile(r"TOTAL\s+\d+\s+\d+\s+\d+\s+(\d+)%")


def _parse_coverage_percent(text: str) -> Optional[float]:
    for line in reversed((text or "").splitlines()):
        match = _COVERAGE_TOTAL_RE.search(line)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                continue
    return None


def _split_targets_for_events(raw: str) -> List[str]:
    tokens = [
        token.strip() for token in re.split(r"[,\s]+", raw or "") if token.strip()
    ]
    return tokens or []


def _render_stage_summary(
    summary: List[dict[str, object]],
    overall_ok: bool,
    first_failure: Optional[dict[str, object]],
    palette: SimpleNamespace,
    context: RexContext,
    mode: str,
) -> None:
    if not summary:
        return
    print("\n--- Summary -----------------------------------------------------")
    grouped: "OrderedDict[str, List[dict[str, object]]]" = OrderedDict()
    for record in summary:
        key = record["group"]  # type: ignore[index]
        grouped.setdefault(key, []).append(record)
    for group, rows in grouped.items():
        print(f"{palette.bold}{group}{palette.reset}")
        for record in rows:
            ok = bool(record["ok"])
            icon = (
                f"{palette.green}✔{palette.reset}"
                if ok
                else f"{palette.red}✖{palette.reset}"
            )
            identifier = record["identifier"]
            description = record["description"]
            elapsed = float(record["elapsed"])
            timing = f"{palette.dim}({elapsed:.2f}s){palette.reset}"
            print(
                f"  {icon} {palette.dim}[{identifier}]{palette.reset} {description} {timing}"
            )
            if not ok:
                reason = _summarize_failure_reason(record.get("tail", ""))
                if reason:
                    print(f"      ↳ {palette.error}{reason}{palette.reset}")
    if not overall_ok and first_failure is not None:
        command = first_failure["command"]
        print(
            f"\n{palette.yellow}Next step:{palette.reset} rerun the first failing command locally:"
        )
        print(f"  {command}")
        print(
            f"Inspect {palette.cyan}./rex-codex logs --discriminator --lines 200{palette.reset} for full output."
        )
    if mode == "global":
        orphans = find_orphan_spec_slugs(context)
        if orphans:
            paths = ", ".join(f"tests/feature_specs/{slug}" for slug in sorted(orphans))
            print(
                f"{palette.yellow}[discriminator] Orphan spec shards detected:{palette.reset} {paths}\n"
                f"  Run `./rex-codex card prune-specs` to tidy up."
            )


def _summarize_failure_reason(tail: object) -> str:
    text = str(tail or "")
    for line in text.splitlines():
        stripped = line.strip()
        if not stripped:
            continue
        if stripped.lower().startswith("bringing up nodes"):
            continue
        if stripped.startswith("SKIPPED "):
            continue
        return stripped[:160]
    return ""


def shutil_which(name: str) -> Optional[str]:
    from shutil import which

    return which(name)


def _ensure_node_present() -> bool:
    return shutil_which("node") is not None


def _collect_test_count(
    mode: str,
    slug: Optional[str],
    context: RexContext,
    env: dict[str, str],
) -> Optional[int]:
    cmd = ["pytest", "--collect-only"]
    if mode == "feature" and slug:
        specs_dir = context.root / "tests" / "feature_specs" / slug
        if specs_dir.is_dir():
            cmd.append(str(specs_dir))
    completed = run(cmd, cwd=context.root, env=env, capture_output=True, check=False)
    text = (completed.stdout or "") + (completed.stderr or "")
    match = re.search(r"collected (\d+) items?", text, re.IGNORECASE)
    if not match:
        return None
    try:
        return int(match.group(1))
    except ValueError:
        return None


def _protected_patterns() -> List[str]:
    raw = os.environ.get("DISCRIMINATOR_PROTECTED_PATHS")
    if raw:
        return [token for token in raw.split() if token.strip()]
    return list(DEFAULT_PROTECTED_PATHS)


def _snapshot_protected_paths(context: RexContext) -> dict[str, str]:
    patterns = _protected_patterns()
    root = context.root
    files: set[Path] = set()

    def record_path(path: Path) -> None:
        if not path.exists():
            return
        if path.is_dir():
            for child in path.rglob("*"):
                if child.is_file():
                    files.add(child)
        elif path.is_file():
            files.add(path)

    for pattern in patterns:
        if not pattern:
            continue
        full_pattern = root / pattern
        matches = glob.glob(str(full_pattern), recursive=True)
        if not matches and not any(ch in pattern for ch in "*?[]"):
            candidate = root / pattern
            if candidate.exists():
                matches = [str(candidate)]
        for match in matches:
            record_path(Path(match))

    snapshot: dict[str, str] = {}
    for file_path in sorted(files):
        try:
            digest = hashlib.sha256(file_path.read_bytes()).hexdigest()
        except OSError:
            continue
        snapshot[str(file_path.relative_to(root))] = digest
    return snapshot


def _detect_protected_changes(
    baseline: dict[str, str], context: RexContext
) -> List[str]:
    current = _snapshot_protected_paths(context)
    changed: set[str] = set()
    for path, digest in baseline.items():
        if path not in current:
            changed.add(path)
        elif current[path] != digest:
            changed.add(path)
    for path in current:
        if path not in baseline:
            changed.add(path)
    return sorted(changed)


def _revert_paths(paths: Iterable[str], context: RexContext) -> None:
    for path in paths:
        target = context.root / path
        result = run(
            ["git", "ls-files", "--error-unmatch", path],
            cwd=context.root,
            capture_output=True,
            check=False,
        )
        if result.returncode == 0:
            run(
                ["git", "restore", "--staged", "--", path],
                cwd=context.root,
                check=False,
            )
            run(
                ["git", "restore", "--worktree", "--", path],
                cwd=context.root,
                check=False,
            )
        elif target.exists():
            if target.is_dir():
                shutil.rmtree(target)
            else:
                target.unlink()


def _revert_all_changes(context: RexContext) -> None:
    result = run(
        ["git", "restore", "--staged", "--worktree", "--source=HEAD", ":/"],
        cwd=context.root,
        check=False,
    )
    if result.returncode != 0:
        run(["git", "reset", "--hard", "-q"], cwd=context.root, check=False)


def _reject_non_runtime_changes(context: RexContext) -> bool:
    runtime_targets = _detect_runtime_targets(context)
    if not runtime_targets:
        return True
    changed = run(
        [
            "bash",
            "-lc",
            "git diff --name-only; git ls-files --others --exclude-standard",
        ],
        cwd=context.root,
        capture_output=True,
        check=False,
    ).stdout.splitlines()
    rejects: List[str] = []
    for path in sorted(set(changed)):
        if not path or path.startswith(".codex_ci/"):
            continue
        allowed = any(
            path == target or path.startswith(f"{target}/")
            for target in runtime_targets
        )
        if not allowed:
            rejects.append(path)
    if rejects:
        print(
            f"[discriminator] LLM edits outside runtime allowlist: {' '.join(rejects)}"
        )
        _revert_paths(rejects, context)
        return False
    return True


def _git_diff_is_empty(context: RexContext) -> bool:
    result = run(["git", "diff", "--quiet"], cwd=context.root, check=False)
    return result.returncode == 0


def _enforce_patch_size(context: RexContext) -> bool:
    max_files = int(
        os.environ.get("DISCRIMINATOR_MAX_FILES", DEFAULT_DISCRIMINATOR_MAX_FILES)
    )
    max_lines = int(
        os.environ.get("DISCRIMINATOR_MAX_LINES", DEFAULT_DISCRIMINATOR_MAX_LINES)
    )
    completed = run(
        ["git", "diff", "--numstat"], cwd=context.root, capture_output=True, check=False
    )
    files = 0
    lines = 0
    for line in completed.stdout.splitlines():
        parts = line.split()
        if len(parts) < 3:
            continue
        added, deleted = parts[0], parts[1]
        if added == "-" or deleted == "-":
            files += 1
            lines += max_lines + 1
            continue
        try:
            files += 1
            lines += int(added) + int(deleted)
        except ValueError:
            continue
    if files > max_files or lines > max_lines:
        print(
            f"[discriminator] LLM patch touched {files} files / {lines} lines "
            f"(limits {max_files}/{max_lines})."
        )
        _revert_all_changes(context)
        return False
    return True


def _apply_mechanical_fixes(
    mode: str,
    slug: Optional[str],
    context: RexContext,
    env: dict[str, str],
    *,
    pass_number: int,
    attempt: int,
    run_id: int,
) -> bool:
    print("Mechanical fixes (ruff/black/isort)…")
    tools = ["ruff", "black", "isort"]
    targets: List[str] = []
    reason: Optional[str] = None
    if mode == "feature":
        if not slug:
            reason = "missing_slug"
            emit_event(
                "discriminator",
                "mechanical_fixes",
                slug=slug,
                mode=mode,
                pass_number=pass_number,
                run_id=run_id,
                attempt=attempt,
                changed=False,
                tools=tools,
                targets=targets,
                reason=reason,
            )
            return False
        target = context.root / "tests" / "feature_specs" / slug
        if not target.is_dir():
            print(
                "[discriminator] No feature specs directory; skipping mechanical fixes."
            )
            reason = "missing_feature_specs"
            emit_event(
                "discriminator",
                "mechanical_fixes",
                slug=slug,
                mode=mode,
                pass_number=pass_number,
                run_id=run_id,
                attempt=attempt,
                changed=False,
                tools=tools,
                targets=targets,
                reason=reason,
            )
            return False
        targets = [str(target)]
    else:
        targets = _detect_runtime_targets(context)
        if not targets:
            print(
                "[discriminator] No runtime targets detected for mechanical style; skipping."
            )
            reason = "no_runtime_targets"
            emit_event(
                "discriminator",
                "mechanical_fixes",
                slug=slug,
                mode=mode,
                pass_number=pass_number,
                run_id=run_id,
                attempt=attempt,
                changed=False,
                tools=tools,
                targets=targets,
                reason=reason,
            )
            return False
    run(["ruff", "check", *targets, "--fix"], cwd=context.root, env=env, check=False)
    run(["black", *targets], cwd=context.root, env=env, check=False)
    run(["isort", *targets], cwd=context.root, env=env, check=False)
    changed = not _git_diff_is_empty(context)
    emit_event(
        "discriminator",
        "mechanical_fixes",
        slug=slug,
        mode=mode,
        pass_number=pass_number,
        run_id=run_id,
        attempt=attempt,
        changed=changed,
        tools=tools,
        targets=targets,
        reason=None if changed else (reason or "no_changes"),
    )
    if not changed:
        return False
    run(["git", "add", "-A"], cwd=context.root, check=False)
    label = "feature" if mode == "feature" else "global"
    run(
        ["git", "commit", "-m", f"style(rex-codex): apply ruff/black/isort ({label})"],
        cwd=context.root,
        check=False,
    )
    return True


def _detect_runtime_targets(context: RexContext) -> List[str]:
    overrides = os.environ.get("DISCRIMINATOR_RUNTIME_ALLOWLIST")
    if overrides:
        runtime = sorted(
            {token.strip() for token in overrides.split() if token.strip()}
        )
        return runtime
    root = context.root
    targets: set[str] = set()
    for default in DEFAULT_RUNTIME_ALLOWLIST:
        candidate = root / default
        if candidate.exists():
            targets.add(default)
    for pkg_init in root.glob("*/__init__.py"):
        pkg_dir = pkg_init.parent
        name = pkg_dir.name
        if name in {"tests", "test", "documents", "docs", ".git", ".github"}:
            continue
        targets.add(name)
    return sorted(targets)


def _tail_text(path: Path, lines: int = 120) -> str:
    if not path.exists():
        return ""
    content = path.read_text(encoding="utf-8", errors="replace").splitlines()
    return "\n".join(content[-lines:])


def _invoke_llm_once(
    options: DiscriminatorOptions,
    mode: str,
    slug: Optional[str],
    context: RexContext,
    env: dict[str, str],
    log_path: Path,
    latest_log_path: Path,
) -> None:
    runtime_allowlist = _detect_runtime_targets(context)
    agents_path = context.root / "AGENTS.md"
    agents_excerpt = ""
    if agents_path.exists():
        agents_excerpt = "\n".join(
            agents_path.read_text(encoding="utf-8", errors="replace").splitlines()[:300]
        )
    log_excerpt = _tail_text(log_path) or _tail_text(latest_log_path)

    prompt_lines = [
        "You are a coding agent for this repository.",
        "Follow AGENTS.md guardrails (runtime vs tests, doc/spec/type, offline by default).",
        "Make ONE minimal change that most reduces non-compliance or failures.",
        "Do not weaken tests or remove functionality.",
        "After edits, run relevant commands locally to validate.",
        "",
        f"Current discriminator mode: {mode}",
    ]
    if slug:
        prompt_lines.append(f"Active feature slug: {slug}")
    prompt_lines.extend(
        [
            "",
            "Runtime directories permitted for edits:",
        ]
    )
    if runtime_allowlist:
        prompt_lines.extend([f" - {target}" for target in runtime_allowlist])
    else:
        prompt_lines.append(
            " - (none discovered; edits outside protected files likely to be rejected)"
        )
    prompt_lines.extend(
        [
            "",
            "--- BEGIN AGENTS.md EXCERPT ---",
            agents_excerpt,
            "--- END AGENTS.md EXCERPT ---",
        ]
    )
    if log_excerpt:
        prompt_lines.extend(
            [
                "",
                "Latest log excerpt:",
                "```",
                log_excerpt,
                "```",
            ]
        )
    prompt_text = "\n".join(prompt_lines)
    prompt_file = context.codex_ci_dir / "discriminator_prompt.txt"
    prompt_file.write_text(prompt_text + "\n", encoding="utf-8")

    cmd = (
        _split_command(options.codex_bin)
        + ["exec"]
        + _split_command(options.codex_flags)
    )
    if options.codex_model:
        cmd += ["--model", options.codex_model]
    cmd += ["--cd", str(context.root), "--", prompt_text]

    print(f"[*] Invoking Codex with: {' '.join(cmd)}")
    completed = subprocess.run(
        cmd,
        cwd=context.root,
        env=env,
        capture_output=True,
        text=True,
    )
    log_file = context.codex_ci_dir / "discriminator_llm_response.log"
    timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    with open(log_file, "a", encoding="utf-8") as fh:
        fh.write(f"=== {timestamp} ===\n")
        fh.write((completed.stdout or "") + (completed.stderr or ""))
        fh.write("\n")


def _record_success(
    mode: str,
    slug: Optional[str],
    context: RexContext,
    env: dict[str, str],
) -> None:
    data = load_json(context.rex_agent_file)
    disc = data.setdefault("discriminator", {})
    disc["last_mode"] = mode
    disc["last_slug"] = slug
    disc["last_green_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    test_count = _collect_test_count(mode, slug, context, env)
    if test_count is not None:
        disc["last_test_count"] = test_count
    dump_json(context.rex_agent_file, data)

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/doctor.py ===
"""Diagnostics for rex-codex."""

from __future__ import annotations

from .utils import which

TOOLS = ("python3", "node", "npx", "docker")


def run_doctor() -> None:
    for tool in TOOLS:
        path = which(tool)
        if path:
            print(f"[doctor] {tool}: {path}")
        else:
            print(f"[doctor] {tool}: missing (install or add to PATH)")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/events.py ===
"""Event stream helpers for generator/discriminator progress."""

from __future__ import annotations

import json
import os
import time
from pathlib import Path
from typing import Any, Mapping

from .utils import ensure_dir, repo_root

_EVENTS_PATH_CACHE: Path | None = None
_MONITOR_EVENTS_PATH_CACHE: Path | None = None


def _default_events_path() -> Path:
    root = repo_root()
    return root / ".codex_ci" / "events.jsonl"


def _default_monitor_events_path() -> Path:
    root = repo_root()
    return root / ".agent" / "logs" / "events.jsonl"


def _resolve_events_path() -> Path:
    global _EVENTS_PATH_CACHE
    if _EVENTS_PATH_CACHE is not None:
        return _EVENTS_PATH_CACHE
    raw = os.environ.get("REX_EVENTS_FILE")
    if raw:
        candidate = Path(raw).expanduser()
    else:
        candidate = _default_events_path()
    ensure_dir(candidate.parent)
    _EVENTS_PATH_CACHE = candidate
    return candidate


def _resolve_monitor_events_path() -> Path:
    global _MONITOR_EVENTS_PATH_CACHE
    if _MONITOR_EVENTS_PATH_CACHE is not None:
        return _MONITOR_EVENTS_PATH_CACHE
    raw = os.environ.get("REX_MONITOR_EVENTS_FILE")
    if raw:
        candidate = Path(raw).expanduser()
    else:
        base = os.environ.get("LOG_DIR")
        if base:
            candidate = Path(base).expanduser() / "events.jsonl"
        else:
            candidate = _default_monitor_events_path()
    ensure_dir(candidate.parent)
    _MONITOR_EVENTS_PATH_CACHE = candidate
    return candidate


def events_path() -> Path:
    """Return the resolved events log path (creates the directory if needed)."""

    return _resolve_events_path()


def reset_events_cache() -> None:
    """Clear the cached events path (mostly for tests)."""

    global _EVENTS_PATH_CACHE
    global _MONITOR_EVENTS_PATH_CACHE
    _EVENTS_PATH_CACHE = None
    _MONITOR_EVENTS_PATH_CACHE = None


def _json_default(value: Any) -> Any:
    if isinstance(value, Path):
        return value.as_posix()
    if isinstance(value, set):
        return sorted(value)
    if isinstance(value, (list, dict, str, int, float, bool)) or value is None:
        return value
    return repr(value)


def emit_event(phase: str, type_: str, *, slug: str | None = None, **data: Any) -> None:
    """Append a structured event to the shared JSONL log.

    Best-effort: failures to serialise or write are swallowed so that progress
    reporting never interferes with the main generator/discriminator flow.
    """

    record: Mapping[str, Any] = {
        "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "phase": phase,
        "type": type_,
        "slug": slug,
        "data": data,
    }
    try:
        payload = json.dumps(record, ensure_ascii=False, default=_json_default)
    except Exception:
        return
    try:
        events_path = _resolve_events_path()
        with events_path.open("a", encoding="utf-8") as fh:
            fh.write(payload)
            fh.write("\n")
        _mirror_to_monitor(record)
    except Exception:
        # Never let monitoring abort the main flow.
        return


def _mirror_to_monitor(record: Mapping[str, Any]) -> None:
    try:
        monitor_path = _resolve_monitor_events_path()
    except Exception:
        return

    monitor_event = _to_monitor_event(record)
    if not monitor_event:
        return
    try:
        with monitor_path.open("a", encoding="utf-8") as fh:
            fh.write(
                json.dumps(monitor_event, ensure_ascii=False, default=_json_default)
            )
            fh.write("\n")
    except Exception:
        return


def _to_monitor_event(record: Mapping[str, Any]) -> Mapping[str, Any] | None:
    ts = record.get("ts")
    if not isinstance(ts, str):
        return None
    phase = str(record.get("phase", "")).strip()
    type_ = str(record.get("type", "")).strip()
    slug = record.get("slug")
    data = record.get("data") or {}
    if not isinstance(data, Mapping):
        data = {}

    level = _monitor_level(type_, data)
    status = _extract_status(data, level)
    progress = _extract_progress(data)
    task = _extract_task(slug, data)
    message = _compose_message(phase, type_, slug, data, status)

    meta = _extract_meta(data, phase, type_)
    if meta is None:
        meta = {}
    meta.setdefault("slug", slug)
    monitor_event = {
        "ts": ts,
        "level": level,
        "message": message,
    }
    if task:
        monitor_event["task"] = task
    if status:
        monitor_event["status"] = status
    if progress is not None:
        monitor_event["progress"] = progress
    if meta:
        monitor_event["meta"] = meta
    return monitor_event


def _monitor_level(type_: str, data: Mapping[str, Any]) -> str:
    explicit = str(data.get("level", "")).lower()
    if explicit in {"info", "warn", "warning", "error", "debug", "task", "progress"}:
        if explicit == "warning":
            return "warn"
        return explicit
    lowered = type_.lower()
    status = str(data.get("status", "")).lower()
    if "error" in lowered or "failed" in lowered or status in {"failed", "error"}:
        return "error"
    if "warn" in lowered or status in {"warning", "warn"}:
        return "warn"
    if "debug" in lowered:
        return "debug"
    if str(data.get("ok")).lower() == "false":
        return "error"
    return "info"


def _extract_status(data: Mapping[str, Any], level: str) -> str | None:
    status = data.get("status")
    if isinstance(status, str):
        return status
    if level == "error":
        return "failed"
    if level == "warn":
        return "warning"
    return None


def _extract_progress(data: Mapping[str, Any]) -> float | None:
    raw = data.get("progress")
    if isinstance(raw, (int, float)):
        try:
            value = float(raw)
        except (TypeError, ValueError):
            return None
        if value != value:  # NaN check
            return None
        return max(0.0, min(1.0, value))
    # Some events report percentages
    percent = data.get("percentage")
    if isinstance(percent, (int, float)):
        try:
            value = float(percent) / 100.0
        except (TypeError, ValueError):
            return None
        return max(0.0, min(1.0, value))
    return None


def _extract_task(slug: Any, data: Mapping[str, Any]) -> str | None:
    task = data.get("task")
    if isinstance(task, str) and task:
        return task
    if isinstance(slug, str) and slug:
        return slug
    stage = data.get("identifier")
    description = data.get("description")
    if isinstance(stage, str) and isinstance(description, str):
        return f"{stage} {description}".strip()
    return None


def _compose_message(
    phase: str,
    type_: str,
    slug: Any,
    data: Mapping[str, Any],
    status: str | None,
) -> str:
    headline = f"{phase}:{type_}" if phase else type_
    if isinstance(slug, str) and slug:
        headline = f"{slug} · {headline}"

    detail_keys = (
        "message",
        "summary",
        "description",
        "command",
        "guidance",
        "reason",
        "failure_reason",
        "note",
    )
    detail = None
    for key in detail_keys:
        value = data.get(key)
        if isinstance(value, str) and value.strip():
            detail = value.strip()
            break

    if detail is None:
        iteration = data.get("iteration")
        total = data.get("total_passes") or data.get("passes")
        if isinstance(iteration, int) and isinstance(total, int) and total > 0:
            detail = f"iteration {iteration}/{total}"
        elif isinstance(iteration, int):
            detail = f"iteration {iteration}"
        elif isinstance(total, int):
            detail = f"{total} total passes"
        elif status:
            detail = status

    if detail:
        return f"{headline} — {detail}"
    return headline


def _extract_meta(
    data: Mapping[str, Any], phase: str, type_: str
) -> Mapping[str, Any] | None:
    ignore_keys = {
        "message",
        "summary",
        "description",
        "command",
        "guidance",
        "reason",
        "failure_reason",
        "note",
        "status",
        "progress",
        "task",
        "level",
    }
    meta: dict[str, Any] = {"phase": phase, "type": type_}
    for key, value in data.items():
        if key in ignore_keys:
            continue
        if isinstance(value, (str, int, float, bool)) or value is None:
            meta[key] = value
        else:
            meta[key] = _json_default(value)
    return meta or None

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/generator.py ===
"""Deterministic spec generator implemented in Python."""

from __future__ import annotations

import ast
import difflib
import os
import re
import shlex
import shutil
import subprocess
import sys
import textwrap
import time
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from types import SimpleNamespace
from typing import Any, Dict, List, Optional, Sequence, Tuple

from .cards import FeatureCard, discover_cards, update_active_card
from .component_planner import ensure_component_plan
from .config import AGENT_SRC, DEFAULT_GENERATOR_MAX_FILES, DEFAULT_GENERATOR_MAX_LINES
from .events import emit_event, events_path
from .generator_ui import GeneratorHUD
from .hud import generator_snapshot_text
from .monitoring import ensure_monitor_server
from .playbook import build_playbook_artifacts
from .self_update import self_update
from .utils import (
    RexContext,
    activate_venv,
    dump_json,
    ensure_dir,
    ensure_python,
    ensure_requirements_installed,
    load_json,
    lock_file,
    repo_root,
    run,
    which,
)

PROGRESS_INTERVAL_SECONDS = max(
    5, int(os.environ.get("GENERATOR_PROGRESS_SECONDS", "15"))
)


def _ansi_palette() -> SimpleNamespace:
    disable = bool(os.environ.get("NO_COLOR")) or not sys.stdout.isatty()
    if disable:
        return SimpleNamespace(
            header="",
            accent="",
            success="",
            warning="",
            error="",
            dim="",
            reset="",
        )
    return SimpleNamespace(
        header="\x1b[95m",
        accent="\x1b[36m",
        success="\x1b[32m",
        warning="\x1b[33m",
        error="\x1b[31m",
        dim="\x1b[2m",
        reset="\x1b[0m",
    )


def _extract_section(lines: List[str], heading: str) -> List[str]:
    target = f"## {heading}".lower()
    start: Optional[int] = None
    for idx, line in enumerate(lines):
        if line.strip().lower() == target:
            start = idx + 1
            break
    if start is None:
        return []
    body: List[str] = []
    for line in lines[start:]:
        if line.strip().startswith("## "):
            break
        body.append(line.rstrip())
    while body and not body[0].strip():
        body.pop(0)
    while body and not body[-1].strip():
        body.pop()
    return body


def _extract_card_metadata(card_path: Path) -> Dict[str, object]:
    metadata: Dict[str, object] = {"title": card_path.stem.replace("-", " ").title()}
    try:
        text = card_path.read_text(encoding="utf-8")
    except OSError:
        return metadata
    lines = text.splitlines()
    for line in lines:
        if line.startswith("# "):
            metadata["title"] = line[2:].strip()
            break
    summary_section = _extract_section(lines, "Summary")
    metadata["summary"] = " ".join(summary_section).strip()
    acceptance_section = _extract_section(lines, "Acceptance Criteria")
    acceptance = [
        item.strip()[2:].strip()
        for item in acceptance_section
        if item.strip().startswith("- ")
    ]
    metadata["acceptance"] = acceptance
    return metadata


def _list_existing_specs(specs_dir: Path) -> List[str]:
    if not specs_dir.exists():
        return []
    items: List[str] = []
    for path in sorted(specs_dir.rglob("*.py")):
        try:
            items.append(str(path.relative_to(specs_dir)))
        except ValueError:
            items.append(path.name)
    return items


def _render_generator_dashboard(
    *,
    card: FeatureCard,
    specs_dir: Path,
    focus: str,
    passes: int,
    options: GeneratorOptions,
    metadata: Optional[Dict[str, object]] = None,
    existing_specs: Optional[List[str]] = None,
) -> None:
    palette = _ansi_palette()
    metadata = metadata or _extract_card_metadata(card.path)
    existing_specs = existing_specs or _list_existing_specs(specs_dir)
    header = f"{palette.header}Generator Dashboard{palette.reset}"
    divider = "-" * 62
    print(f"\n{header}")
    print(divider)
    title = metadata.get("title", card.slug)
    summary_text = metadata.get("summary", "")
    acceptance = metadata.get("acceptance") or []
    print(f"{palette.accent}Feature{palette.reset}: {card.slug} ({title})")
    print(f"{palette.accent}Status{palette.reset}: {card.status}")
    if summary_text:
        print(f"{palette.accent}Summary{palette.reset}: {summary_text}")
    if acceptance:
        print(f"{palette.accent}Acceptance Criteria{palette.reset}:")
        for item in acceptance:
            print(f"  - {item}")
    if existing_specs:
        specs_list = ", ".join(existing_specs)
        print(f"{palette.accent}Existing specs{palette.reset}: {specs_list}")
    else:
        print(f"{palette.accent}Existing specs{palette.reset}: (none yet)")
    print(
        f"{palette.accent}Focus{palette.reset}: {focus or 'default coverage guidance'}"
    )
    print(
        f"{palette.accent}Pass budget{palette.reset}: {passes} (continuous={options.continuous})"
    )
    print(divider)


def _summarize_diff(diff_text: str) -> Tuple[List[Dict[str, object]], Dict[str, int]]:
    entries: List[Dict[str, object]] = []
    totals = defaultdict(int)
    current: Optional[Dict[str, object]] = None
    for line in diff_text.splitlines():
        if line.startswith("diff --git "):
            if current:
                entries.append(current)
            parts = line.split()
            path = parts[-1] if parts else ""
            if path.startswith("b/"):
                path = path[2:]
            current = {
                "path": path,
                "status": "modified",
                "added": 0,
                "removed": 0,
                "added_tests": [],
                "removed_tests": [],
            }
        elif current is None:
            continue
        elif line.startswith("new file mode"):
            current["status"] = "new"
        elif line.startswith("deleted file mode"):
            current["status"] = "deleted"
        elif line.startswith("+++ b/"):
            if line.endswith("/dev/null"):
                current["status"] = "deleted"
        elif line.startswith("--- a/"):
            if line.endswith("/dev/null"):
                current["status"] = "new"
        elif line.startswith("+") and not line.startswith("+++"):
            current["added"] = current.get("added", 0) + 1
            totals["added_lines"] += 1
            stripped = line[1:].lstrip()
            if stripped.startswith("def test"):
                name = stripped.split("(", 1)[0].replace("def", "", 1).strip()
                current["added_tests"].append(name)
        elif line.startswith("-") and not line.startswith("---"):
            current["removed"] = current.get("removed", 0) + 1
            totals["removed_lines"] += 1
            stripped = line[1:].lstrip()
            if stripped.startswith("def test"):
                name = stripped.split("(", 1)[0].replace("def", "", 1).strip()
                current["removed_tests"].append(name)
    if current:
        entries.append(current)
    totals["files"] = len(entries)
    for entry in entries:
        added_tests = set(entry.get("added_tests", []))
        removed_tests = set(entry.get("removed_tests", []))
        modified_tests = sorted(added_tests & removed_tests)
        entry["modified_tests"] = modified_tests
        entry["added_tests"] = sorted(added_tests - removed_tests)
        entry["removed_tests"] = sorted(removed_tests - added_tests)
    return entries, totals


@dataclass
class _TestMetadata:
    name: str
    rel_path: Path
    docstring: str
    normalized_name: str
    normalized_doc: str
    tokens: set[str]
    acceptance_indexes: set[int]

    @property
    def display(self) -> str:
        return f"{self.rel_path.as_posix()}::{self.name}"


@dataclass
class _SpecTraceEntry:
    index: int
    text: str
    tests: List[_TestMetadata]


@dataclass
class _SpecTraceResult:
    entries: List[_SpecTraceEntry]
    missing: List[_SpecTraceEntry]
    orphans: List[_TestMetadata]
    section_lines: List[str]


def _spec_trace_payload(result: _SpecTraceResult) -> Dict[str, Any]:
    def _entry_payload(entry: _SpecTraceEntry) -> Dict[str, Any]:
        return {
            "index": entry.index,
            "text": entry.text,
            "tests": [test.display for test in entry.tests],
            "status": "covered" if entry.tests else "missing",
        }

    return {
        "entries": [_entry_payload(entry) for entry in result.entries],
        "missing": [_entry_payload(entry) for entry in result.missing],
        "orphans": [orphan.display for orphan in result.orphans],
    }


def _print_diff_summary(diff_text: str) -> None:
    entries, totals = _summarize_diff(diff_text)
    if not entries:
        return
    palette = _ansi_palette()
    files_changed = totals.get("files", 0)
    added_lines = totals.get("added_lines", 0)
    removed_lines = totals.get("removed_lines", 0)
    print(
        f"{palette.accent}Diff summary{palette.reset}: {files_changed} files, "
        f"+{added_lines} / -{removed_lines} lines"
    )
    for entry in entries:
        path = entry["path"]
        status = entry["status"]
        added = entry["added"]
        removed = entry["removed"]
        status_label = status
        if status == "new":
            status_label = f"{palette.success}new{palette.reset}"
        elif status == "deleted":
            status_label = f"{palette.warning}deleted{palette.reset}"
        changes = []
        if added:
            changes.append(f"+{added}")
        if removed:
            changes.append(f"-{removed}")
        change_text = ", ".join(changes) if changes else "no line changes"
        print(f"  • {path} ({status_label}, {change_text})")
        for label, tests in (
            ("added", entry["added_tests"]),
            ("modified", entry["modified_tests"]),
            ("removed", entry["removed_tests"]),
        ):
            if tests:
                joined = ", ".join(tests)
                print(f"      {label} tests: {joined}")


def _normalize_spec_text(text: str) -> str:
    return re.sub(r"[^a-z0-9]+", "", text.lower())


def _tokenize_spec_text(text: str) -> set[str]:
    return {token for token in re.split(r"[^a-z0-9]+", text.lower()) if token}


_AC_PATTERN = re.compile(r"AC#(\d+)", re.IGNORECASE)


def _attribute_chain(node: ast.AST) -> List[str]:
    parts: List[str] = []
    current = node
    while isinstance(current, ast.Attribute):
        parts.append(current.attr)
        current = current.value
    if isinstance(current, ast.Name):
        parts.append(current.id)
    return list(reversed(parts))


def _literal_int(node: ast.AST) -> Optional[int]:
    if isinstance(node, ast.Constant) and isinstance(node.value, int):
        return int(node.value)
    if isinstance(node, ast.Num):  # pragma: no cover - python <3.8 compat
        return int(node.n)
    return None


def _extract_acceptance_indexes(node: ast.AST) -> set[int]:
    indexes: set[int] = set()
    docstring = ast.get_docstring(node) or ""
    for match in _AC_PATTERN.findall(docstring):
        try:
            indexes.add(int(match))
        except ValueError:
            continue
    for decorator in node.decorator_list:
        if isinstance(decorator, ast.Call):
            func = decorator.func
            if isinstance(func, ast.Attribute) and func.attr == "ac":
                chain = _attribute_chain(func.value)
                if chain and chain[-1] == "mark":
                    for arg in decorator.args:
                        value = _literal_int(arg)
                        if value is not None:
                            indexes.add(value)
            elif isinstance(func, ast.Name) and func.id == "ac":
                for arg in decorator.args:
                    value = _literal_int(arg)
                    if value is not None:
                        indexes.add(value)
    return indexes


def _collect_test_metadata(root: Path, specs_dir: Path) -> List[_TestMetadata]:
    if not specs_dir.exists():
        return []
    results: List[_TestMetadata] = []
    for path in sorted(specs_dir.rglob("*.py")):
        try:
            source = path.read_text(encoding="utf-8")
        except OSError:
            continue
        try:
            tree = ast.parse(source, filename=str(path))
        except SyntaxError:
            continue
        rel_path = path.relative_to(root)
        for node in tree.body:
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if not node.name.startswith("test"):
                    continue
                docstring = ast.get_docstring(node) or ""
                normalized_name = _normalize_spec_text(node.name)
                normalized_doc = _normalize_spec_text(docstring)
                tokens = _tokenize_spec_text(node.name) | _tokenize_spec_text(docstring)
                acceptance_indexes = _extract_acceptance_indexes(node)
                results.append(
                    _TestMetadata(
                        name=node.name,
                        rel_path=rel_path,
                        docstring=docstring.strip(),
                        normalized_name=normalized_name,
                        normalized_doc=normalized_doc,
                        tokens=tokens,
                        acceptance_indexes=acceptance_indexes,
                    )
                )
    return results


def _bullet_matches(
    bullet_norm: str,
    bullet_tokens: set[str],
    candidate: _TestMetadata,
) -> bool:
    if not bullet_norm and not bullet_tokens:
        return False
    if bullet_norm and bullet_norm in candidate.normalized_doc:
        return True
    if bullet_norm and bullet_norm in candidate.normalized_name:
        return True
    if not bullet_tokens:
        return False
    shared = bullet_tokens & candidate.tokens
    if shared == bullet_tokens:
        return True
    if len(shared) >= max(1, len(bullet_tokens) - 1):
        return True
    return False


def _build_spec_trace_result(
    *,
    card: FeatureCard,
    slug: str,
    context: RexContext,
) -> Optional[_SpecTraceResult]:
    metadata = _extract_card_metadata(card.path)
    acceptance = metadata.get("acceptance") or []
    root = context.root
    specs_dir = root / "tests" / "feature_specs" / slug
    tests = _collect_test_metadata(root, specs_dir)
    if not acceptance and not tests:
        return None

    matched: set[str] = set()
    entries: List[_SpecTraceEntry] = []
    for index, text in enumerate(acceptance, start=1):
        matches = [
            candidate for candidate in tests if index in candidate.acceptance_indexes
        ]
        matches_sorted = sorted(matches, key=lambda c: c.display)
        for candidate in matches_sorted:
            matched.add(candidate.display)
        entries.append(_SpecTraceEntry(index=index, text=text, tests=matches_sorted))

    section_lines: List[str] = []
    if entries:
        for entry in entries:
            section_lines.append(f'- [AC#{entry.index}] "{entry.text}"')
            if entry.tests:
                for linked_test in entry.tests:
                    section_lines.append(
                        f"  -> [AC#{entry.index}] {linked_test.display}"
                    )
            else:
                section_lines.append(f"  -> [AC#{entry.index}] (missing)")
    else:
        section_lines.append("- (no acceptance criteria listed)")

    missing = [entry for entry in entries if not entry.tests]
    orphans = sorted(
        [test for test in tests if test.display not in matched],
        key=lambda item: item.display,
    )
    return _SpecTraceResult(
        entries=entries, missing=missing, orphans=orphans, section_lines=section_lines
    )


def _replace_card_section(
    card_path: Path, heading: str, content_lines: Sequence[str]
) -> bool:
    try:
        original = card_path.read_text(encoding="utf-8")
    except OSError:
        return False
    lines = original.splitlines()
    heading_lower = f"## {heading}".lower()
    start_idx: Optional[int] = None
    for idx, line in enumerate(lines):
        if line.strip().lower() == heading_lower:
            start_idx = idx
            break
    if start_idx is None:
        # Append heading at the end
        if lines and lines[-1].strip():
            lines.append("")
        lines.append(f"## {heading}")
        start_idx = len(lines) - 1
        lines.append("")
    end_idx = start_idx + 1
    while end_idx < len(lines) and not lines[end_idx].strip().startswith("## "):
        end_idx += 1

    replacement: List[str] = [""]
    replacement.extend(content_lines)
    if content_lines:
        replacement.append("")
    new_lines = lines[: start_idx + 1] + replacement + lines[end_idx:]

    # Remove duplicate trailing blank lines
    while (
        len(new_lines) > 1 and not new_lines[-1].strip() and not new_lines[-2].strip()
    ):
        new_lines.pop()

    updated = "\n".join(new_lines)
    if original.rstrip("\n") == updated.rstrip("\n"):
        return False
    card_path.write_text(updated + "\n", encoding="utf-8")
    return True


def _update_spec_trace(
    *,
    card: FeatureCard,
    slug: str,
    context: RexContext,
) -> Tuple[Optional[_SpecTraceResult], bool]:
    result = _build_spec_trace_result(card=card, slug=slug, context=context)
    if result is None:
        return None, False
    changed = _replace_card_section(card.path, "Spec Trace", result.section_lines)
    return result, changed


def _print_spec_trace_result(result: _SpecTraceResult) -> None:
    palette = _ansi_palette()
    print(f"{palette.accent}Spec Trace coverage{palette.reset}:")
    if not result.entries:
        print("  (no acceptance criteria listed)")
    for entry in result.entries:
        label = f"[AC#{entry.index}] {entry.text}"
        print(f"  {label}")
        if entry.tests:
            for matched in entry.tests:
                print(f"      -> {matched.display}")
        else:
            print(f"      -> {palette.warning}(missing){palette.reset}")
    if result.missing:
        for entry in result.missing:
            print(
                f"{palette.warning}[generator] Acceptance criterion lacks coverage:{palette.reset} "
                f"[AC#{entry.index}] {entry.text}"
            )
    if result.orphans:
        print(
            f"{palette.warning}[generator] The following tests do not map to any acceptance bullet:{palette.reset}"
        )
        for orphan in result.orphans:
            hint = (
                f"docstring: {orphan.docstring}" if orphan.docstring else "no docstring"
            )
            print(f"      - {orphan.display} ({hint})")


def _load_pass_durations(context: RexContext) -> List[float]:
    data = load_json(context.rex_agent_file)
    generator_state = data.get("generator", {})
    durations = generator_state.get("pass_durations", [])
    if isinstance(durations, list):
        return [float(value) for value in durations if isinstance(value, (int, float))]
    return []


def _average_pass_duration(context: RexContext) -> Optional[float]:
    durations = _load_pass_durations(context)
    if len(durations) < 2:
        return None
    return sum(durations) / len(durations)


def _record_pass_duration(context: RexContext, seconds: float) -> None:
    data = load_json(context.rex_agent_file)
    generator_state = data.setdefault("generator", {})
    durations = generator_state.get("pass_durations", [])
    if not isinstance(durations, list):
        durations = []
    durations.append(round(seconds, 2))
    generator_state["pass_durations"] = durations[-10:]
    dump_json(context.rex_agent_file, data)


def _emit_codex_updates(chunk: str, palette: SimpleNamespace, last_update: str) -> str:
    lines = [line.strip() for line in chunk.splitlines() if line.strip()]
    if not lines:
        return last_update
    interesting: List[str] = []
    for line in lines:
        if line.startswith(
            ("diff --git", "index ", "--- ", "+++ ", "@@ ", "+", "-", "Applying diff")
        ):
            continue
        if line.startswith("Total patch size"):
            continue
        interesting.append(line)
    candidates = interesting or lines
    for line in candidates[-3:]:
        snippet = line
        if len(snippet) > 160:
            snippet = snippet[:157] + "…"
        if snippet and snippet != last_update:
            print(f"{palette.accent}[generator] Codex: {snippet}{palette.reset}")
            last_update = snippet
    return last_update


def _diagnose_missing_cards(statuses: List[str], context: RexContext) -> None:
    cards = discover_cards(context=context)
    if not cards:
        print("[generator] No Feature Cards found in documents/feature_cards/.")
        return
    palette = _ansi_palette()
    print("[generator] Feature Cards present but none matched the requested statuses.")
    for card in cards:
        suggestion = ""
        for target in statuses:
            if not target:
                continue
            ratio = difflib.SequenceMatcher(None, card.status, target).ratio()
            if ratio >= 0.75 and card.status != target:
                suggestion = (
                    f' ({palette.warning}did you mean "{target}"?{palette.reset})'
                )
                break
        status_display = f"status={card.status}"
        print(f"  - {card.slug}: {status_display}{suggestion}")


def _default_ui_hz() -> float:
    raw = os.environ.get("GENERATOR_UI_HZ")
    if raw is None:
        return 1.0
    try:
        value = float(raw)
    except ValueError:
        return 5.0
    return value if value > 0 else 5.0


def _parse_env_toggle(raw: Optional[str]) -> Optional[bool]:
    if raw is None:
        return None
    value = raw.strip().lower()
    if not value or value == "auto":
        return None
    if value in {"1", "true", "yes", "on"}:
        return True
    if value in {"0", "false", "no", "off"}:
        return False
    return None


def _default_popout_enabled() -> bool:
    env = _parse_env_toggle(os.environ.get("GENERATOR_UI_POPOUT"))
    if env is not None:
        return env
    return False


def _default_popout_linger() -> float:
    raw = os.environ.get("GENERATOR_UI_LINGER")
    if raw is None:
        if repo_root().name == "rex_codex_agent":
            return 30.0
        return 5.0
    try:
        value = float(raw)
    except ValueError:
        return 5.0
    return max(0.0, value)


def _default_scrub_specs_flag() -> Optional[bool]:
    return _parse_env_toggle(os.environ.get("GENERATOR_SCRUB_SPECS"))


def _default_ui_mode() -> str:
    value = os.environ.get("GENERATOR_UI")
    if not value:
        return "off"
    normalized = value.strip().lower()
    if normalized == "auto":
        return "monitor"
    return normalized


@dataclass
class GeneratorOptions:
    continuous: bool = True
    max_passes: int = int(os.environ.get("GENERATOR_MAX_PASSES", "5"))
    focus: str = ""
    card_path: Optional[Path] = None
    iterate_all: bool = False
    statuses: List[str] = field(default_factory=lambda: ["proposed"])
    codex_bin: str = os.environ.get("CODEX_BIN", "npx --yes @openai/codex")
    codex_flags: str = os.environ.get("CODEX_FLAGS", "--yolo")
    codex_model: str = os.environ.get("MODEL", "")
    verbose: bool = True
    tail_lines: int = 0
    reconcile_only: bool = False
    ui_mode: str = field(default_factory=_default_ui_mode)
    ui_refresh_hz: float = field(default_factory=_default_ui_hz)
    spawn_popout: bool = field(default_factory=_default_popout_enabled)
    popout_linger: float = field(default_factory=_default_popout_linger)
    scrub_specs: Optional[bool] = field(default_factory=_default_scrub_specs_flag)


@dataclass
class _CodexResult:
    returncode: int
    stdout: str
    stderr: str
    elapsed_seconds: int


def parse_statuses(raw: str | None) -> List[str]:
    if not raw:
        return ["proposed"]
    tokens = [piece.strip().lower() for piece in raw.split(",") if piece.strip()]
    return tokens or ["proposed"]


def _split_command(raw: str) -> List[str]:
    import shlex

    return shlex.split(raw)


_TERMINAL_CANDIDATES: List[Tuple[str, List[str]]] = [
    ("gnome-terminal", ["--title", "{title}", "--", "bash", "-lc", "{command}"]),
    ("kitty", ["--title", "{title}", "bash", "-lc", "{command}"]),
    ("wezterm", ["start", "--", "bash", "-lc", "{command}"]),
    ("alacritty", ["-t", "{title}", "-e", "bash", "-lc", "{command}"]),
    ("x-terminal-emulator", ["-T", "{title}", "-e", "bash", "-lc", "{command}"]),
    ("xterm", ["-T", "{title}", "-hold", "-e", "bash", "-lc", "{command}"]),
]


def _format_terminal_args(
    executable: str, tokens: Sequence[str], *, title: str, command: str
) -> List[str]:
    args = [executable]
    for token in tokens:
        if token == "{title}":
            args.append(title)
        elif token == "{command}":
            args.append(command)
        else:
            args.append(token)
    return args


def _launch_terminal(
    title: str, command: str
) -> Optional[Tuple[subprocess.Popen, str]]:
    for exe, tokens in _TERMINAL_CANDIDATES:
        exe_path = which(exe)
        if not exe_path:
            continue
        argv = _format_terminal_args(exe_path, tokens, title=title, command=command)
        try:
            proc = subprocess.Popen(argv, start_new_session=True)
            return proc, exe
        except OSError as exc:  # pragma: no cover - depends on local terminal setup
            print(f"[generator] Failed to launch {exe}: {exc}")
            continue
    print("[generator] Unable to launch HUD popout; no terminal emulator detected.")
    return None


def _spawn_generator_tui_popout(
    *,
    context: RexContext,
    slug: str,
) -> Optional[subprocess.Popen]:
    env_toggle = _parse_env_toggle(os.environ.get("GENERATOR_UI_TUI"))
    if env_toggle is False:
        return None
    tui_dir = context.root / "tui"
    if not tui_dir.exists() or not tui_dir.is_dir():
        return None
    if which("npm") is None:
        return None
    if which("node") is None:
        return None
    events_file = context.codex_ci_dir / "events.jsonl"
    diff_file = context.codex_ci_dir / "generator_patch.diff"
    install_cmd = (
        "if [ ! -d tui/node_modules ]; then "
        "npm --prefix tui install --no-fund --no-audit >/dev/null 2>&1 || exit 1; "
        "fi"
    )
    build_cmd = (
        "if [ ! -f tui/dist/index.js ]; then "
        "npm --prefix tui run build >/dev/null 2>&1 || exit 1; "
        "fi"
    )
    env_assignments = " ".join(
        [
            "FORCE_COLOR=1",
            f"TUI_SLUG={shlex.quote(slug)}",
            f"TUI_REPO_ROOT={shlex.quote(str(context.root))}",
            f"TUI_EVENTS_FILE={shlex.quote(str(events_file))}",
            f"TUI_DIFF_FILE={shlex.quote(str(diff_file))}",
        ]
    )
    entry_path = tui_dir / "dist" / "index.js"
    npm_command = f"{env_assignments} node {shlex.quote(str(entry_path))}"
    shell_command = (
        f"cd {shlex.quote(str(context.root))} && "
        f"{install_cmd} && {build_cmd} && {npm_command}"
    )
    title = f"rex-codex HUD :: {slug}"
    launched = _launch_terminal(title, shell_command)
    if launched is None:
        return None
    process, exe = launched
    print(f"[generator] HUD popout launched via {exe} (tui).")
    return process


def _spawn_generator_popout(
    *,
    context: RexContext,
    slug: str,
    refresh_hz: float,
    linger: float,
) -> Optional[subprocess.Popen]:
    tui_process = _spawn_generator_tui_popout(context=context, slug=slug)
    if tui_process is not None:
        return tui_process
    refresh_seconds = max(0.2, 1.0 / max(refresh_hz, 0.1))
    command_parts = [
        "./bin/rex-codex",
        "hud",
        "generator",
        "--slug",
        slug,
        "--follow",
        f"--refresh={refresh_seconds:.2f}",
        f"--linger={linger:.2f}",
    ]
    hud_command = shlex.join(command_parts)
    shell_command = f"cd {shlex.quote(str(context.root))} && {hud_command}"
    title = f"rex-codex HUD :: {slug}"
    launched = _launch_terminal(title, shell_command)
    if launched is None:
        return None
    process, exe = launched
    print(f"[generator] HUD popout launched via {exe}.")
    return process


def _should_scrub_specs(context: RexContext, option: Optional[bool]) -> bool:
    if option is not None:
        return option
    return context.root.name == "rex_codex_agent"


def _scrub_spec_directory(slug: str, context: RexContext) -> None:
    specs_dir = context.root / "tests" / "feature_specs" / slug
    if not specs_dir.exists():
        return
    print(f"[generator] Scrubbing spec shard: {context.relative(specs_dir)}")
    shutil.rmtree(specs_dir, ignore_errors=True)


def _run_codex_with_progress(
    cmd: Sequence[str],
    *,
    cwd: Path,
    verbose: bool,
    progress_label: str,
    slug: str | None = None,
) -> _CodexResult:
    start = time.time()
    emit_event(
        "generator",
        "codex_started",
        slug=slug,
        command=list(cmd[:-1]) + ["<prompt>"] if cmd else [],
    )
    process = subprocess.Popen(
        cmd,
        cwd=cwd,
        text=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    stdout_buffer: List[str] = []
    stderr_buffer: List[str] = []
    palette = _ansi_palette()
    last_update = ""
    while True:
        try:
            stdout, stderr = process.communicate(timeout=PROGRESS_INTERVAL_SECONDS)
            if stdout:
                if not isinstance(stdout, str):
                    stdout = stdout.decode()
                stdout_buffer.append(stdout)
                if verbose:
                    last_update = _emit_codex_updates(stdout, palette, last_update)
            if stderr:
                if not isinstance(stderr, str):
                    stderr = stderr.decode()
                stderr_buffer.append(stderr)
            break
        except subprocess.TimeoutExpired as exc:
            # exc.output / exc.stderr contain partial data when text=True and pipes are used
            if exc.output:
                chunk = (
                    exc.output if isinstance(exc.output, str) else exc.output.decode()
                )
                stdout_buffer.append(chunk)
                if verbose:
                    last_update = _emit_codex_updates(chunk, palette, last_update)
            if exc.stderr:
                chunk_err = (
                    exc.stderr if isinstance(exc.stderr, str) else exc.stderr.decode()
                )
                stderr_buffer.append(chunk_err)
            elapsed = int(time.time() - start)
            if verbose:
                print(f"[generator] {progress_label}… {elapsed}s elapsed", flush=True)
            emit_event(
                "generator",
                "codex_heartbeat",
                slug=slug,
                seconds=elapsed,
                progress_label=progress_label,
            )
    elapsed_total = int(time.time() - start)
    stdout_combined = "".join(stdout_buffer)
    stderr_combined = "".join(stderr_buffer)
    emit_event(
        "generator",
        "codex_completed",
        slug=slug,
        returncode=int(process.returncode or 0),
        elapsed_seconds=elapsed_total,
    )
    return _CodexResult(
        returncode=process.returncode or 0,
        stdout=stdout_combined,
        stderr=stderr_combined,
        elapsed_seconds=elapsed_total,
    )


def _run_card_with_ui(
    card: FeatureCard, options: GeneratorOptions, context: RexContext
) -> int:
    ui_mode = (options.ui_mode or "monitor").lower()
    popout_requested = ui_mode == "popout"
    if ui_mode in {"auto", "plain"}:
        ui_mode = "monitor"
    if popout_requested:
        ui_mode = "monitor"
        options.spawn_popout = True
    options.ui_mode = ui_mode

    if options.reconcile_only:
        return _process_card(card, options, context)

    if _should_scrub_specs(context, options.scrub_specs):
        _scrub_spec_directory(card.slug, context)

    events_file = events_path()
    if ui_mode != "off":
        try:
            events_file.unlink()
        except FileNotFoundError:
            pass

    # Build deterministic playbook artefacts before planning/generation.
    try:
        build_playbook_artifacts(card=card, context=context)
    except Exception as exc:  # pragma: no cover - defensive logging
        print(f"[generator] Failed to build playbook artefacts: {exc}")

    if os.environ.get("REX_DISABLE_PLANNER", "").lower() not in {"1", "true", "yes"}:
        ensure_component_plan(
            card=card,
            context=context,
            codex_bin=options.codex_bin,
            codex_flags=options.codex_flags,
            codex_model=options.codex_model,
            verbose=options.verbose,
        )

    if options.spawn_popout and ui_mode == "monitor":
        popout_launched = _spawn_generator_popout(
            context=context,
            slug=card.slug,
            refresh_hz=options.ui_refresh_hz,
            linger=options.popout_linger,
        )
        if popout_launched is None and popout_requested:
            print(
                "[generator] Popout HUD requested but no terminal emulator was launched."
            )

    if ui_mode == "snapshot":
        exit_code = _process_card(card, options, context)
        try:
            snapshot = generator_snapshot_text(card.slug, events_file)
            if snapshot:
                print(snapshot, end="")
        except Exception:
            pass
        status_label = "PASS" if exit_code == 0 else f"EXIT {exit_code}"
        console_log = context.codex_ci_dir / f"generator_console_{card.slug}.log"
        print(
            f"[generator] Finished {card.slug} ({status_label}). Console log: {console_log}"
        )
        return exit_code

    if ui_mode == "off":
        return _process_card(card, options, context)

    hud = GeneratorHUD(
        slug=card.slug,
        codex_ci_dir=context.codex_ci_dir,
        ui_mode=ui_mode,
        refresh_hz=options.ui_refresh_hz,
    )
    if not hud.enabled:
        exit_code = _process_card(card, options, context)
        try:
            snapshot = generator_snapshot_text(card.slug, events_file)
            if snapshot:
                print(snapshot, end="")
        except Exception:
            pass
        status_label = "PASS" if exit_code == 0 else f"EXIT {exit_code}"
        console_log = context.codex_ci_dir / f"generator_console_{card.slug}.log"
        print(
            f"[generator] Finished {card.slug} ({status_label}). Console log: {console_log}"
        )
        return exit_code

    exit_code = 0
    with hud:
        exit_code = _process_card(card, options, context)
    try:
        snapshot = generator_snapshot_text(card.slug, events_file)
        if snapshot:
            print(snapshot, end="")
    except Exception:
        pass
    hud.print_footer(exit_code)
    return exit_code


def run_generator(
    options: GeneratorOptions, *, context: RexContext | None = None
) -> int:
    context = context or RexContext.discover()
    ensure_monitor_server(context, open_browser=True)
    self_update()
    ensure_dir(context.codex_ci_dir)
    lock_path = context.codex_ci_dir / "rex_generator.lock"
    with lock_file(lock_path):
        ensure_python(context, quiet=True)
        env_verbose = os.environ.get("GENERATOR_DEBUG")
        if env_verbose is not None:
            options.verbose = env_verbose.lower() not in {"0", "false", ""}
        requirements_template = AGENT_SRC / "templates" / "requirements-dev.txt"
        ensure_requirements_installed(context, requirements_template)
        if options.scrub_specs is None:
            options.scrub_specs = _should_scrub_specs(context, None)
        cards: List[FeatureCard]
        if options.card_path:
            if not options.card_path.exists():
                print(f"[generator] Feature Card not found: {options.card_path}")
                return 1
            slug = options.card_path.stem
            cards = [
                FeatureCard(
                    path=options.card_path,
                    slug=slug,
                    status=options.statuses[0] if options.statuses else "unknown",
                )
            ]
        else:
            cards = discover_cards(statuses=options.statuses, context=context)

        if not cards:
            status_list = ", ".join(options.statuses)
            print(f"[generator] No Feature Cards with statuses: {status_list}")
            if options.statuses:
                _diagnose_missing_cards(options.statuses, context)
            return 1

        if options.reconcile_only:
            targets = cards if options.iterate_all else [cards[0]]
            exit_status = 0
            for card in targets:
                exit_status = max(exit_status, _reconcile_card(card, context))
            return exit_status

        if options.iterate_all:
            for card in cards:
                print(f"[generator] === Processing card {card.path} ===")
                exit_code = _run_card_with_ui(card, options, context)
                if exit_code != 0:
                    return exit_code
            return 0

        return _run_card_with_ui(cards[0], options, context)


def _process_card(
    card: FeatureCard, options: GeneratorOptions, context: RexContext
) -> int:
    slug = card.slug
    status = card.status
    focus = options.focus
    passes = options.max_passes if options.continuous else 1
    specs_dir = context.root / "tests" / "feature_specs" / slug
    metadata = _extract_card_metadata(card.path)
    existing_specs = _list_existing_specs(specs_dir)

    update_active_card(context, card=card)
    _render_generator_dashboard(
        card=card,
        specs_dir=specs_dir,
        focus=focus,
        passes=passes,
        options=options,
        metadata=metadata,
        existing_specs=existing_specs,
    )
    emit_event(
        "generator",
        "feature_started",
        slug=slug,
        title=str(metadata.get("title", card.slug)),
        status=status,
        card_path=str(card.relative_path),
        summary=metadata.get("summary"),
        acceptance=metadata.get("acceptance") or [],
        existing_specs=existing_specs,
        focus=focus,
        passes=passes,
        continuous=options.continuous,
    )

    for iteration in range(1, passes + 1):
        avg_duration = _average_pass_duration(context)
        if avg_duration and avg_duration >= 20:
            print(
                f"[generator] Recent passes averaged {avg_duration:.1f}s; Codex may report progress more slowly."
            )
        print(
            f"[generator] Iteration {iteration}/{passes} (slug: {slug}, status: {status})"
        )
        iteration_start = time.perf_counter()
        emit_event(
            "generator",
            "iteration_started",
            slug=slug,
            iteration=iteration,
            total_passes=passes,
            focus=focus,
            status=status,
        )
        exit_code, _ = _run_once(
            card=card,
            slug=slug,
            status=status,
            focus=focus,
            generation_pass=iteration,
            total_passes=passes,
            options=options,
            context=context,
        )
        elapsed = time.perf_counter() - iteration_start
        if exit_code == 0:
            _record_pass_duration(context, elapsed)
        emit_event(
            "generator",
            "iteration_completed",
            slug=slug,
            iteration=iteration,
            total_passes=passes,
            exit_code=exit_code,
            elapsed_seconds=round(elapsed, 2),
        )
        if exit_code != 0:
            emit_event(
                "generator",
                "feature_failed",
                slug=slug,
                iteration=iteration,
                exit_code=exit_code,
            )
            return exit_code

        _run_pytest_snapshot(slug, context)
        critic_ok, critic_focus = _run_critic(
            card=card,
            slug=slug,
            generation_pass=iteration,
            options=options,
            context=context,
        )
        if critic_ok:
            print(f"[generator] Critic returned DONE after pass {iteration}")
            emit_event(
                "generator",
                "critic_guidance",
                slug=slug,
                iteration=iteration,
                done=True,
                guidance="DONE",
            )
            emit_event(
                "generator",
                "feature_completed",
                slug=slug,
                iteration=iteration,
            )
            return 0
        if not critic_focus:
            print("[generator] Critic response empty; stopping.")
            emit_event(
                "generator",
                "critic_guidance",
                slug=slug,
                iteration=iteration,
                done=False,
                guidance="",
            )
            return 5
        print("[generator] Critic requested coverage updates:")
        print(critic_focus)
        emit_event(
            "generator",
            "critic_guidance",
            slug=slug,
            iteration=iteration,
            done=False,
            guidance=critic_focus,
        )
        focus = critic_focus

    print(f"[generator] Hit max passes ({passes}) without critic approval.")
    emit_event(
        "generator",
        "feature_failed",
        slug=slug,
        iteration=passes,
        exit_code=6,
        reason="max_passes_exhausted",
    )
    return 6


def _run_once(
    *,
    card: FeatureCard,
    slug: str,
    status: str,
    focus: str,
    generation_pass: int,
    total_passes: int,
    options: GeneratorOptions,
    context: RexContext,
) -> Tuple[int, Optional[str]]:
    root = context.root
    specs_dir = root / "tests" / "feature_specs" / slug
    specs_dir.mkdir(parents=True, exist_ok=True)

    try:
        build_playbook_artifacts(card=card, context=context)
    except Exception as exc:  # pragma: no cover - defensive logging
        print(f"[generator] Failed to refresh playbook artefacts in run loop: {exc}")

    card_path = root / "documents" / "feature_cards" / f"{slug}.md"
    baseline_card_text: Optional[str] = None
    if card_path.exists():
        try:
            baseline_card_text = card_path.read_text(encoding="utf-8")
        except OSError:
            baseline_card_text = None
    spec_trace_result: Optional[_SpecTraceResult] = None
    card_trace_changed = False

    prompt_path = context.codex_ci_dir / "generator_prompt.txt"
    response_path = context.codex_ci_dir / "generator_response.log"
    patch_path = context.codex_ci_dir / "generator_patch.diff"

    prompt = _build_prompt(card, slug, focus, generation_pass, context)
    prompt_path.write_text(prompt, encoding="utf-8")

    cmd = (
        _split_command(options.codex_bin)
        + ["exec"]
        + _split_command(options.codex_flags)
    )
    if options.codex_model:
        cmd += ["--model", options.codex_model]
    cmd += ["--cd", str(root), "--", prompt]

    if options.verbose:
        print("[generator] Calling Codex CLI…")
    completed = _run_codex_with_progress(
        cmd,
        cwd=root,
        verbose=options.verbose,
        progress_label=f"Codex CLI running (pass {generation_pass}/{total_passes})",
        slug=slug,
    )
    response_path.write_text(
        (completed.stdout or "") + ("\n" if completed.stdout else ""),
        encoding="utf-8",
    )
    if options.verbose:
        print(f"[generator] Codex CLI finished in {completed.elapsed_seconds}s.")
    if completed.returncode != 0:
        stderr = completed.stderr or ""
        response_path.write_text(
            response_path.read_text(encoding="utf-8") + stderr,
            encoding="utf-8",
        )
        print(stderr, file=sys.stderr)
        return 2, None

    diff_text = _extract_diff(response_path, slug)
    patch_path.write_text(diff_text, encoding="utf-8")
    entries, totals = _summarize_diff(diff_text)
    emit_event(
        "generator",
        "diff_summary",
        slug=slug,
        files=entries,
        totals=dict(totals),
    )
    if not diff_text.strip():
        print("[generator] Codex response did not contain a usable diff")
        return 3, None

    if not _enforce_patch_size(diff_text):
        return 3, None

    if not _validate_card_diff(diff_text, slug):
        print(
            "[generator] Codex attempted to modify a protected part of the Feature Card (e.g. the `status:` line)."
        )
        print(
            "[generator] Rejected. Only append inside '## Links' or '## Spec Trace' as documented in AGENTS.md."
        )
        return 3, None

    if options.verbose:
        print(f"[generator] Codex response saved to {context.relative(response_path)}")
        print(f"[generator] Applying diff from {context.relative(patch_path)}:")
        _print_diff_preview(diff_text)
        _print_diff_summary(diff_text)

    applied, patch_error = _apply_patch(patch_path, root)
    if not applied:
        print("[generator] Failed to apply Codex diff.")
        if patch_error:
            tail = "\n".join(patch_error.splitlines()[-8:])
            print(tail)
        print(
            f"[generator] Inspect {context.relative(patch_path)} for the diff and {context.relative(response_path)} for raw output."
        )
        print(
            "[generator] Tip: rerun with `./rex-codex generator --tail 200` to review the Codex response."
        )
        return 4, None
    if options.verbose:
        print("[generator] Diff applied successfully.")

    if card_path.exists():
        spec_trace_result, card_trace_changed = _update_spec_trace(
            card=card, slug=slug, context=context
        )

    if not _guard_card_edits(slug, root, baseline_card_text):
        _revert_generated_files(slug, root)
        return 7, None

    if not _enforce_hermetic_tests(slug, root):
        _revert_generated_files(slug, root)
        return 7, None

    if card_trace_changed:
        run(["git", "add", str(card_path)], cwd=root, check=False)
    if spec_trace_result:
        _print_spec_trace_result(spec_trace_result)
        emit_event(
            "generator",
            "spec_trace_update",
            slug=slug,
            changed=card_trace_changed,
            coverage=_spec_trace_payload(spec_trace_result),
        )

    if status == "proposed":
        _update_metadata(card, slug, context)
    print(f"[generator] Specs updated from {card.path}")
    emit_event("generator", "feature_specs_updated", slug=slug)
    return 0, None


def _build_prompt(
    card: FeatureCard, slug: str, focus: str, generation_pass: int, context: RexContext
) -> str:
    agents_excerpt = (context.root / "AGENTS.md").read_text(
        encoding="utf-8", errors="ignore"
    )
    card_text = card.path.read_text(encoding="utf-8")
    existing = _append_existing_tests(slug, context)
    playbook_prompt_path = context.codex_ci_dir / f"playbook_{slug}.prompt"
    playbook_block = ""
    if playbook_prompt_path.exists():
        try:
            playbook_block = playbook_prompt_path.read_text(encoding="utf-8")
        except OSError:
            playbook_block = ""
    prompt = textwrap.dedent(
        f"""\
        You are a senior test architect.
        Produce a *unified git diff* that adds deterministic pytest specs under tests/feature_specs/<feature>/.
        Only touch:
        - tests/feature_specs/<feature>/...
        - documents/feature_cards/<same-card>.md  (to update state/links once tests are created)

        Guardrails:
        - Follow AGENTS.md. Do NOT modify runtime.
        - Tests must import the intended module so first failure is ModuleNotFoundError.
        - Force offline defaults (no network/time.sleep).
        - Include happy-path, env toggle, and explicit error coverage.
        Diff contract: unified diff only (start each file with 'diff --git').
        Determinism:
        - Avoid non-determinism (seed randomness, freeze time, avoid sleeps and network).
        - Prefer explicit assertions and minimal fixtures; ensure failures point to the right module.

        Feature slug: {slug}
        All updates must remain under tests/feature_specs/{slug}/ and the card document.

        --- PASS NUMBER ---
        {generation_pass}
        """
    )
    if focus:
        prompt += "\nAdditional coverage goals from previous critic pass:\n"
        prompt += f"{focus}\n"
    prompt += "\n--- BEGIN AGENTS.md EXCERPT ---\n"
    prompt += agents_excerpt
    prompt += "\n--- END AGENTS.md EXCERPT ---\n\n"
    prompt += "--- BEGIN FEATURE CARD ---\n"
    prompt += card_text
    prompt += "\n--- END FEATURE CARD ---\n"
    if playbook_block:
        prompt += "\n--- BEGIN CANONICAL PLAYBOOK SUMMARY ---\n"
        prompt += playbook_block
        if not playbook_block.endswith("\n"):
            prompt += "\n"
        prompt += "--- END CANONICAL PLAYBOOK SUMMARY ---\n"
    prompt += existing
    return prompt


def _append_existing_tests(slug: str, context: RexContext) -> str:
    specs_dir = context.root / "tests" / "feature_specs" / slug
    if not specs_dir.exists():
        return ""
    output = ["\n--- EXISTING TEST FILES ---"]
    for path in sorted(specs_dir.glob("**/*.py")):
        try:
            snippet = path.read_text(encoding="utf-8")
        except OSError:
            continue
        output.append(f"\n### {path}")
        output.append(snippet)
    return "\n".join(output)


def _normalize_unified_diff(diff_text: str) -> str:
    """Normalize line endings and ensure git-apply-friendly trailing newline."""
    normalized = diff_text.replace("\r\n", "\n").replace("\r", "\n")
    if normalized and not normalized.endswith("\n"):
        normalized += "\n"
    return normalized


def _extract_diff(response_path: Path, slug: str) -> str:
    text = response_path.read_text(encoding="utf-8", errors="replace")
    pattern = re.compile(r"^diff --git .*$", re.MULTILINE)
    segments: List[str] = []
    allowed_doc = f"documents/feature_cards/{slug}.md"
    allowed_prefix = f"tests/feature_specs/{slug}/"

    matches = list(pattern.finditer(text))
    for idx, match in enumerate(matches):
        start = match.start()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)
        block = text[start:end]
        header = block.splitlines()[0]
        header_match = re.match(r"^diff --git a/(.*?) b/(.*?)$", header)
        if not header_match:
            continue
        a_path, b_path = header_match.groups()
        if any(
            candidate.startswith(allowed_prefix) or candidate == allowed_doc
            for candidate in (a_path, b_path)
        ):
            segments.append(block.rstrip("\n"))
    return _normalize_unified_diff("\n\n".join(segments))


def _enforce_patch_size(diff_text: str) -> bool:
    max_files = int(os.environ.get("GENERATOR_MAX_FILES", DEFAULT_GENERATOR_MAX_FILES))
    max_lines = int(os.environ.get("GENERATOR_MAX_LINES", DEFAULT_GENERATOR_MAX_LINES))
    files = 0
    lines = 0
    for line in diff_text.splitlines():
        if line.startswith("diff --git "):
            files += 1
        elif line.startswith(("+", "-")) and not line.startswith(("+++", "---")):
            lines += 1
    if files > max_files or lines > max_lines:
        print(
            f"[generator] diff touches {files} files / {lines} lines "
            f"(limits {max_files}/{max_lines})"
        )
        return False
    return True


def _validate_card_diff(diff_text: str, slug: str) -> bool:
    card_target = f"documents/feature_cards/{slug}.md"
    if card_target not in diff_text:
        return True
    card_pattern = re.compile(
        rf"^diff --git a/{re.escape(card_target)} b/{re.escape(card_target)}$",
        re.MULTILINE,
    )
    match = card_pattern.search(diff_text)
    if not match:
        return True
    section = diff_text[match.start() :]
    next_diff = section.find("\ndiff --git ")
    if next_diff != -1:
        section = section[:next_diff]
    if re.search(r"^[+-]\s*status\s*:", section, flags=re.IGNORECASE | re.MULTILINE):
        return False
    return True


def _print_diff_preview(diff_text: str) -> None:
    lines = diff_text.splitlines()
    if not lines:
        print("[generator] (no diff content to preview)")
        return
    limit_env = os.environ.get("GENERATOR_DIFF_PREVIEW_LINES")
    try:
        limit = int(limit_env) if limit_env else 200
    except ValueError:
        limit = 200
    preview = lines[:limit]
    for line in preview:
        print(line)
    remaining = len(lines) - len(preview)
    if remaining > 0:
        print(f"[generator] … (diff truncated, {remaining} more lines)")


def _apply_patch(patch_path: Path, root: Path) -> Tuple[bool, Optional[str]]:
    apply_index = run(
        ["git", "apply", "--index", str(patch_path)],
        cwd=root,
        check=False,
        capture_output=True,
    )
    if apply_index.returncode == 0:
        return True, None
    print("[generator] git apply --index failed; retrying without --index")
    apply_wc = run(
        ["git", "apply", str(patch_path)],
        cwd=root,
        check=False,
        capture_output=True,
    )
    if apply_wc.returncode == 0:
        run(["git", "add", "tests", "documents/feature_cards"], cwd=root, check=False)
        return True, None
    combined_error = (apply_wc.stderr or "") + (apply_wc.stdout or "")
    if not combined_error:
        combined_error = (apply_index.stderr or "") + (apply_index.stdout or "")
    return False, combined_error or None


def _guard_card_edits(slug: str, root: Path, baseline_text: Optional[str]) -> bool:
    card_path = root / "documents" / "feature_cards" / f"{slug}.md"
    if not card_path.exists():
        return True

    try:
        after = card_path.read_text(encoding="utf-8")
    except OSError:
        print(f"[generator] Unable to read Feature Card {card_path}")
        return False

    if baseline_text is not None:
        before_text = baseline_text
    else:
        try:
            before_text = run(
                ["git", "show", f"HEAD:{card_path.as_posix()}"],
                capture_output=True,
                check=True,
            ).stdout
        except subprocess.CalledProcessError:
            before_text = ""

    before_lines = before_text.splitlines()
    after_lines = after.splitlines()

    if before_lines == after_lines:
        return True

    allowed_headers = {"## Links", "## Spec Trace"}

    def nearest_header(lines: List[str], idx: int) -> Optional[str]:
        for pos in range(min(idx, len(lines)) - 1, -1, -1):
            stripped = lines[pos].strip()
            if stripped.startswith("## "):
                return stripped
        return None

    def header_key(header: Optional[str]) -> Optional[str]:
        if header is None:
            return None
        return next(
            (h for h in allowed_headers if header.lower().startswith(h.lower())),
            None,
        )

    sm = difflib.SequenceMatcher(a=before_lines, b=after_lines)
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == "equal":
            continue
        removed = before_lines[i1:i2]
        added = after_lines[j1:j2]
        if any(
            re.search(r"\bstatus\s*:", line, flags=re.IGNORECASE)
            for line in removed + added
        ):
            print("[generator] Card edit touches status line; abort.")
            return False
        header_before = header_key(nearest_header(before_lines, i1))
        header_after = header_key(nearest_header(after_lines, j1))
        allowed_here = header_before or header_after
        if tag == "insert":
            if not allowed_here:
                header = nearest_header(after_lines, j1)
                if header is None and added:
                    candidate = added[0].strip()
                    if candidate.startswith("## "):
                        header = candidate
                    header_after = header_key(header)
                    allowed_here = header_after
            if not allowed_here:
                header = nearest_header(after_lines, j1)
                if header is None:
                    print(
                        "[generator] Card edits must appear under an allowed section."
                    )
                else:
                    print(
                        f"[generator] Card edits under section '{header}' are not permitted."
                    )
                return False
        elif tag in {"delete", "replace"}:
            if not allowed_here:
                header = nearest_header(before_lines, i1)
                if header is None:
                    print("[generator] Card edits may only modify allowed sections.")
                else:
                    print(
                        f"[generator] Card edits under section '{header}' are not permitted."
                    )
                return False
            # Modifications within allowed sections are permitted.
    return True


def _revert_generated_files(slug: str, root: Path) -> None:
    specs_dir = root / "tests" / "feature_specs" / slug
    if specs_dir.exists():
        tracked = run(
            ["git", "ls-files", str(specs_dir)],
            cwd=root,
            capture_output=True,
            check=False,
        ).stdout.splitlines()
        for path in tracked:
            path = path.strip()
            if not path:
                continue
            run(["git", "restore", "--worktree", "--", path], cwd=root, check=False)
        run(["git", "clean", "-fd", "--", str(specs_dir)], cwd=root, check=False)
    card = root / "documents" / "feature_cards" / f"{slug}.md"
    tracked_card = run(
        ["git", "ls-files", "--error-unmatch", str(card)],
        cwd=root,
        capture_output=True,
        check=False,
    )
    if tracked_card.returncode == 0:
        run(
            ["git", "restore", "--staged", "--worktree", "--", str(card)],
            cwd=root,
            check=False,
        )
    elif card.exists():
        card.unlink()


def _enforce_hermetic_tests(slug: str, root: Path) -> bool:
    specs_dir = root / "tests" / "feature_specs" / slug
    if not specs_dir.exists():
        return True

    from .hermetic import ensure_hermetic  # Local import to avoid cycles

    return ensure_hermetic(specs_dir)


def _run_pytest_snapshot(slug: str, context: RexContext) -> None:
    specs_dir = context.root / "tests" / "feature_specs" / slug
    log = context.codex_ci_dir / "generator_tests.log"
    if not specs_dir.exists():
        log.write_text(
            f"[generator] No tests/feature_specs/{slug} directory yet; skipping pytest snapshot.\n",
            encoding="utf-8",
        )
        emit_event(
            "generator",
            "pytest_snapshot",
            slug=slug,
            status="skipped",
            reason="no_specs_dir",
        )
        return
    ensure_python(context, quiet=True)
    env = activate_venv(context)
    env["PYTHONHASHSEED"] = env.get("PYTHONHASHSEED", "0")
    timeout_sec = int(os.environ.get("GENERATOR_SNAPSHOT_TIMEOUT", "300"))
    pytest_cmd = ["pytest", str(specs_dir), "-q", "-x", "--maxfail=1"]

    def _tail(text: str, limit: int = 4000) -> str:
        if len(text) <= limit:
            return text
        return text[-limit:]

    try:
        completed = subprocess.run(
            pytest_cmd,
            cwd=context.root,
            env=env,
            text=True,
            capture_output=True,
            timeout=timeout_sec,
            check=True,
        )
        log.write_text("", encoding="utf-8")
        emit_event(
            "generator",
            "pytest_snapshot",
            slug=slug,
            status="passed",
            command=pytest_cmd,
            output=_tail((completed.stdout or "") + (completed.stderr or "")),
        )
    except subprocess.TimeoutExpired:
        log.write_text(
            f"[generator] Pytest snapshot timed out after {timeout_sec}s\n",
            encoding="utf-8",
        )
        emit_event(
            "generator",
            "pytest_snapshot",
            slug=slug,
            status="timeout",
            command=pytest_cmd,
            timeout_seconds=timeout_sec,
        )
    except subprocess.CalledProcessError as exc:
        output = (exc.stdout or "") + (exc.stderr or "")
        log.write_text(output, encoding="utf-8")
        emit_event(
            "generator",
            "pytest_snapshot",
            slug=slug,
            status="failed",
            command=pytest_cmd,
            output=_tail(output),
        )


def _run_critic(
    *,
    card: FeatureCard,
    slug: str,
    generation_pass: int,
    options: GeneratorOptions,
    context: RexContext,
) -> Tuple[bool, str]:
    root = context.root
    prompt_path = context.codex_ci_dir / "generator_critic_prompt.txt"
    response_path = context.codex_ci_dir / "generator_critic_response.log"
    tests_log = context.codex_ci_dir / "generator_tests.log"

    tests_summary = ""
    if tests_log.exists():
        tests_summary = tests_log.read_text(encoding="utf-8", errors="replace")

    card_text = card.path.read_text(encoding="utf-8")
    files_output = []
    specs_dir = root / "tests" / "feature_specs" / slug
    if specs_dir.exists():
        for path in sorted(specs_dir.glob("**/*.py")):
            files_output.append(
                f"### {path}\n{path.read_text(encoding='utf-8', errors='replace')}"
            )

    discriminator_tail = ""
    latest_log = root / ".codex_ci_latest.log"
    if latest_log.exists():
        lines = latest_log.read_text(encoding="utf-8", errors="replace").splitlines()
        discriminator_tail = "\n".join(lines[-120:])

    prompt_sections = [
        "You are reviewing pytest specs that were just generated for the following Feature Card.",
        "Decide whether the tests fully capture the acceptance criteria and obvious negative cases.",
        "Respond in ONE of two ways:",
        "1. `DONE` (exact uppercase word) if coverage is sufficient.",
        "2. `TODO:` followed by bullet items describing additional scenarios to cover.",
        "Do NOT provide code; only guidance.",
        "",
        "--- GENERATOR PASS ---",
        str(generation_pass),
        "",
        f"Feature slug: {slug}",
        "",
        "--- FEATURE CARD ---",
        card_text,
        "",
        "--- CURRENT TEST FILES ---",
        "\n\n".join(files_output),
        "--- END TEST FILES ---",
    ]
    prompt = "\n".join(prompt_sections)
    if tests_summary:
        prompt += (
            f"\n--- PYTEST OUTPUT (tests/feature_specs/{slug}) ---\n{tests_summary}\n"
        )
    if discriminator_tail:
        prompt += "\n--- MOST RECENT DISCRIMINATOR LOG (tail) ---\n"
        prompt += discriminator_tail + "\n"

    prompt_path.write_text(prompt, encoding="utf-8")

    cmd = (
        _split_command(options.codex_bin)
        + ["exec"]
        + _split_command(options.codex_flags)
    )
    if options.codex_model:
        cmd += ["--model", options.codex_model]
    cmd += ["--cd", str(root), "--", prompt]

    completed = subprocess.run(
        cmd,
        cwd=root,
        capture_output=True,
        text=True,
    )
    response_path.write_text(
        (completed.stdout or "") + ("\n" if completed.stdout else ""),
        encoding="utf-8",
    )
    if completed.returncode != 0:
        if completed.stderr:
            response_path.write_text(
                response_path.read_text(encoding="utf-8") + completed.stderr,
                encoding="utf-8",
            )
        return False, ""

    trimmed = (completed.stdout or "").strip()
    if not trimmed:
        return False, ""
    normalized = re.sub(r"\s+", " ", trimmed.replace("`", "")).strip().upper()
    if normalized == "DONE":
        return True, ""
    return False, trimmed


def _reconcile_card(card: FeatureCard, context: RexContext) -> int:
    palette = _ansi_palette()
    print(
        f"\n{palette.accent}Reconcile Feature Card{palette.reset}: "
        f"{card.slug} ({context.relative(card.path)})"
    )
    update_active_card(context, card=card)
    result = _build_spec_trace_result(card=card, slug=card.slug, context=context)
    if result is None:
        print("  No acceptance criteria or spec shard detected yet.")
        return 0
    _print_spec_trace_result(result)
    return 1 if (result.missing or result.orphans) else 0


def _update_metadata(card: FeatureCard, slug: str, context: RexContext) -> None:
    data = load_json(context.rex_agent_file)
    feature = data.setdefault("feature", {})
    feature["active_card"] = str(card.relative_path)
    feature["active_slug"] = slug
    feature["updated_at"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    dump_json(context.rex_agent_file, data)

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/generator_ui.py ===
"""Terminal HUD for generator progress."""

from __future__ import annotations

import contextlib
import io
import json
import sys
import threading
import time
from collections import deque
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Deque, Dict, Iterable, List, Optional

from .events import events_path

STATUS_ICONS = {
    "planned": "[ ]",
    "missing": "[ ]",
    "covered": "[*]",
    "verified": "[*]",
    "failing": "[x]",
}


def _shorten(text: str, width: int) -> str:
    if len(text) <= width:
        return text
    if width <= 1:
        return text[:width]
    return text[: width - 1] + "…"


def _format_duration(seconds: Optional[float]) -> str:
    if not seconds or seconds <= 0:
        return "0s"
    if seconds < 60:
        return f"{int(seconds)}s"
    minutes, rem = divmod(int(seconds), 60)
    if minutes < 60:
        return f"{minutes}m{rem:02d}s"
    hours, minutes = divmod(minutes, 60)
    return f"{hours}h{minutes:02d}m"


@dataclass
class AcceptanceItem:
    index: int
    text: str
    tests: List[str] = field(default_factory=list)
    status: str = "planned"


class GeneratorHUDModel:
    """Maintain generator progress state derived from events."""

    def __init__(self, slug: str) -> None:
        self.slug = slug
        self.feature_title = slug
        self.feature_status = ""
        self.feature_summary = ""
        self.acceptance: List[AcceptanceItem] = []
        self.iteration_current = 0
        self.iteration_total = 0
        self.iteration_status = "idle"
        self.iteration_history: List[float] = []
        self.codex_status = "idle"
        self.codex_elapsed_hint = 0.0
        self.codex_returncode: Optional[int] = None
        self.diff_files: List[Dict[str, Any]] = []
        self.diff_totals: Dict[str, int] = {}
        self.pytest_status = "pending"
        self.pytest_output = ""
        self.critic_status = "pending"
        self.critic_guidance = ""
        self.feature_outcome = "running"
        self.orphan_tests: List[str] = []
        self.messages: Deque[str] = deque(maxlen=8)
        self.coverage_percent: float = 0.0
        self.coverage_linked = 0
        self.coverage_total = 0
        self.coverage_failing = 0

    def _set_acceptance(self, items: Iterable[str]) -> None:
        self.acceptance = [
            AcceptanceItem(index=idx, text=item.strip())
            for idx, item in enumerate(items, start=1)
            if item.strip()
        ]
        self._recompute_coverage_metrics()

    def _update_acceptance_tests(self, coverage: Dict[str, Any]) -> None:
        entries = coverage.get("entries") or []
        indexed_tests = {
            entry.get("index"): entry.get("tests", []) for entry in entries
        }
        for item in self.acceptance:
            tests = indexed_tests.get(item.index, [])
            item.tests = tests
            item.status = "covered" if tests else "missing"
        missing = coverage.get("missing") or []
        for entry in missing:
            idx = entry.get("index")
            for item in self.acceptance:
                if item.index == idx:
                    item.status = "missing"
        self.orphan_tests = coverage.get("orphans") or []
        self._recompute_coverage_metrics()

    def _recompute_coverage_metrics(self) -> None:
        total = len(self.acceptance)
        if total == 0:
            self.coverage_percent = 0.0
            self.coverage_linked = 0
            self.coverage_total = 0
            self.coverage_failing = 0
            return
        contributions: List[float] = []
        linked = 0
        failing = 0
        for item in self.acceptance:
            if item.tests:
                linked += 1
                if self.pytest_status == "passed":
                    item.status = "verified"
                    contribution = 1.0
                elif self.pytest_status in {"failed", "timeout"}:
                    item.status = "failing"
                    contribution = 0.5
                    failing += 1
                else:
                    item.status = "covered"
                    contribution = 0.5
            else:
                item.status = "missing"
                contribution = 0.0
            contributions.append(contribution)
        total_score = sum(contributions)
        percent = (total_score / total) * 100 if total else 0.0
        percent = max(0.0, min(100.0, percent))
        self.coverage_percent = round(percent, 1)
        self.coverage_linked = linked
        self.coverage_total = total
        self.coverage_failing = failing

    def _add_message(self, text: str) -> None:
        text = text.strip()
        if not text:
            return
        if self.messages and self.messages[-1] == text:
            return
        self.messages.append(text)

    def apply_event(self, event: Dict[str, Any]) -> None:
        data = event.get("data", {})
        etype = event.get("type", "")
        if etype == "feature_started":
            self.feature_title = data.get("title") or self.slug
            self.feature_status = data.get("status") or ""
            self.feature_summary = data.get("summary") or ""
            self._set_acceptance(data.get("acceptance") or [])
            self.iteration_total = int(data.get("passes") or 0)
            self.feature_outcome = "running"
            focus = data.get("focus")
            if focus:
                self._add_message(f"Focus: {_shorten(str(focus), 80)}")
        elif etype == "iteration_started":
            self.iteration_current = int(data.get("iteration") or 0)
            self.iteration_total = int(data.get("total_passes") or self.iteration_total)
            self.iteration_status = "running"
            self._add_message(
                f"Iteration {self.iteration_current}/{self.iteration_total} started"
            )
        elif etype == "iteration_completed":
            self.iteration_status = "waiting"
            elapsed = data.get("elapsed_seconds")
            if isinstance(elapsed, (float, int)):
                self.iteration_history.append(float(elapsed))
            exit_code = data.get("exit_code")
            if exit_code not in (0, None):
                self._add_message(f"Iteration ended with exit code {exit_code}")
                self.feature_outcome = "failed"
            elif exit_code == 0:
                self.feature_outcome = "completed"
        elif etype == "codex_started":
            self.codex_status = "running"
            self.codex_returncode = None
        elif etype == "codex_heartbeat":
            seconds = data.get("seconds")
            if isinstance(seconds, (int, float)):
                self.codex_elapsed_hint = max(self.codex_elapsed_hint, float(seconds))
        elif etype == "codex_completed":
            self.codex_status = "completed"
            self.codex_returncode = data.get("returncode")
            elapsed = data.get("elapsed_seconds")
            if isinstance(elapsed, (int, float)):
                self.codex_elapsed_hint = float(elapsed)
            rc = self.codex_returncode
            label = "success" if rc == 0 else f"exit {rc}"
            self._add_message(f"Codex run finished ({label})")
        elif etype == "diff_summary":
            self.diff_files = data.get("files") or []
            self.diff_totals = data.get("totals") or {}
        elif etype == "pytest_snapshot":
            status = data.get("status") or "pending"
            self.pytest_status = status
            output = data.get("output")
            if isinstance(output, str):
                self.pytest_output = output.strip()
            if status == "failed":
                self._add_message("Pytest snapshot failed")
            elif status == "timeout":
                self._add_message("Pytest snapshot timed out")
            elif status == "passed":
                self._add_message("Pytest snapshot passed")
            self._recompute_coverage_metrics()
        elif etype == "critic_guidance":
            done = bool(data.get("done"))
            self.critic_status = "done" if done else "todo"
            guidance = data.get("guidance") or ""
            self.critic_guidance = guidance.strip()
            if guidance:
                label = "DONE" if done else "Critic guidance"
                self._add_message(f"{label}: {_shorten(guidance, 80)}")
        elif etype == "spec_trace_update":
            coverage = data.get("coverage") or {}
            if isinstance(coverage, dict):
                self._update_acceptance_tests(coverage)
        elif etype == "feature_completed":
            self.feature_outcome = "completed"
            self.iteration_status = "completed"
            self._add_message("Feature completed")
            self._recompute_coverage_metrics()
        elif etype == "feature_failed":
            self.feature_outcome = "failed"
            reason = data.get("reason")
            if reason:
                self._add_message(f"Feature failed: {reason}")
            else:
                self._add_message("Feature failed.")
            self._recompute_coverage_metrics()

    def _acceptance_rows(self) -> List[str]:
        if not self.acceptance:
            return ["  (no acceptance criteria listed)"]
        rows: List[str] = []
        for item in self.acceptance:
            icon = STATUS_ICONS.get(item.status, "[ ]")
            tests = ", ".join(_shorten(t, 40) for t in item.tests) or "(missing)"
            rows.append(f"  {icon} {_shorten(item.text, 40):<40} │ {tests}")
        if self.orphan_tests:
            rows.append("  --- Orphan tests ---")
            for test in self.orphan_tests[:5]:
                rows.append(f"    • {_shorten(test, 64)}")
        return rows

    def _coverage_line(self) -> str:
        if not self.acceptance:
            return "Coverage: (no acceptance criteria listed)"
        percent_display = int(round(self.coverage_percent))
        percent_display = max(0, min(100, percent_display))
        total_blocks = 10
        filled_blocks = max(0, min(total_blocks, int(round(percent_display / 10))))
        bar = "█" * filled_blocks + "░" * (total_blocks - filled_blocks)
        summary_parts: List[str] = []
        if self.coverage_total:
            summary_parts.append(
                f"{self.coverage_linked}/{self.coverage_total} bullets linked"
            )
        if self.coverage_failing:
            summary_parts.append(f"{self.coverage_failing} failing")
        missing = self.coverage_total - self.coverage_linked
        if missing and self.coverage_total:
            summary_parts.append(f"{missing} missing")
        if (
            self.coverage_total
            and not self.coverage_failing
            and missing == 0
            and self.pytest_status == "passed"
        ):
            summary_parts.append("all passing")
        if not summary_parts:
            summary_parts.append("no coverage data")
        summary = "; ".join(summary_parts)
        return f"Coverage: {bar} {percent_display}% ({summary})"

    def _diff_summary(self) -> str:
        totals = self.diff_totals or {}
        files = totals.get("files", 0)
        added = totals.get("added_lines", 0)
        removed = totals.get("removed_lines", 0)
        parts = []
        if files:
            parts.append(f"{files} file{'s' if files != 1 else ''}")
        if added or removed:
            parts.append(f"+{added}/-{removed} lines")
        return ", ".join(parts) if parts else "pending"

    def _iteration_summary(self, elapsed: Optional[float]) -> str:
        if not self.iteration_total:
            return "Idle"
        current = self.iteration_current or 1
        status = self.iteration_status
        avg = None
        if self.iteration_history:
            avg = sum(self.iteration_history) / len(self.iteration_history)
        parts = [f"{current}/{self.iteration_total} ({status})"]
        if elapsed:
            parts.append(f"elapsed {_format_duration(elapsed)}")
        if avg:
            parts.append(f"avg {_format_duration(avg)}")
        return ", ".join(parts)

    def _codex_summary(self, elapsed: Optional[float]) -> str:
        status = self.codex_status
        if status == "idle":
            return "Idle"
        parts = [status]
        if elapsed or self.codex_elapsed_hint:
            duration = elapsed if elapsed is not None else self.codex_elapsed_hint
            parts.append(_format_duration(duration))
        if status == "completed" and self.codex_returncode not in (0, None):
            parts.append(f"exit {self.codex_returncode}")
        return ", ".join(parts)

    def _pytest_summary(self) -> str:
        status = self.pytest_status
        if status == "pending":
            return "Not run"
        if status == "passed":
            return "Passed"
        if status == "failed":
            return "Failed"
        if status == "timeout":
            return "Timeout"
        if status == "skipped":
            return "Skipped"
        return status

    def _critic_summary(self) -> str:
        if self.critic_status == "done":
            return "DONE"
        if self.critic_guidance:
            return _shorten(self.critic_guidance, 80)
        return "Waiting"

    def render(
        self, iteration_elapsed: Optional[float], codex_elapsed: Optional[float]
    ) -> str:
        lines: List[str] = []
        state = self.feature_outcome.upper()
        header = f"Feature: {self.feature_title}  [status: {self.feature_status or 'unknown'}]  → {state}"
        lines.append(header)
        if self.feature_summary:
            lines.append(f"Summary: {_shorten(self.feature_summary, 100)}")
        lines.append("")
        lines.append("Acceptance → Tests")
        lines.extend(self._acceptance_rows())
        lines.append(self._coverage_line())
        lines.append("")
        lines.append("Stages")
        lines.append(f"  Iteration     : {self._iteration_summary(iteration_elapsed)}")
        lines.append(f"  Codex         : {self._codex_summary(codex_elapsed)}")
        lines.append(f"  Diff summary  : {self._diff_summary()}")
        lines.append(f"  Pytest shard  : {self._pytest_summary()}")
        lines.append(f"  Critic        : {self._critic_summary()}")
        if self.pytest_status == "failed" and self.pytest_output:
            lines.append("")
            lines.append("Pytest output (tail)")
            tail = self.pytest_output.splitlines()[-6:]
            lines.extend(f"  {line}" for line in tail)
        if self.messages:
            lines.append("")
            lines.append("Recent notes")
            for message in list(self.messages)[-6:]:
                lines.append(f"  - {_shorten(message, 100)}")
        return "\n".join(lines)


class _HUDCapture(io.TextIOBase):
    """Redirect stdout/stderr into a log file to avoid scrolling output."""

    def __init__(self, handle: io.TextIOBase) -> None:
        self._handle = handle

    def write(self, s: str) -> int:  # type: ignore[override]
        try:
            self._handle.write(s)
        except ValueError:
            return 0
        return len(s)

    def flush(self) -> None:  # type: ignore[override]
        if getattr(self._handle, "closed", False):
            return
        try:
            self._handle.flush()
        except ValueError:
            return

    def isatty(self) -> bool:  # type: ignore[override]
        return False


class GeneratorHUD(contextlib.AbstractContextManager["GeneratorHUD"]):
    """Manage the generator HUD lifecycle and stdout redirection."""

    def __init__(
        self,
        *,
        slug: str,
        codex_ci_dir: Path,
        ui_mode: str = "auto",
        refresh_hz: float = 1.0,
        terminal: io.TextIOBase | None = None,
    ) -> None:
        self.slug = slug
        self.codex_ci_dir = codex_ci_dir
        self.ui_mode = (ui_mode or "monitor").lower()
        self.refresh_interval = max(0.2, 1.0 / max(refresh_hz, 0.1))
        self.terminal = terminal or getattr(sys, "__stdout__", None)  # type: ignore[name-defined]
        if self.terminal is None:
            import sys as _sys

            self.terminal = _sys.__stdout__
        self.enabled = self._should_enable()
        self.log_path = codex_ci_dir / f"generator_console_{slug}.log"
        self._stack: Optional[contextlib.ExitStack] = None
        self._capture: Optional[_HUDCapture] = None
        self._log_handle: Optional[io.TextIOBase] = None
        self._thread: Optional[threading.Thread] = None
        self._stop_event = threading.Event()
        self._events_path = events_path()
        self._offset = 0
        self._model = GeneratorHUDModel(slug)
        self._last_render = ""
        self._cursor_hidden = False
        self._alt_screen = False
        self._use_alternate = self.ui_mode == "monitor"
        self._iteration_start: Optional[float] = None
        self._codex_start: Optional[float] = None

    def _should_enable(self) -> bool:
        if self.ui_mode == "off":
            return False
        is_tty = getattr(self.terminal, "isatty", lambda: False)()
        if self.ui_mode in {"monitor", "auto"}:
            return bool(is_tty)
        return False

    # Terminal helpers -------------------------------------------------

    def _term_write(self, text: str) -> None:
        try:
            self.terminal.write(text)
            self.terminal.flush()
        except Exception:
            return

    def _activate_alternate(self) -> None:
        if not self.enabled or self._alt_screen or not self._use_alternate:
            return
        self._term_write("\033[?1049h\033[H")
        self._alt_screen = True

    def _release_alternate(self) -> None:
        if not self.enabled or not self._alt_screen or not self._use_alternate:
            return
        self._term_write("\033[?1049l")
        self._alt_screen = False

    def _hide_cursor(self) -> None:
        if not self.enabled or self._cursor_hidden:
            return
        self._term_write("\033[?25l")
        self._cursor_hidden = True

    def _show_cursor(self) -> None:
        if not self.enabled or not self._cursor_hidden:
            return
        self._term_write("\033[?25h")
        self._cursor_hidden = False

    def _clear_screen(self) -> None:
        if not self.enabled:
            return
        self._term_write("\033[2J\033[H")

    # Context manager --------------------------------------------------

    def __enter__(self) -> "GeneratorHUD":
        if not self.enabled:
            return self
        self._offset = 0
        if self._events_path.exists():
            self._offset = self._events_path.stat().st_size
        self.codex_ci_dir.mkdir(parents=True, exist_ok=True)
        self._log_handle = self.log_path.open("w", encoding="utf-8")
        self._capture = _HUDCapture(self._log_handle)
        self._stack = contextlib.ExitStack()
        self._stack.enter_context(contextlib.redirect_stdout(self._capture))
        self._stack.enter_context(contextlib.redirect_stderr(self._capture))
        self._activate_alternate()
        self._hide_cursor()
        self._thread = threading.Thread(target=self._loop, daemon=True)
        self._thread.start()
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        if self.enabled:
            self._stop()
            if self._stack:
                self._stack.close()
            if self._log_handle:
                self._log_handle.flush()
                self._log_handle.close()
            self._release_alternate()
            self._show_cursor()
        return None

    # Public helpers ---------------------------------------------------

    def print_footer(self, exit_code: int) -> None:
        if not self.enabled:
            return
        status = "PASS" if exit_code == 0 else f"EXIT {exit_code}"
        self._term_write(
            f"\n[generator] Finished {self.slug} ({status}). Console log: {self.log_path}\n"
        )

    # Internal event loop ----------------------------------------------

    def _loop(self) -> None:
        try:
            while not self._stop_event.is_set():
                self._poll_events()
                self._render()
                self._stop_event.wait(self.refresh_interval)
            self._poll_events()
            self._render(final=True)
        except Exception:
            # Fallback: ensure cursor is visible even if rendering fails
            self._show_cursor()

    def _stop(self) -> None:
        self._stop_event.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _poll_events(self) -> None:
        if not self._events_path.exists():
            return
        try:
            with self._events_path.open("r", encoding="utf-8") as fh:
                fh.seek(self._offset)
                for line in fh:
                    self._handle_line(line)
                self._offset = fh.tell()
        except OSError:
            return

    def _handle_line(self, line: str) -> None:
        line = line.strip()
        if not line:
            return
        try:
            event = json.loads(line)
        except json.JSONDecodeError:
            return
        slug = event.get("slug")
        if slug not in (self.slug, None):
            return
        self._model.apply_event(event)
        etype = event.get("type")
        now = time.monotonic()
        if etype == "iteration_started":
            self._iteration_start = now
        elif etype == "iteration_completed":
            self._iteration_start = None
        elif etype == "feature_failed":
            self._iteration_start = None
        elif etype == "feature_completed":
            self._iteration_start = None
        elif etype == "codex_started":
            self._codex_start = now
        elif etype == "codex_completed":
            self._codex_start = None
        elif etype == "codex_heartbeat":
            seconds = event.get("data", {}).get("seconds")
            if isinstance(seconds, (int, float)):
                self._model.codex_elapsed_hint = float(seconds)

    def _render(self, *, final: bool = False) -> None:
        if not self.enabled:
            return
        now = time.monotonic()
        iteration_elapsed = (
            now - self._iteration_start if self._iteration_start else None
        )
        codex_elapsed = now - self._codex_start if self._codex_start else None
        snapshot = self._model.render(iteration_elapsed, codex_elapsed)
        if snapshot == self._last_render and not final:
            return
        self._last_render = snapshot
        self._clear_screen()
        self._term_write(snapshot + "\n")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/hermetic.py ===
"""Hermeticity enforcement for generated specs."""

from __future__ import annotations

import ast
from pathlib import Path
from typing import List, Tuple

BANNED_IMPORT_MODULES = {
    "requests": "network access via requests",
    "httpx": "network access via httpx",
    "urllib": "network access via urllib",
    "urllib3": "network access via urllib3",
    "aiohttp": "network access via aiohttp",
    "socket": "network access via socket",
    "secrets": "secrets module is non-deterministic; inject fixed values instead",
}

BANNED_CALL_PREFIXES = {
    "requests.": "network access via requests",
    "httpx.": "network access via httpx",
    "urllib.": "network access via urllib",
    "urllib3.": "network access via urllib3",
    "aiohttp.": "network access via aiohttp",
    "socket.": "network access via socket",
    "secrets.": "secrets module is non-deterministic; inject fixed values instead",
    "numpy.random.": "numpy.random must be seeded deterministically; avoid direct usage",
    "random.SystemRandom.": "SystemRandom uses system entropy; avoid in specs",
}

BANNED_CALL_EXACT = {
    "time.sleep": "time.sleep introduces nondeterministic delays",
    "asyncio.sleep": "asyncio.sleep introduces nondeterministic delays",
    "subprocess.run": "subprocess usage requires explicit stubbing",
    "subprocess.Popen": "subprocess usage requires explicit stubbing",
    "subprocess.call": "subprocess usage requires explicit stubbing",
    "os.system": "os.system usage should be avoided in specs",
    "time.time": "use a deterministic clock stub instead of time.time",
    "time.perf_counter": "use a deterministic clock stub instead of time.perf_counter",
    "time.monotonic": "use a deterministic clock stub instead of time.monotonic",
    "datetime.datetime.now": "use a frozen datetime or dependency injection",
    "datetime.datetime.utcnow": "use a frozen datetime or dependency injection",
    "datetime.datetime.today": "use a frozen datetime or dependency injection",
    "datetime.date.today": "use a frozen date or dependency injection",
    "pytest.skip": "skipping generated specs is not allowed",
    "pytest.xfail": "xfailing generated specs is not allowed",
    "os.urandom": "use deterministic stubs instead of os.urandom",
    "uuid.uuid4": "use a fixed UUID in specs instead of uuid.uuid4",
    "uuid.uuid1": "use a fixed UUID in specs instead of uuid.uuid1",
}

RANDOM_PREFIXES = ("random.", "numpy.random.")
RANDOM_ALLOWED = {"random.seed", "numpy.random.seed"}


class HermeticVisitor(ast.NodeVisitor):
    def __init__(self, path: Path) -> None:
        self.path = path
        self.aliases: dict[str, str] = {}
        self.violations: List[Tuple[Path, int, str]] = []

    def add_violation(self, lineno: int, detail: str) -> None:
        self.violations.append((self.path, lineno, detail))

    def visit_Import(self, node: ast.Import) -> None:
        for alias in node.names:
            name = alias.asname or alias.name.split(".")[-1]
            self.aliases[name] = alias.name
            root = alias.name.split(".")[0]
            if root in BANNED_IMPORT_MODULES:
                self.add_violation(
                    node.lineno, f"import {alias.name} ({BANNED_IMPORT_MODULES[root]})"
                )
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        module = node.module or ""
        root = module.split(".")[0] if module else ""
        if root in BANNED_IMPORT_MODULES:
            self.add_violation(
                node.lineno, f"from {module} import ... ({BANNED_IMPORT_MODULES[root]})"
            )
        for alias in node.names:
            target = f"{module}.{alias.name}" if module else alias.name
            name = alias.asname or alias.name
            self.aliases[name] = target
        self.generic_visit(node)

    def resolve(self, node: ast.AST) -> str | None:
        if isinstance(node, ast.Name):
            return self.aliases.get(node.id, node.id)
        if isinstance(node, ast.Attribute):
            base = self.resolve(node.value)
            if base:
                return f"{base}.{node.attr}"
            return node.attr
        return None

    def visit_Call(self, node: ast.Call) -> None:
        call_name = self.resolve(node.func)
        if call_name:
            if call_name in BANNED_CALL_EXACT:
                self.add_violation(
                    node.lineno, f"{call_name} ({BANNED_CALL_EXACT[call_name]})"
                )
            elif (
                any(call_name.startswith(prefix) for prefix in RANDOM_PREFIXES)
                and call_name not in RANDOM_ALLOWED
            ):
                self.add_violation(
                    node.lineno,
                    f"{call_name} (set a deterministic seed or avoid randomness)",
                )
            else:
                for prefix, reason in BANNED_CALL_PREFIXES.items():
                    if call_name.startswith(prefix):
                        self.add_violation(node.lineno, f"{call_name} ({reason})")
                        break
        self.generic_visit(node)

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        for dec in node.decorator_list:
            name = self.resolve(dec)
            if not name:
                continue
            lname = name.lower()
            if lname.startswith("pytest.mark.skip") or lname.startswith(
                "pytest.mark.xfail"
            ):
                self.add_violation(
                    getattr(dec, "lineno", node.lineno),
                    f"{name} (skipping/xfailing specs is forbidden)",
                )
            elif lname.startswith("pytest.mark.skipif"):
                args = getattr(dec, "args", [])
                if args:
                    first = args[0]
                    value = getattr(first, "value", None)
                    if value is True:
                        self.add_violation(
                            getattr(dec, "lineno", node.lineno),
                            "pytest.mark.skipif(True, ...) is forbidden",
                        )
        self.generic_visit(node)


def ensure_hermetic(specs_dir: Path) -> bool:
    violations: List[Tuple[Path, int, str]] = []
    for path in specs_dir.rglob("*.py"):
        try:
            tree = ast.parse(path.read_text(encoding="utf-8"))
        except SyntaxError as err:
            violations.append((path, err.lineno or 0, f"SyntaxError: {err}"))
            continue
        visitor = HermeticVisitor(path)
        visitor.visit(tree)
        violations.extend(visitor.violations)

    if violations:
        for path, lineno, detail in violations:
            location = f"{path}:{lineno}" if lineno else str(path)
            print(f"{location}: {detail}")
        return False
    return True

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/hud.py ===
"""HUD utilities for snapshot and live monitoring modes."""

from __future__ import annotations

import json
import sys
import time
from collections import OrderedDict
from pathlib import Path
from typing import Any, Dict, Iterable, Optional

from .cards import latest_card
from .events import events_path as default_events_path
from .generator_ui import GeneratorHUD, GeneratorHUDModel
from .utils import RexContext


class _HUDPrinter:
    def __init__(self, *, width: int = 100) -> None:
        self.width = width

    def divider(self, title: str) -> str:
        title = title.strip()
        pad = max(0, self.width - len(title) - 4)
        return f"{title} {'-' * pad}"


def _load_events(path: Path) -> Iterable[dict[str, Any]]:
    if not path.exists():
        return []
    results = []
    for line in path.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            results.append(json.loads(line))
        except json.JSONDecodeError:
            continue
    return results


def _resolve_generator_slug(
    slug: Optional[str], *, context: RexContext
) -> Optional[str]:
    if slug:
        return slug
    card = latest_card()
    return card.slug if card else None


def render_generator_snapshot(
    *,
    slug: str,
    events: Iterable[dict[str, Any]],
    printer: _HUDPrinter,
) -> str:
    model = GeneratorHUDModel(slug)
    relevant = [event for event in events if event.get("slug") in (slug, None)]
    start_index = 0
    for idx, event in enumerate(reversed(relevant)):
        if event.get("type") == "feature_started" and event.get("slug") == slug:
            start_index = len(relevant) - idx - 1
            break
    for event in relevant[start_index:]:
        model.apply_event(event)
    snapshot = model.render(iteration_elapsed=None, codex_elapsed=None)
    header = printer.divider(f"Generator HUD :: {slug}")
    return f"{header}\n{snapshot}\n"


def generator_snapshot_text(slug: str, path: Path) -> str:
    events = _load_events(path)
    if not events:
        return ""
    printer = _HUDPrinter()
    return render_generator_snapshot(slug=slug, events=events, printer=printer)


def _follow_generator_hud(
    *,
    slug: str,
    events_path: Path,
    refresh: float,
    linger: float,
) -> None:
    refresh = max(0.2, refresh)
    linger = max(0.0, linger)
    printer = _HUDPrinter()
    if not sys.stdout.isatty():
        # Fallback: poll snapshots without terminal control.
        last_render = ""
        done_since: Optional[float] = None
        while True:
            events = list(_load_events(events_path))
            if events:
                snapshot = render_generator_snapshot(
                    slug=slug, events=events, printer=printer
                )
                if snapshot != last_render:
                    print("\033[2J\033[H", end="", flush=True)
                    print(snapshot, end="", flush=True)
                    last_render = snapshot
                for event in reversed(events):
                    if event.get("slug") not in (slug, None):
                        continue
                    etype = event.get("type")
                    if etype in {"feature_completed", "feature_failed"}:
                        if done_since is None:
                            done_since = time.monotonic()
                        break
                    if etype == "iteration_completed":
                        exit_code = event.get("data", {}).get("exit_code")
                        if exit_code is not None:
                            if done_since is None:
                                done_since = time.monotonic()
                            break
                else:
                    done_since = None
            else:
                print(
                    "\033[2J\033[H[hud] Waiting for generator events…",
                    end="",
                    flush=True,
                )
            if done_since is not None and time.monotonic() - done_since >= linger:
                break
            time.sleep(refresh)
        return

    hud = GeneratorHUD(
        slug=slug,
        codex_ci_dir=events_path.parent,
        ui_mode="monitor",
        refresh_hz=max(1.0, 1.0 / refresh),
        terminal=sys.stdout,
    )
    hud._events_path = events_path
    done_since: Optional[float] = None
    try:
        hud._activate_alternate()
        hud._hide_cursor()
        waiting_message = False
        while True:
            if events_path.exists():
                waiting_message = False
                hud._poll_events()
                hud._render()
                for event in reversed(list(_load_events(events_path))):
                    if event.get("slug") not in (slug, None):
                        continue
                    if event.get("type") in {"feature_completed", "feature_failed"}:
                        if done_since is None:
                            done_since = time.monotonic()
                        break
                else:
                    done_since = None
            else:
                if not waiting_message:
                    hud._clear_screen()
                    hud._term_write("[hud] Waiting for generator events…\n")
                    waiting_message = True
            if done_since is not None and time.monotonic() - done_since >= linger:
                break
            time.sleep(refresh)
        hud._render(final=True)
    except KeyboardInterrupt:  # pragma: no cover - user interruption
        hud._term_write("\n[hud] Interrupted by user.\n")
    finally:
        hud._release_alternate()
        hud._show_cursor()


def _format_elapsed(value: Any) -> Optional[str]:
    if isinstance(value, (int, float)):
        return f"{float(value):.2f}s"
    return None


class DiscriminatorHUDModel:
    def __init__(self) -> None:
        self.mode = "global"
        self.slug: Optional[str] = None
        self.pass_number: Optional[int] = None
        self.run_id: Optional[int] = None
        self.stage_groups: list[str] = []
        self.stages: "OrderedDict[str, Dict[str, Any]]" = OrderedDict()
        self.coverage_percent: Optional[float] = None
        self.coverage_threshold: Optional[str] = None
        self.coverage_targets: list[str] = []
        self.mechanical: Optional[dict[str, Any]] = None
        self.llm_decision: Optional[dict[str, Any]] = None
        self.result: Optional[bool] = None

    def _reset_for_run(self, pass_number: Optional[int], run_id: int) -> None:
        self.pass_number = pass_number
        self.run_id = run_id
        self.stage_groups = []
        self.stages = OrderedDict()
        self.coverage_percent = None
        self.coverage_threshold = None
        self.coverage_targets = []
        self.mechanical = None
        self.llm_decision = None
        self.result = None

    def apply_event(self, event: Dict[str, Any]) -> None:
        if event.get("phase") != "discriminator":
            return
        data: Dict[str, Any] = event.get("data", {}) or {}
        run_id = data.get("run_id")
        pass_number = data.get("pass_number")

        if run_id is not None:
            if self.run_id is None or run_id > self.run_id:
                self._reset_for_run(pass_number, run_id)
            elif run_id < self.run_id:
                return
        if pass_number is not None and self.pass_number is None:
            self.pass_number = pass_number

        etype = event.get("type")
        if etype == "run_started":
            self.mode = data.get("mode") or self.mode
            slug = event.get("slug")
            if slug is not None:
                self.slug = slug
            self.stage_groups = list(data.get("stage_groups") or [])
            return
        if etype == "stage_start":
            identifier = data.get("identifier")
            if not identifier:
                return
            stage = self.stages.setdefault(
                identifier,
                {
                    "description": data.get("description") or "",
                    "group": data.get("group") or "",
                    "status": "RUN",
                    "elapsed": None,
                    "failure_reason": "",
                },
            )
            stage["description"] = data.get("description") or stage["description"]
            stage["group"] = data.get("group") or stage["group"]
            stage["status"] = "RUN"
            return
        if etype == "stage_end":
            identifier = data.get("identifier")
            if not identifier:
                return
            stage = self.stages.setdefault(
                identifier,
                {
                    "description": data.get("description") or "",
                    "group": data.get("group") or "",
                    "status": "",
                    "elapsed": None,
                    "failure_reason": "",
                },
            )
            stage["description"] = data.get("description") or stage["description"]
            stage["group"] = data.get("group") or stage["group"]
            stage["status"] = "PASS" if data.get("ok") else "FAIL"
            elapsed = data.get("elapsed")
            stage["elapsed"] = (
                float(elapsed) if isinstance(elapsed, (int, float)) else None
            )
            stage["failure_reason"] = data.get("failure_reason") or ""
            return
        if etype == "coverage_update":
            percent = data.get("percent")
            if isinstance(percent, (int, float)):
                self.coverage_percent = float(percent)
            threshold = data.get("threshold")
            if threshold is not None:
                self.coverage_threshold = str(threshold)
            targets = data.get("targets")
            if isinstance(targets, list):
                self.coverage_targets = [str(item) for item in targets if str(item)]
            return
        if etype == "mechanical_fixes":
            self.mechanical = data
            return
        if etype == "llm_patch_decision":
            self.llm_decision = data
            return
        if etype == "run_completed":
            self.result = bool(data.get("ok"))
            self.mode = data.get("mode") or self.mode
            slug = event.get("slug")
            if slug is not None:
                self.slug = slug

    def render(self) -> str:
        slug_display = self.slug or "global"
        pass_label = (
            f"pass {self.pass_number}" if self.pass_number is not None else "pass ?"
        )
        run_label = f"run {self.run_id}" if self.run_id is not None else "run ?"
        lines = [
            f"Mode: {self.mode} | Slug: {slug_display} | {pass_label}, {run_label}",
            "",
        ]
        lines.append("Stage Results")
        if not self.stages:
            lines.append("  (no stages recorded)")
        else:
            for identifier, info in self.stages.items():
                description = info.get("description") or ""
                status = info.get("status") or "pending"
                elapsed_text = ""
                formatted = _format_elapsed(info.get("elapsed"))
                if formatted:
                    elapsed_text = f" ({formatted})"
                lines.append(
                    f"  [{identifier}] {description} :: {status}{elapsed_text}"
                )
                failure_reason = info.get("failure_reason")
                if failure_reason:
                    lines.append(f"      ↳ {failure_reason}")
        if self.coverage_percent is not None:
            percent_display = int(round(self.coverage_percent))
            parts = [f"Coverage: {percent_display}%"]
            if self.coverage_threshold:
                parts.append(f"threshold {self.coverage_threshold}")
            if self.coverage_targets:
                parts.append(f"targets: {', '.join(self.coverage_targets)}")
            lines.append("")
            lines.append(" ".join(parts))
        if self.mechanical is not None:
            changed = self.mechanical.get("changed")
            tools = ", ".join(self.mechanical.get("tools") or [])
            reason = self.mechanical.get("reason")
            status = "applied" if changed else "skipped"
            entry = f"Mechanical fixes: {status}"
            if tools:
                entry += f" [{tools}]"
            lines.append(entry)
            if reason and not changed:
                lines.append(f"  ↳ {reason}")
        if self.llm_decision is not None:
            accepted = bool(self.llm_decision.get("accepted"))
            reason = self.llm_decision.get("reason") or ""
            lines.append(
                f"LLM patch: {'accepted' if accepted else 'rejected'} ({reason})"
            )
        if self.result is not None:
            lines.append("")
            lines.append(f"Result: {'PASS' if self.result else 'FAIL'}")
        return "\n".join(lines)


def render_discriminator_snapshot(
    *,
    slug: Optional[str],
    events: Iterable[dict[str, Any]],
    printer: _HUDPrinter,
) -> str:
    model = DiscriminatorHUDModel()
    relevant: list[dict[str, Any]] = []
    for event in events:
        if event.get("phase") != "discriminator":
            continue
        event_slug = event.get("slug")
        if slug is not None and event_slug not in (slug, None):
            continue
        relevant.append(event)
    if not relevant:
        return ""
    for event in relevant:
        model.apply_event(event)
    header_slug = slug or model.slug or "global"
    snapshot = model.render()
    header = printer.divider(f"Discriminator HUD :: {header_slug}")
    return f"{header}\n{snapshot}\n"


def discriminator_snapshot_text(slug: Optional[str], path: Path) -> str:
    events = _load_events(path)
    if not events:
        return ""
    printer = _HUDPrinter()
    return render_discriminator_snapshot(slug=slug, events=events, printer=printer)


def render_hud(
    *,
    phase: str,
    slug: Optional[str],
    events_file: Optional[str],
    context: RexContext,
    follow: bool = False,
    refresh: float = 1.0,
    linger: float = 5.0,
) -> None:
    path = Path(events_file).expanduser() if events_file else default_events_path()
    if phase == "generator":
        resolved_slug = _resolve_generator_slug(slug, context=context)
        if not resolved_slug:
            print("[hud] No feature slug provided and no active card detected.")
            raise SystemExit(1)
        if follow:
            _follow_generator_hud(
                slug=resolved_slug,
                events_path=path,
                refresh=max(0.2, refresh),
                linger=max(0.0, linger),
            )
            return
        snapshot = generator_snapshot_text(resolved_slug, path)
        if not snapshot:
            print(f"[hud] No events recorded yet at {path}.")
            raise SystemExit(1)
        print(snapshot, end="")
        return
    if phase == "discriminator":
        if follow:
            raise SystemExit(
                "[hud] --follow is currently supported for generator only."
            )
        snapshot = discriminator_snapshot_text(slug, path)
        if not snapshot:
            print(f"[hud] No discriminator events recorded yet at {path}.")
            raise SystemExit(1)
        print(snapshot, end="")
        return
    raise SystemExit(f"[hud] Unsupported phase: {phase}")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/init.py ===
"""Project initialisation for rex-codex."""

from __future__ import annotations

import shutil
from pathlib import Path

from .. import __version__
from .config import AGENT_SRC
from .self_update import self_update
from .utils import (
    RexContext,
    dump_json,
    ensure_dir,
    ensure_python,
    ensure_requirements_installed,
    run,
    which,
)


def _copy_if_missing(src: Path, dest: Path) -> None:
    if dest.exists():
        return
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)


def _copy_with_overwrite(src: Path, dest: Path) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)


def run_init(
    *, context: RexContext | None = None, perform_self_update: bool = True
) -> None:
    context = context or RexContext.discover()
    if perform_self_update:
        self_update()

    print("[*] Bootstrapping Python environment…")
    ensure_python(context)

    requirements_template = AGENT_SRC / "requirements.txt"
    ensure_requirements_installed(context, requirements_template, quiet=False)
    _copy_with_overwrite(requirements_template, context.root / "requirements.txt")

    root = context.root
    ensure_dir(root / "tests" / "enforcement")
    ensure_dir(root / "documents" / "feature_cards")
    ensure_dir(root / "documents" / "assumption_ledgers")

    template_root = AGENT_SRC / "templates"
    copies = {
        "AGENTS.md": root / "AGENTS.md",
        "AGENTS.local.md": root / "AGENTS.local.md",
        "pytest.ini": root / "pytest.ini",
        "pyproject.toml": root / "pyproject.toml",
        "mypy.ini": root / "mypy.ini",
        "conftest.py": root / "conftest.py",
        ".flake8": root / ".flake8",
    }
    for rel, dest in copies.items():
        src = template_root / rel
        if src.exists():
            _copy_if_missing(src, dest)

    card_readme = template_root / "documents" / "feature_cards" / "README.md"
    if card_readme.exists():
        _copy_if_missing(
            card_readme, root / "documents" / "feature_cards" / "README.md"
        )

    ledger_readme = template_root / "documents" / "assumption_ledgers" / "README.md"
    if ledger_readme.exists():
        _copy_if_missing(
            ledger_readme, root / "documents" / "assumption_ledgers" / "README.md"
        )

    enforcement_dir = template_root / "tests" / "enforcement"
    if enforcement_dir.exists():
        for item in enforcement_dir.glob("**/*"):
            if item.is_file():
                rel = item.relative_to(enforcement_dir)
                dest = root / "tests" / "enforcement" / rel
                _copy_if_missing(item, dest)

    monitor_dir = root / "monitor"
    package_json = monitor_dir / "package.json"
    if package_json.exists():
        node_modules = monitor_dir / "node_modules"
        if node_modules.exists():
            print(
                "[*] Monitor dependencies already installed (monitor/node_modules present)."
            )
        else:
            npm = which("npm")
            if npm is None:
                print("[!] Skipping monitor npm install: npm not found on PATH.")
            else:
                print("[*] Installing monitor dependencies (npm install)…")
                run(
                    [npm, "install", "--no-fund", "--no-audit"],
                    cwd=monitor_dir,
                    check=True,
                )
                print("[✓] Monitor dependencies installed.")

    agent_state = {
        "stages": [
            "sanity",
            "deps",
            "specs",
            "unit",
            "style",
        ],
        "llm": {"bin": "npx --yes @openai/codex", "flags": "--yolo", "model": ""},
        "feature": {
            "active_card": None,
            "active_slug": None,
            "updated_at": None,
        },
        "version": __version__,
    }
    dump_json(context.rex_agent_file, agent_state)
    print("[✓] Project initialized. Try: ./rex-codex loop")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/logs.py ===
"""Log helpers for rex-codex."""

from __future__ import annotations

import time
from pathlib import Path

from .utils import RexContext


def tail_log(path: Path, *, lines: int = 120) -> None:
    if not path.exists():
        print(f"[logs] {path} not found.")
        return
    content = path.read_text(encoding="utf-8", errors="replace").splitlines()
    start = max(0, len(content) - lines)
    for line in content[start:]:
        print(line)


def follow_log(path: Path) -> None:
    if not path.exists():
        print(f"[logs] {path} not found.")
        return
    print(f"[logs] Following {path} (press Ctrl-C to stop)")
    try:
        with path.open("r", encoding="utf-8", errors="replace") as handle:
            handle.seek(0, 2)
            while True:
                line = handle.readline()
                if not line:
                    time.sleep(0.5)
                    continue
                print(line, end="")
    except KeyboardInterrupt:  # pragma: no cover - user interaction
        print("\n[logs] Follow stopped.")


def show_latest_logs(
    context: RexContext,
    *,
    lines: int = 120,
    generator: bool = False,
    discriminator: bool = False,
    follow: bool = False,
) -> None:
    sections: list[tuple[str, Path]] = []

    include_generator = generator or not (generator or discriminator)
    include_discriminator = discriminator or not (generator or discriminator)

    if follow:
        if include_discriminator:
            target = context.root / ".codex_ci_latest.log"
        else:
            target = context.codex_ci_dir / "generator_response.log"
        follow_log(target)
        return

    if include_generator:
        sections.extend(
            [
                ("Generator response", context.codex_ci_dir / "generator_response.log"),
                ("Generator patch", context.codex_ci_dir / "generator_patch.diff"),
                ("Generator tests", context.codex_ci_dir / "generator_tests.log"),
            ]
        )
    if include_discriminator:
        sections.extend(
            [
                (
                    "Discriminator log",
                    context.codex_ci_dir / "latest_discriminator.log",
                ),
                ("Discriminator latest", context.root / ".codex_ci_latest.log"),
            ]
        )

    seen: set[Path] = set()
    for label, path in sections:
        if path in seen:
            continue
        seen.add(path)
        if path.exists():
            print(f"--- {label}: {context.relative(path)} (last {lines} lines) ---")
            tail_log(path, lines=lines)
        else:
            print(f"[logs] Missing {label.lower()} at {context.relative(path)}")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/loop.py ===
"""Generator → discriminator orchestration."""

from __future__ import annotations

import json
import os
import sys
from dataclasses import dataclass, field, replace
from types import SimpleNamespace
from typing import List, Optional

from .cards import card_content_hash, card_path_for, discover_cards, load_rex_agent
from .discriminator import DiscriminatorOptions, run_discriminator
from .doctor import run_doctor
from .generator import GeneratorOptions, run_generator
from .logs import show_latest_logs
from .monitoring import ensure_monitor_server
from .self_update import self_update
from .utils import (
    RexContext,
    activate_venv,
    create_audit_snapshot,
    dump_json,
    lock_file,
    run,
)

GENERATOR_EXIT_MESSAGES = {
    0: "Specs updated",
    1: "No matching Feature Cards",
    2: "Codex CLI error (see generator logs)",
    3: "Diff rejected by guardrail",
    4: "Diff failed to apply cleanly",
    5: "Critic returned empty guidance",
    6: "Max passes reached without DONE",
    7: "Guardrail rejection (card edit or hermetic failure)",
}

DISCRIMINATOR_EXIT_MESSAGES = {
    0: "Ladder passed",
    1: "Stage failure (see summary above)",
    2: "LLM disabled or patch rejected",
}


def _current_card_hash(context: RexContext, slug: str | None) -> Optional[str]:
    if not slug:
        return None
    path = card_path_for(context, slug)
    return card_content_hash(path)


def _stored_card_hash(context: RexContext, slug: str | None) -> Optional[str]:
    if not slug:
        return None
    data = load_rex_agent(context)
    feature = data.get("feature", {})
    hashes = feature.get("card_hashes", {})
    return hashes.get(slug)


def _record_card_hash(context: RexContext, slug: str | None) -> None:
    if not slug:
        return
    digest = _current_card_hash(context, slug)
    if digest is None:
        return
    data = load_rex_agent(context)
    feature = data.setdefault("feature", {})
    hashes = feature.setdefault("card_hashes", {})
    hashes[slug] = digest
    dump_json(context.rex_agent_file, data)


def _card_drift_message(context: RexContext, slug: str | None) -> Optional[str]:
    if not slug:
        return None
    stored = _stored_card_hash(context, slug)
    current = _current_card_hash(context, slug)
    if stored and current and stored != current:
        return f"Feature Card '{slug}' changed since last green; regenerate specs before proceeding."
    return None


def _load_discriminator_metadata(context: RexContext) -> dict[str, object]:
    path = context.codex_ci_dir / "discriminator_result.json"
    if not path.exists():
        return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except (json.JSONDecodeError, OSError):  # pragma: no cover - corruption
        return {}


def _missing_tooling(context: RexContext) -> List[str]:
    env = activate_venv(context)
    modules = ["pytest", "pytest_cov", "black", "isort", "ruff", "flake8", "mypy"]
    missing: List[str] = []
    for module in modules:
        result = run(
            ["python", "-c", f"import {module}"],
            cwd=context.root,
            env=env,
            capture_output=True,
            check=False,
        )
        if result.returncode != 0:
            missing.append(module)
    return missing


def _ansi_palette() -> SimpleNamespace:
    disable = bool(os.environ.get("NO_COLOR")) or not sys.stdout.isatty()
    if disable:
        return SimpleNamespace(
            success="",
            warning="",
            error="",
            label="",
            dim="",
            reset="",
        )
    return SimpleNamespace(
        success="\x1b[32m",
        warning="\x1b[33m",
        error="\x1b[31m",
        label="\x1b[36m",
        dim="\x1b[2m",
        reset="\x1b[0m",
    )


def _describe_generator_exit(code: int | None) -> tuple[str, str]:
    if code is None:
        return "skipped", "Skipped (flagged off)"
    message = GENERATOR_EXIT_MESSAGES.get(code, "Unknown generator exit")
    if code == 0:
        return "pass", message
    if code in (1, 2):
        return "warn", message
    return "fail", message


def _describe_discriminator_exit(code: int | None) -> tuple[str, str]:
    if code is None:
        return "skipped", "Skipped (flagged off)"
    message = DISCRIMINATOR_EXIT_MESSAGES.get(code, "Unknown discriminator exit")
    if code == 0:
        return "pass", message
    return "fail", message


def _render_loop_summary(
    *,
    generator_code: int | None,
    discriminator_code: int | None,
    notes: Optional[List[str]] = None,
) -> None:
    palette = _ansi_palette()
    gen_state, gen_message = _describe_generator_exit(generator_code)
    disc_state, disc_message = _describe_discriminator_exit(discriminator_code)

    def _format(state: str, label: str) -> str:
        if state == "pass":
            color = palette.success
        elif state == "warn":
            color = palette.warning
        elif state == "fail":
            color = palette.error
        else:
            color = palette.dim
        return f"{color}{label}{palette.reset}"

    print("\n=== Loop Summary =============================================")
    print(
        f"{palette.label}Generator{palette.reset}: {_format(gen_state, gen_state.upper())} — {gen_message}"
    )
    print(
        f"{palette.label}Discriminator{palette.reset}: {_format(disc_state, disc_state.upper())} — {disc_message}"
    )
    if notes:
        for note in notes:
            print(f"  - {note}")
    print("==============================================================")


def _collect_summary_lines(
    generator_code: int | None,
    discriminator_code: int | None,
    notes: Optional[List[str]] = None,
) -> List[str]:
    lines: List[str] = []
    gen_state, gen_message = _describe_generator_exit(generator_code)
    lines.append(f"Generator: {gen_state.upper()} — {gen_message}")
    disc_state, disc_message = _describe_discriminator_exit(discriminator_code)
    lines.append(f"Discriminator: {disc_state.upper()} — {disc_message}")
    if notes:
        lines.extend(notes)
    return lines


def _perform_audit(context: RexContext, summary: Optional[List[str]] = None) -> None:
    try:
        extra = [("Loop Summary", summary)] if summary else None
        create_audit_snapshot(context, extra_sections=extra)
    except Exception as exc:  # pragma: no cover - filesystem/git errors
        print(f"[loop] Audit snapshot failed: {exc}")


def _print_batch_summary(entries: List[dict[str, Optional[int]]]) -> None:
    if not entries:
        return
    palette = _ansi_palette()
    print("\n=== Loop Batch Summary =======================================")
    print(f"{'Slug':<24} {'Generator':<16} {'Discriminator':<16}")

    def format_status(code: Optional[int]) -> str:
        if code is None:
            return f"{palette.dim}SKIP{palette.reset}"
        if code == 0:
            return f"{palette.success}PASS{palette.reset}"
        return f"{palette.error}FAIL({code}){palette.reset}"

    for entry in entries:
        slug = entry.get("slug", "")
        gen = format_status(entry.get("generator"))
        disc = format_status(entry.get("discriminator"))
        print(f"{slug:<24} {gen:<16} {disc:<16}")
    print("==============================================================")


def _batch_summary_lines(entries: List[dict[str, Optional[int]]]) -> List[str]:
    lines: List[str] = []
    for entry in entries:
        slug = entry.get("slug", "")
        gen = entry.get("generator")
        disc = entry.get("discriminator")
        gen_state, gen_message = _describe_generator_exit(gen)
        disc_state, disc_message = _describe_discriminator_exit(disc)
        lines.append(f"{slug}: Generator {gen_state.upper()} — {gen_message}")
        lines.append(f"{slug}: Discriminator {disc_state.upper()} — {disc_message}")
    return lines


@dataclass
class LoopOptions:
    generator_options: GeneratorOptions = field(default_factory=GeneratorOptions)
    discriminator_options: DiscriminatorOptions = field(
        default_factory=DiscriminatorOptions
    )
    run_generator: bool = True
    run_discriminator: bool = True
    run_feature: bool = True
    run_global: bool = True
    each_features: bool = False
    perform_self_update: bool = True
    explain: bool = False
    verbose: bool = True
    tail_lines: int = 0
    continue_on_fail: bool = False


def run_loop(options: LoopOptions, *, context: RexContext | None = None) -> int:
    context = context or RexContext.discover()
    ensure_monitor_server(context, open_browser=True)
    if options.explain:
        for line in _describe_plan(options, context):
            print(line)
    if options.perform_self_update:
        self_update()
    options.generator_options.verbose = options.verbose
    options.discriminator_options.verbose = options.verbose
    lock_path = context.codex_ci_dir / "rex.lock"
    with lock_file(lock_path):
        run_doctor()
        missing_tools = _missing_tooling(context)
        if missing_tools:
            roster = ", ".join(missing_tools)
            print(f"[loop] Required tooling missing: {roster}")
            print("[loop] Run `./rex-codex init` to install the development toolchain.")
            return 1
        if options.each_features:
            return _run_each(options, context)
        return _run_single(options, context)


def _describe_plan(options: LoopOptions, context: RexContext) -> List[str]:
    lines: List[str] = []
    statuses = options.generator_options.statuses or ["proposed"]
    lines.append(
        f"Self-update: {'enabled' if options.perform_self_update else 'disabled'} "
        "(honours REX_AGENT_NO_UPDATE)"
    )
    lines.append(
        f"Generator phase: {'enabled' if options.run_generator else 'skipped'}"
    )
    if options.run_generator:
        if options.generator_options.card_path:
            target = str(options.generator_options.card_path)
        else:
            target = ", ".join(statuses)
        lines.append(f"  target: {target}")
        lines.append(f"  iterate-each: {'yes' if options.each_features else 'no'}")
    lines.append(
        f"Discriminator phase: {'enabled' if options.run_discriminator else 'skipped'}"
    )
    if options.run_discriminator:
        lines.append(f"  feature shard: {'yes' if options.run_feature else 'no'}")
        lines.append(f"  global sweep: {'yes' if options.run_global else 'no'}")
        lines.append(
            f"  LLM runtime edits: "
            f"{'disabled' if options.discriminator_options.disable_llm else 'enabled'}"
        )
    if options.each_features and options.run_generator:
        cards = discover_cards(
            statuses=options.generator_options.statuses, context=context
        )
        if cards:
            preview = ", ".join(card.slug for card in cards[:5])
            if len(cards) > 5:
                preview += f", … (+{len(cards) - 5} more)"
            lines.append(f"  queued cards: {preview}")
        else:
            lines.append("  queued cards: none")
    return lines


def _run_each(options: LoopOptions, context: RexContext) -> int:
    cards = discover_cards(statuses=options.generator_options.statuses, context=context)
    if not cards:
        statuses = ", ".join(options.generator_options.statuses)
        print(f"[loop] No Feature Cards with statuses: {statuses}")
        return 1

    batch_results: List[dict[str, Optional[int]]] = []
    final_exit = 0

    for card in cards:
        print(f"=== rex-codex loop: processing {card.path} (slug: {card.slug}) ===")
        drift = _card_drift_message(context, card.slug)
        if drift:
            palette = _ansi_palette()
            print(f"{palette.warning}[loop] WARNING:{palette.reset} {drift}")

        generator_exit: Optional[int] = None
        discriminator_exit: Optional[int] = None

        if options.run_generator:
            generator_opts = replace(options.generator_options, card_path=card.path)
            result = run_generator(generator_opts, context=context)
            generator_exit = result
            if result != 0:
                _maybe_tail_logs("generator", options.tail_lines, context)
                print(f"[loop] Generator failed on {card.path} (exit {result})")
                if not options.continue_on_fail:
                    summary_lines = _batch_summary_lines(
                        [
                            {
                                "slug": card.slug,
                                "generator": result,
                                "discriminator": None,
                            }
                        ]
                    )
                    _perform_audit(context, summary_lines)
                    return result
                final_exit = final_exit or result
                batch_results.append(
                    {"slug": card.slug, "generator": result, "discriminator": None}
                )
                continue
            if options.verbose:
                _announce_log(context, "generator_response.log")
        else:
            print("[loop] Generator skipped.")

        if options.run_discriminator:
            exit_code = _run_discriminator_phases(options, card.slug, context)
            discriminator_exit = exit_code
            metadata = _load_discriminator_metadata(context)
            if metadata.get("coverage_failed"):
                palette = _ansi_palette()
                target = metadata.get("coverage_targets") or "coverage targets"
                threshold = metadata.get("coverage_threshold")
                target_display = str(target).strip() or "coverage targets"
                message = f"Coverage shortfall on {target_display}"
                if threshold:
                    message += f" (min {threshold}%)"
                print(f"{palette.warning}[loop] WARNING:{palette.reset} {message}")
            if exit_code != 0:
                if not options.continue_on_fail:
                    summary_lines = _batch_summary_lines(
                        [
                            {
                                "slug": card.slug,
                                "generator": generator_exit,
                                "discriminator": exit_code,
                            }
                        ]
                    )
                    _perform_audit(context, summary_lines)
                    return exit_code
                final_exit = final_exit or exit_code
        else:
            print("[loop] Discriminator skipped.")

        batch_results.append(
            {
                "slug": card.slug,
                "generator": generator_exit,
                "discriminator": discriminator_exit,
            }
        )

    if options.continue_on_fail:
        _print_batch_summary(batch_results)
    summary_lines = _batch_summary_lines(batch_results)
    _perform_audit(context, summary_lines)
    return final_exit


def _run_single(options: LoopOptions, context: RexContext) -> int:
    summary_notes: List[str] = []
    seen_notes: set[str] = set()
    palette = _ansi_palette()

    def note_warning(message: Optional[str]) -> None:
        if not message or message in seen_notes:
            return
        seen_notes.add(message)
        print(f"{palette.warning}[loop] WARNING:{palette.reset} {message}")
        summary_notes.append(message)

    slug_hint: Optional[str] = None
    if options.generator_options.card_path:
        slug_hint = options.generator_options.card_path.stem
    else:
        slug_hint = _discover_active_slug(context)
    note_warning(_card_drift_message(context, slug_hint))

    generator_code: int | None = None
    if options.run_generator:
        print("=== rex-codex loop: generator phase ===")
        generator_code = run_generator(options.generator_options, context=context)
        if generator_code == 0:
            print("[loop] Generator produced new specs; running discriminator…")
            if options.verbose:
                _announce_log(context, "generator_response.log")
        elif generator_code == 1:
            print(
                "[loop] Generator found no matching Feature Cards; running discriminator anyway."
            )
        else:
            print(f"[loop] Generator failed (exit {generator_code}); aborting.")
            _maybe_tail_logs("generator", options.tail_lines, context)
            _render_loop_summary(generator_code=generator_code, discriminator_code=None)
            summary_lines = _collect_summary_lines(generator_code, None, summary_notes)
            _perform_audit(context, summary_lines)
            return generator_code
    else:
        print("[loop] Generator skipped; running discriminator only.")
        generator_code = None

    discriminator_code: int | None = None
    exit_code = 0
    if options.run_discriminator:
        slug = _discover_active_slug(context) or slug_hint
        note_warning(_card_drift_message(context, slug))
        print("=== rex-codex loop: discriminator phase ===")
        discriminator_code = _run_discriminator_phases(options, slug, context)
        exit_code = discriminator_code
        if discriminator_code == 0 and options.verbose:
            _announce_log(context, "latest_discriminator.log")
        metadata = _load_discriminator_metadata(context)
        if metadata.get("coverage_failed"):
            target = metadata.get("coverage_targets") or "coverage targets"
            threshold = metadata.get("coverage_threshold")
            target_display = str(target).strip() or "coverage targets"
            note = f"Coverage shortfall on {target_display}"
            if threshold:
                note += f" (min {threshold}%)"
            note_warning(note)
    else:
        print("[loop] Discriminator skipped; generator phase complete.")
        exit_code = generator_code if generator_code not in (None, 0, 1) else 0

    _render_loop_summary(
        generator_code=generator_code,
        discriminator_code=discriminator_code,
        notes=summary_notes,
    )
    summary_lines = _collect_summary_lines(
        generator_code, discriminator_code, summary_notes
    )
    _perform_audit(context, summary_lines)
    return exit_code


def _run_discriminator_phases(
    options: LoopOptions, slug: str | None, context: RexContext
) -> int:
    if options.run_feature:
        if slug:
            feature_opts = replace(
                options.discriminator_options, mode="feature", slug=slug
            )
            result = run_discriminator(feature_opts, context=context)
            if result != 0:
                _maybe_tail_logs("discriminator", options.tail_lines, context)
                return result
        else:
            print(
                "[loop] No active feature slug; skipping feature-only discriminator run."
            )
    if options.run_global:
        global_opts = replace(options.discriminator_options, mode="global", slug=None)
        result = run_discriminator(global_opts, context=context)
        if result != 0:
            _maybe_tail_logs("discriminator", options.tail_lines, context)
        else:
            _record_card_hash(context, slug)
        return result
    print("[loop] Global discriminator run skipped by flag.")
    return 0


def _discover_active_slug(context: RexContext) -> str | None:
    data = load_rex_agent(context)
    feature = data.get("feature", {})
    slug = feature.get("active_slug")
    if slug:
        return slug
    cards = discover_cards(statuses=["proposed"], context=context)
    return cards[0].slug if cards else None


def _maybe_tail_logs(kind: str, lines: int, context: RexContext) -> None:
    if lines <= 0:
        return
    if kind == "generator":
        show_latest_logs(context, lines=lines, generator=True)
    elif kind == "discriminator":
        show_latest_logs(context, lines=lines, discriminator=True)


def _announce_log(context: RexContext, filename: str) -> None:
    path = context.codex_ci_dir / filename
    if path.exists():
        print(f"[loop] Logs: {context.relative(path)}")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/monitoring.py ===
"""Helpers for launching the local monitoring UI."""

from __future__ import annotations

import os
import subprocess
from typing import Optional

from .utils import RexContext, which

_MONITOR_STARTED = False


def ensure_monitor_server(
    context: RexContext,
    *,
    open_browser: bool = True,
    extra_env: Optional[dict[str, str]] = None,
) -> None:
    """Launch the monitor web server in the background if available.

    The monitor is optional; failures to spawn are ignored so the core agent
    workflow keeps running even when Node/monitor assets are missing.
    """

    if os.environ.get("REX_DISABLE_MONITOR_UI", "").lower() in {"1", "true", "yes"}:
        return

    global _MONITOR_STARTED
    if _MONITOR_STARTED:
        return

    launcher = context.root / "monitor" / "agent" / "launch-monitor.js"
    if not launcher.exists():
        return

    node = which("node")
    if node is None:
        return

    os.environ.setdefault("LOG_DIR", str(context.monitor_log_dir))
    os.environ.setdefault("REPO_ROOT", str(context.root))
    os.environ.setdefault("GENERATOR_UI_POPOUT", "0")
    os.environ.setdefault("GENERATOR_UI_TUI", "0")

    env = os.environ.copy()
    env.setdefault("LOG_DIR", str(context.monitor_log_dir))
    env.setdefault("REPO_ROOT", str(context.root))
    env.setdefault("MONITOR_PORT", os.environ.get("MONITOR_PORT", "4321"))
    env.setdefault("GENERATOR_UI_POPOUT", os.environ.get("GENERATOR_UI_POPOUT", "0"))
    env.setdefault("GENERATOR_UI_TUI", os.environ.get("GENERATOR_UI_TUI", "0"))

    if open_browser:
        if os.environ.get("REX_MONITOR_OPEN_BROWSER", "").lower() in {"0", "false"}:
            env.setdefault("OPEN_BROWSER", "false")
        else:
            env.setdefault("OPEN_BROWSER", "true")
    else:
        env.setdefault("OPEN_BROWSER", env.get("OPEN_BROWSER", "false"))

    if extra_env:
        env.update(extra_env)

    args = [node, str(launcher), "--background"]
    try:
        result = subprocess.run(
            args,
            cwd=context.root,
            env=env,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=False,
            timeout=10,
        )
    except (OSError, subprocess.TimeoutExpired):
        return

    stdout = (result.stdout or "").strip()
    if stdout:
        for line in stdout.splitlines():
            print(f"[monitor] {line}")
    elif result.returncode != 0 and result.stderr:
        print("[monitor] Failed to launch UI:", result.stderr.strip())

    if result.returncode == 0:
        _MONITOR_STARTED = True

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/playbook.py ===
"""Implementation of the Codex testing playbook.

This module codifies the guidance from AGENTS.md into deterministic helpers that
translate Feature Cards into canonical data, assumption ledgers, scenario plans,
and repository intelligence snapshots. The generator imports these artefacts to
keep prompts grounded and to persist traceability evidence for audits.
"""

from __future__ import annotations

import csv
import json
import re
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

from .cards import FeatureCard
from .utils import RexContext, ensure_dir

# ---------------------------------------------------------------------------
# Canonical data model
# ---------------------------------------------------------------------------


def _slug_to_feature_id(slug: str) -> str:
    token = re.sub(r"[^A-Za-z0-9]+", "-", slug).strip("-")
    token = token.upper()
    if token.startswith("FC-"):
        return token
    return f"FC-{token}"


def _normalize_heading(name: str) -> str:
    normalized = re.sub(r"[^a-z0-9]+", "_", name.lower()).strip("_")
    return normalized or "section"


def _parse_sections(text: str) -> Tuple[Dict[str, List[str]], Optional[str]]:
    sections: Dict[str, List[str]] = {"__root__": []}
    current = "__root__"
    first_heading: Optional[str] = None
    for raw_line in text.splitlines():
        line = raw_line.rstrip("\n")
        heading = re.match(r"^(#{1,6})\s+(.*)$", line)
        if heading:
            level = len(heading.group(1))
            title = heading.group(2).strip()
            key = _normalize_heading(title)
            sections.setdefault(key, [])
            current = key
            if level == 1 and first_heading is None:
                first_heading = title
            continue
        sections.setdefault(current, []).append(line)
    return sections, first_heading


def _extract_metadata(lines: List[str]) -> Dict[str, str]:
    metadata: Dict[str, str] = {}
    for line in lines:
        if not line or line.lstrip().startswith("#"):
            continue
        if ":" not in line:
            continue
        key, value = line.split(":", 1)
        key = key.strip().lower()
        value = value.strip()
        if not key:
            continue
        metadata[key] = value
    return metadata


def _extract_bullets(lines: List[str]) -> List[str]:
    bullets: List[str] = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith("- "):
            bullets.append(stripped[2:].strip())
    return bullets


def _extract_keyed_lists(lines: List[str]) -> Dict[str, List[str]]:
    keyed: Dict[str, List[str]] = {}
    current: Optional[str] = None
    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if stripped.endswith(":"):
            current = _normalize_heading(stripped[:-1])
            keyed.setdefault(current, [])
            continue
        if stripped.startswith("- "):
            value = stripped[2:].strip()
            if current:
                keyed.setdefault(current, []).append(value)
            else:
                keyed.setdefault("items", []).append(value)
        elif current:
            keyed.setdefault(current, []).append(stripped)
    return keyed


def _parse_csv_list(value: str) -> List[str]:
    if not value:
        return []
    value = value.strip()
    if value.startswith("[") and value.endswith("]"):
        inner = value[1:-1]
        items = [item.strip().strip("'\"") for item in inner.split(",")]
        return [item for item in items if item]
    items = [item.strip() for item in re.split(r"[,;]", value)]
    if len(items) == 1 and " " in items[0]:
        # allow space-separated dependency list
        items = [item.strip() for item in items[0].split() if item.strip()]
    return [item for item in items if item]


def _strip_wrapper(text: str) -> str:
    return text.strip().strip("\"'`")


@dataclass
class AcceptanceCriterion:
    id: str
    text: str

    def to_dict(self) -> Dict[str, str]:
        return {"id": self.id, "text": self.text}


@dataclass
class ObservabilityHints:
    logs: List[str] = field(default_factory=list)
    events: List[str] = field(default_factory=list)
    metrics: List[str] = field(default_factory=list)
    traces: List[str] = field(default_factory=list)
    other: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, List[str]]:
        return {
            "logs": self.logs,
            "events": self.events,
            "metrics": self.metrics,
            "traces": self.traces,
            "other": self.other,
        }


@dataclass
class FeatureCardModel:
    slug: str
    card_path: str
    id: str
    title: str
    epic: str
    risk_level: str
    priority: str
    owner: str
    version: int
    dependencies: List[str]
    acceptance_criteria: List[AcceptanceCriterion]
    non_goals: List[str]
    open_questions: List[str]
    constraints: Dict[str, List[str]]
    observability: ObservabilityHints
    notes: str
    summary: str

    def to_dict(self) -> Dict[str, object]:
        data = asdict(self)
        data["acceptance_criteria"] = [
            criterion.to_dict() for criterion in self.acceptance_criteria
        ]
        data["observability"] = self.observability.to_dict()
        return data


def canonicalize_feature_card(card: FeatureCard) -> FeatureCardModel:
    text = card.path.read_text(encoding="utf-8")
    sections, first_heading = _parse_sections(text)
    metadata = _extract_metadata(sections.get("__root__", []))
    summary_lines = sections.get("summary", [])
    notes_lines = sections.get("notes", [])

    meta_aliases = {
        "id": "id",
        "feature_id": "id",
        "feature-card": "id",
        "feature_card": "id",
        "card_id": "id",
        "risk": "risk_level",
        "risk_level": "risk_level",
        "priority": "priority",
        "owner": "owner",
        "team": "owner",
        "version": "version",
        "dependencies": "dependencies",
        "depends": "dependencies",
        "epic": "epic",
    }

    meta_store: Dict[str, str] = {}
    for key, value in metadata.items():
        target = meta_aliases.get(key)
        if not target:
            continue
        meta_store[target] = value

    title = first_heading or card.slug.replace("-", " ").title()
    card_id = meta_store.get("id") or _slug_to_feature_id(card.slug)
    epic = meta_store.get("epic", "")
    risk_level = (meta_store.get("risk_level") or "unknown").lower()
    priority = meta_store.get("priority", "unknown").upper()
    owner = meta_store.get("owner", "")
    version_value = meta_store.get("version", "1")
    try:
        version = int(float(version_value))
    except ValueError:
        version = 1
    dependencies = _parse_csv_list(meta_store.get("dependencies", ""))

    acceptance_lines = []
    for tag in ("acceptance_criteria", "acceptance", "criteria"):
        if sections.get(tag):
            acceptance_lines = sections.get(tag, [])
            break
    acceptance_bullets = _extract_bullets(acceptance_lines)
    acceptance: List[AcceptanceCriterion] = []
    for index, bullet in enumerate(acceptance_bullets, start=1):
        match = re.match(
            r"^(AC(?:[-_#\s]?)(\d+))[:\s.-]*(.*)$", bullet, flags=re.IGNORECASE
        )
        if match:
            number = match.group(2)
            remainder = match.group(3).strip() or bullet
            acceptance.append(AcceptanceCriterion(f"AC-{int(number)}", remainder))
        else:
            acceptance.append(AcceptanceCriterion(f"AC-{index}", bullet.strip()))

    non_goals = _extract_bullets(
        sections.get("non_goals", [])
        or sections.get("non-goals", [])
        or sections.get("out_of_scope", [])
    )
    open_questions = _extract_bullets(
        sections.get("open_questions", [])
        or sections.get("questions", [])
        or sections.get("unknowns", [])
    )

    constraint_lines = (
        sections.get("constraints", [])
        or sections.get("limitations", [])
        or sections.get("domain_invariants", [])
    )
    constraints = _extract_keyed_lists(constraint_lines)
    if not constraints:
        # Preserve raw text when structure is unknown
        filtered = [line for line in constraint_lines if line.strip()]
        if filtered:
            constraints["items"] = filtered

    observability_lines = (
        sections.get("observability", [])
        or sections.get("observability_hints", [])
        or sections.get("telemetry", [])
    )
    observability_pairs = _extract_keyed_lists(observability_lines)
    observability = ObservabilityHints()
    for key, values in observability_pairs.items():
        if key in {"logs", "log"}:
            observability.logs.extend(map(_strip_wrapper, values))
        elif key in {"metrics", "metric"}:
            observability.metrics.extend(map(_strip_wrapper, values))
        elif key in {"events", "event"}:
            observability.events.extend(map(_strip_wrapper, values))
        elif key in {"traces", "trace"}:
            observability.traces.extend(map(_strip_wrapper, values))
        else:
            observability.other.extend(map(_strip_wrapper, values))

    notes_text = "\n".join(line for line in notes_lines if line.strip()).strip()
    summary_text = "\n".join(line for line in summary_lines if line.strip()).strip()

    return FeatureCardModel(
        slug=card.slug,
        card_path=str(card.path),
        id=card_id,
        title=title,
        epic=epic,
        risk_level=risk_level or "unknown",
        priority=priority or "UNKNOWN",
        owner=owner,
        version=version,
        dependencies=dependencies,
        acceptance_criteria=acceptance,
        non_goals=non_goals,
        open_questions=open_questions,
        constraints=constraints,
        observability=observability,
        notes=notes_text,
        summary=summary_text,
    )


# ---------------------------------------------------------------------------
# Assumption ledger
# ---------------------------------------------------------------------------


def _normalise_assumption_text(text: str) -> str:
    return re.sub(r"\s+", " ", text.strip().lower())


@dataclass
class Assumption:
    id: str
    text: str
    rationale: str = ""
    risk: str = "medium"
    default_choice: str = ""
    ways_to_falsify: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, object]:
        return {
            "id": self.id,
            "text": self.text,
            "rationale": self.rationale,
            "risk": self.risk,
            "default_choice": self.default_choice,
            "ways_to_falsify": self.ways_to_falsify,
        }


class AssumptionLedger:
    def __init__(self, path: Path, feature_id: str):
        self.path = path
        self.feature_id = feature_id
        self.assumptions: List[Assumption] = []
        self.escalation_hints: List[str] = []
        self._index: Dict[str, Assumption] = {}

    @classmethod
    def load(cls, context: RexContext, feature: FeatureCardModel) -> "AssumptionLedger":
        ledger_dir = ensure_dir(context.root / "documents" / "assumption_ledgers")
        ledger_path = ledger_dir / f"{feature.slug}.json"
        ledger = cls(ledger_path, feature.id)
        if ledger_path.exists():
            try:
                data = json.loads(ledger_path.read_text(encoding="utf-8"))
            except json.JSONDecodeError:
                data = {}
            for item in data.get("assumptions", []):
                assumption = Assumption(
                    id=item.get("id", ""),
                    text=item.get("text", ""),
                    rationale=item.get("rationale", ""),
                    risk=item.get("risk", "medium"),
                    default_choice=item.get("default_choice", ""),
                    ways_to_falsify=item.get("ways_to_falsify", []),
                )
                if assumption.id:
                    ledger.assumptions.append(assumption)
                    ledger._index[_normalise_assumption_text(assumption.text)] = (
                        assumption
                    )
            ledger.escalation_hints = data.get("escalation_hints", [])
        return ledger

    def _next_id(self) -> str:
        existing_numbers = [
            int(match.group(1))
            for assumption in self.assumptions
            if (match := re.match(r"A-(\d+)", assumption.id))
        ]
        next_number = max(existing_numbers, default=0) + 1
        return f"A-{next_number:03d}"

    def require(
        self,
        text: str,
        *,
        rationale: str,
        risk: str = "medium",
        default_choice: str = "",
        ways_to_falsify: Optional[Sequence[str]] = None,
    ) -> str:
        normalized = _normalise_assumption_text(text)
        existing = self._index.get(normalized)
        if existing:
            return existing.id
        assumption = Assumption(
            id=self._next_id(),
            text=text.strip(),
            rationale=rationale.strip(),
            risk=risk,
            default_choice=default_choice.strip(),
            ways_to_falsify=list(ways_to_falsify or []),
        )
        self.assumptions.append(assumption)
        self._index[normalized] = assumption
        return assumption.id

    def add_escalation_hint(self, hint: str) -> None:
        cleaned = hint.strip()
        if not cleaned:
            return
        if cleaned not in self.escalation_hints:
            self.escalation_hints.append(cleaned)

    def to_dict(self) -> Dict[str, object]:
        return {
            "feature_id": self.feature_id,
            "assumptions": [assumption.to_dict() for assumption in self.assumptions],
            "escalation_hints": self.escalation_hints,
        }

    def save(self) -> None:
        payload = self.to_dict()
        self.path.write_text(
            json.dumps(payload, indent=2, ensure_ascii=False) + "\n", encoding="utf-8"
        )


# ---------------------------------------------------------------------------
# Repository intelligence
# ---------------------------------------------------------------------------


EXTENSION_LANG_MAP = {
    ".py": "python",
    ".js": "javascript",
    ".ts": "typescript",
    ".tsx": "typescript",
    ".jsx": "javascript",
    ".go": "go",
    ".rs": "rust",
    ".java": "java",
    ".cs": "csharp",
    ".rb": "ruby",
    ".php": "php",
}


@dataclass
class RepositoryInventory:
    languages: List[str]
    test_frameworks: List[str]
    important_paths: Dict[str, str]
    feature_tags: Dict[str, List[str]]
    api_schemas: List[str]
    event_emitters: Dict[str, List[str]]

    def to_dict(self) -> Dict[str, object]:
        return {
            "languages": self.languages,
            "test_frameworks": self.test_frameworks,
            "important_paths": self.important_paths,
            "feature_tags": self.feature_tags,
            "api_schemas": self.api_schemas,
            "event_emitters": self.event_emitters,
        }

    def components_for_feature(self, feature_id: str, slug: str) -> List[str]:
        matches = set()
        lookup_keys = {feature_id.upper(), slug.upper(), slug.replace("-", "_").upper()}
        for key, paths in self.feature_tags.items():
            if key.upper() in lookup_keys:
                matches.update(paths)
        return sorted(matches)


def inventory_repository(context: RexContext) -> RepositoryInventory:
    root = context.root
    languages: set[str] = set()
    feature_tags: Dict[str, List[str]] = {}
    event_emitters: Dict[str, List[str]] = {}

    feature_pattern = re.compile(r"FC-[A-Za-z0-9_-]+")
    event_pattern = re.compile(
        r"""(?:
        emit\(\s*["'](?P<event1>[a-zA-Z0-9_.:-]+)["']
        |
        ["'](?P<event2>[a-zA-Z0-9_.:-]+)["']\s*\)
        )""",
        re.VERBOSE,
    )

    search_extensions = {".py", ".js", ".ts", ".tsx", ".jsx", ".md"}

    for path in root.rglob("*"):
        if path.is_dir():
            continue
        suffix = path.suffix.lower()
        language = EXTENSION_LANG_MAP.get(suffix)
        if language:
            languages.add(language)
        if suffix not in search_extensions:
            continue
        try:
            text = path.read_text(encoding="utf-8", errors="ignore")
        except OSError:
            continue
        rel_path = context.relative(path)
        for match in feature_pattern.findall(text):
            feature_tags.setdefault(match.upper(), []).append(rel_path)
        for event_match in event_pattern.finditer(text):
            event_name = event_match.group("event1") or event_match.group("event2")
            if not event_name:
                continue
            event_emitters.setdefault(event_name, []).append(rel_path)

    test_frameworks: List[str] = []
    if (root / "pytest.ini").exists() or (root / "pyproject.toml").exists():
        test_frameworks.append("pytest")
    if (root / "playwright.config.ts").exists() or (
        root / "playwright.config.js"
    ).exists():
        test_frameworks.append("playwright")
    if (root / "package.json").exists():
        try:
            pkg = json.loads((root / "package.json").read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            pkg = {}
        scripts = ",".join(pkg.get("scripts", {}).keys())
        deps = ",".join(pkg.get("dependencies", {}).keys())
        dev_deps = ",".join(pkg.get("devDependencies", {}).keys())
        combined = f"{scripts},{deps},{dev_deps}".lower()
        if "jest" in combined and "jest" not in test_frameworks:
            test_frameworks.append("jest")

    api_schemas: List[str] = []
    for candidate in root.rglob("*.yaml"):
        name = candidate.name.lower()
        if "openapi" in name or "swagger" in name:
            api_schemas.append(context.relative(candidate))
    for candidate in root.rglob("*.graphql"):
        api_schemas.append(context.relative(candidate))

    important_paths = {
        "tests_dir": context.relative(root / "tests"),
        "src_dir": context.relative(root / "src"),
        "documents_dir": context.relative(root / "documents"),
    }

    return RepositoryInventory(
        languages=sorted(languages),
        test_frameworks=sorted(test_frameworks),
        important_paths=important_paths,
        feature_tags={key: sorted(set(paths)) for key, paths in feature_tags.items()},
        api_schemas=sorted(api_schemas),
        event_emitters={
            key: sorted(set(paths)) for key, paths in event_emitters.items()
        },
    )


# ---------------------------------------------------------------------------
# Scenario synthesis
# ---------------------------------------------------------------------------


@dataclass
class Scenario:
    id: str
    kind: str
    summary: str
    preconditions: List[str]
    steps: List[str]
    assertions: List[str]
    observables: List[str]
    assumptions: List[str]
    test_types: List[str]
    components: List[str]

    def to_dict(self) -> Dict[str, object]:
        return asdict(self)


@dataclass
class Capability:
    id: str
    source_ac: str
    statement: str
    preconditions: List[str]
    triggers: List[str]
    observables: List[str]
    negative_space: List[str]
    measurement_strategy: List[str]
    test_types: List[str]
    edge_cases: List[str]
    invariants: List[str]
    scenarios: List[Scenario]

    def to_dict(self) -> Dict[str, object]:
        data = asdict(self)
        data["scenarios"] = [scenario.to_dict() for scenario in self.scenarios]
        return data


@dataclass
class TestSpecGraph:
    feature_card_id: str
    capabilities: List[Capability]

    def to_dict(self) -> Dict[str, object]:
        return {
            "feature_card_id": self.feature_card_id,
            "capabilities": [capability.to_dict() for capability in self.capabilities],
        }


def _split_sentences(text: str) -> List[str]:
    raw = re.split(r"(?<=[.!?])\s+", text)
    return [segment.strip() for segment in raw if segment.strip()]


def _lower(text: str) -> str:
    return text.lower()


def _extract_phrases_by_keywords(
    sentences: Iterable[str], keywords: Sequence[str]
) -> List[str]:
    matches: List[str] = []
    for sentence in sentences:
        lowered = sentence.lower()
        if any(keyword in lowered for keyword in keywords):
            matches.append(sentence)
    return matches


def _fallback_assumption(
    ledger: AssumptionLedger, capability_id: str, subject: str
) -> str:
    return ledger.require(
        f"{capability_id}: clarify {subject}",
        rationale="Auto-generated because the Feature Card omits this detail.",
        risk="medium",
        default_choice="Document behaviour with product/QA follow-up.",
        ways_to_falsify=[
            "Product guidance contradicts this assumption",
            "Existing implementation documents explicit behaviour",
        ],
    )


def _derive_measurements(observables: List[str]) -> List[str]:
    measurements: List[str] = []
    for observable in observables:
        measurements.append(f"Validate observable: {observable}")
    if not measurements:
        measurements.append(
            "Establish measurable outcome for this capability (event, API response, or state change)."
        )
    return measurements


def _derive_invariants(constraints: Dict[str, List[str]]) -> List[str]:
    invariants: List[str] = []
    for key, values in constraints.items():
        if "invariant" in key or "domain" in key:
            invariants.extend(values)
    return invariants


def _select_test_types(
    capability_statement: str,
    observables: List[str],
    repo_inventory: RepositoryInventory,
) -> List[str]:
    lowered = capability_statement.lower()
    types: List[str] = ["unit", "integration"]
    if any(term in lowered for term in ("api", "endpoint", "http", "response")):
        types.append("contract")
    if any(term in lowered for term in ("ui", "screen", "button", "page")):
        types.append("e2e")
    if any("event" in obs.lower() for obs in observables):
        types.append("contract")
    if "playwright" in repo_inventory.test_frameworks:
        types.append("e2e")
    return sorted(dict.fromkeys(types))


def _scenario_test_types(kind: str, base_types: Sequence[str]) -> List[str]:
    mapping = {
        "happy_path": ("integration", "e2e"),
        "boundary": ("unit", "property", "integration"),
        "negative": ("unit", "integration"),
        "idempotency": ("property", "integration"),
    }
    return sorted(dict.fromkeys(mapping.get(kind, base_types)))


def _derive_components(
    inventory: RepositoryInventory, feature_id: str, slug: str
) -> List[str]:
    components = inventory.components_for_feature(feature_id, slug)
    if components:
        return components
    default_paths = [inventory.important_paths.get("tests_dir", "tests")]
    return [path for path in default_paths if path]


def _build_scenarios_for_capability(
    *,
    feature: FeatureCardModel,
    capability: Capability,
    sentences: List[str],
    ledger: AssumptionLedger,
    inventory: RepositoryInventory,
) -> List[Scenario]:
    scenarios: List[Scenario] = []
    counter = 1

    def next_id() -> str:
        nonlocal counter
        ident = f"SC-{counter:02d}"
        counter += 1
        return ident

    components = _derive_components(inventory, feature.id, feature.slug)

    # Happy path scenario
    happy_id = next_id()
    happy_observables = capability.observables or capability.preconditions
    happy_assumptions: List[str] = []
    if not capability.triggers:
        happy_assumptions.append(
            _fallback_assumption(ledger, capability.id, "trigger condition")
        )
    if not happy_observables:
        happy_assumptions.append(
            _fallback_assumption(ledger, capability.id, "observable outcome")
        )
    happy_preconditions = capability.preconditions or [
        "System in default state derived from card summary."
    ]
    happy_steps = capability.triggers or [capability.statement]
    scenarios.append(
        Scenario(
            id=happy_id,
            kind="happy_path",
            summary=f"Validate {capability.statement}",
            preconditions=happy_preconditions,
            steps=happy_steps,
            assertions=capability.observables or [capability.statement],
            observables=capability.observables or happy_observables,
            assumptions=happy_assumptions,
            test_types=_scenario_test_types("happy_path", capability.test_types),
            components=components,
        )
    )

    # Boundary scenario heuristics
    boundary_sentences = _extract_phrases_by_keywords(
        sentences,
        [
            "edge",
            "boundary",
            "just before",
            "just after",
            "within",
            "until",
            "before",
            "after",
            "minimum",
            "maximum",
        ],
    )
    if boundary_sentences:
        boundary_id = next_id()
        scenarios.append(
            Scenario(
                id=boundary_id,
                kind="boundary",
                summary=f"Exercise boundary conditions for {capability.statement}",
                preconditions=happy_preconditions,
                steps=boundary_sentences,
                assertions=capability.observables or boundary_sentences,
                observables=capability.observables or boundary_sentences,
                assumptions=[],
                test_types=_scenario_test_types("boundary", capability.test_types),
                components=components,
            )
        )

    # Negative scenario heuristics
    negative_sentences = _extract_phrases_by_keywords(
        sentences,
        ["cannot", "must not", "should not", "reject", "error", "invalid"],
    )
    if negative_sentences:
        negative_id = next_id()
        scenarios.append(
            Scenario(
                id=negative_id,
                kind="negative",
                summary=f"Reject invalid flows for {capability.statement}",
                preconditions=happy_preconditions,
                steps=negative_sentences,
                assertions=negative_sentences,
                observables=capability.observables or negative_sentences,
                assumptions=[],
                test_types=_scenario_test_types("negative", capability.test_types),
                components=components,
            )
        )

    # Idempotency scenario encourages monotonic improvement
    idempotency_id = next_id()
    assumption = ledger.require(
        f"{capability.id}: repeated trigger should be idempotent",
        rationale="Guard against regressions when actions repeat.",
        risk="medium",
        default_choice="Repeated invocation preserves state.",
        ways_to_falsify=["Existing system intentionally allows repeated side-effects"],
    )
    scenarios.append(
        Scenario(
            id=idempotency_id,
            kind="idempotency",
            summary=f"Repeated execution of {capability.statement} is idempotent",
            preconditions=happy_preconditions,
            steps=[
                "Execute capability once to reach expected state",
                "Execute the same trigger again",
            ],
            assertions=["State and observable outputs remain unchanged"],
            observables=capability.observables or ["State unchanged"],
            assumptions=[assumption],
            test_types=_scenario_test_types("idempotency", capability.test_types),
            components=components,
        )
    )

    return scenarios


def build_test_spec_graph(
    feature: FeatureCardModel,
    *,
    ledger: AssumptionLedger,
    inventory: RepositoryInventory,
) -> TestSpecGraph:
    capabilities: List[Capability] = []
    constraints = feature.constraints
    invariants = _derive_invariants(constraints)

    if not feature.acceptance_criteria:
        # Create placeholder capability when the card is missing ACs
        placeholder = Capability(
            id="CAP-1",
            source_ac="AC-1",
            statement="Documented behaviour missing from Feature Card.",
            preconditions=[],
            triggers=[],
            observables=[],
            negative_space=[],
            measurement_strategy=["Define acceptance criteria for this Feature Card."],
            test_types=["unit", "integration"],
            edge_cases=[],
            invariants=invariants,
            scenarios=[],
        )
        assumption_id = ledger.require(
            "Feature Card lacks explicit acceptance criteria.",
            rationale="Specs cannot proceed without acceptance criteria; placeholder added.",
            risk="high",
            default_choice="Collaborate with product to capture criteria.",
            ways_to_falsify=["Product requirements document includes explicit ACs."],
        )
        placeholder.scenarios = [
            Scenario(
                id="SC-01",
                kind="gap",
                summary="Capture missing acceptance criteria before proceeding.",
                preconditions=[],
                steps=["Document acceptance criteria for this feature."],
                assertions=["Acceptance criteria recorded in Feature Card."],
                observables=["Updated Feature Card"],
                assumptions=[assumption_id],
                test_types=["process"],
                components=[],
            )
        ]
        capabilities.append(placeholder)
        return TestSpecGraph(feature.id, capabilities)

    for index, criterion in enumerate(feature.acceptance_criteria, start=1):
        cap_id = f"CAP-{index}"
        sentences = _split_sentences(criterion.text)
        preconditions = _extract_phrases_by_keywords(
            sentences, ["given ", "given that", "assume"]
        )
        triggers = _extract_phrases_by_keywords(
            sentences, ["when ", "once ", "after ", "before ", "trigger", "user", "api"]
        )
        observables = _extract_phrases_by_keywords(
            sentences,
            ["then", "should", "must", "ensure", "result", "observable", "state"],
        )
        negative_space = _extract_phrases_by_keywords(
            sentences, ["cannot", "must not", "should not", "never"]
        )
        measurement = _derive_measurements(observables)
        test_types = _select_test_types(criterion.text, observables, inventory)

        capability = Capability(
            id=cap_id,
            source_ac=criterion.id,
            statement=criterion.text,
            preconditions=preconditions,
            triggers=triggers,
            observables=observables,
            negative_space=negative_space,
            measurement_strategy=measurement,
            test_types=test_types,
            edge_cases=[],
            invariants=invariants,
            scenarios=[],
        )
        capability.scenarios = _build_scenarios_for_capability(
            feature=feature,
            capability=capability,
            sentences=sentences,
            ledger=ledger,
            inventory=inventory,
        )
        capabilities.append(capability)

    return TestSpecGraph(feature.id, capabilities)


# ---------------------------------------------------------------------------
# Artefact emission
# ---------------------------------------------------------------------------


@dataclass
class PlaybookArtifacts:
    feature: FeatureCardModel
    inventory: RepositoryInventory
    graph: TestSpecGraph
    ledger: AssumptionLedger
    traceability_rows: List[Dict[str, str]]
    prompt_block: str

    def to_dict(self) -> Dict[str, object]:
        return {
            "feature_card": self.feature.to_dict(),
            "repository_inventory": self.inventory.to_dict(),
            "test_spec_graph": self.graph.to_dict(),
            "assumptions": self.ledger.to_dict(),
            "traceability": self.traceability_rows,
            "prompt_block": self.prompt_block,
        }


def _build_traceability_rows(
    feature: FeatureCardModel, graph: TestSpecGraph
) -> List[Dict[str, str]]:
    rows: List[Dict[str, str]] = []
    for capability in graph.capabilities:
        for scenario in capability.scenarios:
            test_id = "-".join(
                part
                for part in (
                    feature.id.replace(" ", "").upper(),
                    capability.id,
                    scenario.id,
                )
                if part
            )
            rows.append(
                {
                    "test_id": test_id,
                    "feature_card": feature.id,
                    "capability": capability.id,
                    "scenario": scenario.id,
                    "observables": "; ".join(sorted(set(scenario.observables))),
                    "assumptions": "; ".join(sorted(scenario.assumptions)),
                    "test_type": "; ".join(sorted(set(scenario.test_types))),
                    "components": "; ".join(sorted(set(scenario.components))),
                }
            )
    return rows


def _render_prompt_block(artifacts: PlaybookArtifacts) -> str:
    feature = artifacts.feature
    graph = artifacts.graph
    ledger = artifacts.ledger
    inventory = artifacts.inventory

    lines = [
        f"Feature Card ID: {feature.id}",
        f"Title: {feature.title}",
        f"Priority: {feature.priority} | Risk: {feature.risk_level} | Owner: {feature.owner or 'unknown'}",
    ]
    if feature.dependencies:
        lines.append(f"Dependencies: {', '.join(feature.dependencies)}")
    if feature.summary:
        lines.append(f"Summary: {feature.summary}")
    lines.append("")
    lines.append("Acceptance Criteria → Capabilities:")
    for capability in graph.capabilities:
        lines.append(
            f"- {capability.id} ({capability.source_ac}): {capability.statement}"
        )
        if capability.preconditions:
            lines.append(f"  Preconditions: {', '.join(capability.preconditions)}")
        if capability.triggers:
            lines.append(f"  Triggers: {', '.join(capability.triggers)}")
        if capability.observables:
            lines.append(f"  Observables: {', '.join(capability.observables)}")
        lines.append(f"  Test types: {', '.join(capability.test_types)}")
        for scenario in capability.scenarios:
            lines.append(f"    • {scenario.id} [{scenario.kind}]: {scenario.summary}")
            if scenario.assumptions:
                lines.append(f"      Assumptions: {', '.join(scenario.assumptions)}")
    lines.append("")
    if ledger.assumptions:
        lines.append("Assumption Ledger:")
        for assumption in ledger.assumptions:
            lines.append(
                f"- {assumption.id} ({assumption.risk}): {assumption.text} "
                f"[default={assumption.default_choice}]"
            )
    if ledger.escalation_hints:
        lines.append("")
        lines.append("Escalation Hints:")
        for hint in ledger.escalation_hints:
            lines.append(f"- {hint}")
    lines.append("")
    lines.append("Repository Signals:")
    lines.append(f"- Languages: {', '.join(inventory.languages) or 'unknown'}")
    lines.append(
        f"- Test frameworks: {', '.join(inventory.test_frameworks) or 'unspecified'}"
    )
    mapped_components = inventory.components_for_feature(feature.id, feature.slug)
    if mapped_components:
        lines.append(f"- Existing feature tags: {', '.join(mapped_components)}")
    if inventory.api_schemas:
        lines.append(f"- API schemas: {', '.join(inventory.api_schemas)}")
    if inventory.event_emitters:
        sample = ", ".join(sorted(list(inventory.event_emitters.keys())[:5]))
        lines.append(f"- Known events: {sample}")
    return "\n".join(lines)


def build_playbook_artifacts(
    *,
    card: FeatureCard,
    context: RexContext,
) -> PlaybookArtifacts:
    feature = canonicalize_feature_card(card)
    inventory = inventory_repository(context)
    ledger = AssumptionLedger.load(context, feature)
    graph = build_test_spec_graph(feature, ledger=ledger, inventory=inventory)
    traceability_rows = _build_traceability_rows(feature, graph)
    artifacts = PlaybookArtifacts(
        feature=feature,
        inventory=inventory,
        graph=graph,
        ledger=ledger,
        traceability_rows=traceability_rows,
        prompt_block="",  # set below after ledger refresh
    )
    artifacts.prompt_block = _render_prompt_block(artifacts)

    # Persist artefacts
    codex_ci = ensure_dir(context.codex_ci_dir)
    playbook_json = codex_ci / f"playbook_{card.slug}.json"
    playbook_prompt = codex_ci / f"playbook_{card.slug}.prompt"
    traceability_csv = codex_ci / f"traceability_{card.slug}.csv"

    ledger.save()

    playbook_json.write_text(
        json.dumps(artifacts.to_dict(), indent=2, ensure_ascii=False) + "\n",
        encoding="utf-8",
    )
    playbook_prompt.write_text(artifacts.prompt_block + "\n", encoding="utf-8")

    if traceability_rows:
        with traceability_csv.open("w", newline="", encoding="utf-8") as handle:
            writer = csv.DictWriter(
                handle,
                fieldnames=[
                    "test_id",
                    "feature_card",
                    "capability",
                    "scenario",
                    "observables",
                    "assumptions",
                    "test_type",
                    "components",
                ],
            )
            writer.writeheader()
            for row in traceability_rows:
                writer.writerow(row)
    else:
        traceability_csv.write_text(
            "test_id,feature_card,capability,scenario,observables,assumptions,test_type,components\n",
            encoding="utf-8",
        )

    return artifacts


# Public exports
__all__ = [
    "AcceptanceCriterion",
    "Assumption",
    "AssumptionLedger",
    "Capability",
    "FeatureCardModel",
    "ObservabilityHints",
    "PlaybookArtifacts",
    "RepositoryInventory",
    "Scenario",
    "TestSpecGraph",
    "build_playbook_artifacts",
    "build_test_spec_graph",
    "canonicalize_feature_card",
    "inventory_repository",
]

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/self_update.py ===
"""Compatibility bridge exposing self-update helpers within the project scope."""

from __future__ import annotations

from ..scope_global.self_update import *  # noqa: F401,F403

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/status.py ===
"""Helpers to surface rex-agent.json metadata."""

from __future__ import annotations

import datetime as dt
import json
from pathlib import Path
from typing import Any, Dict, Iterable

from .cards import (
    FeatureCard,
    card_content_hash,
    card_path_for,
    discover_cards,
    load_rex_agent,
)
from .utils import RexContext


def _format_timestamp(value: str | None) -> str:
    if not value:
        return "unknown"
    try:
        normalized = value[:-1] + "+00:00" if value.endswith("Z") else value
        when = dt.datetime.fromisoformat(normalized)
    except ValueError:
        return value
    return when.isoformat(timespec="seconds")


def summarize_context(context: RexContext) -> Dict[str, Any]:
    data = load_rex_agent(context)
    feature = data.get("feature", {})
    active_slug = feature.get("active_slug")
    active_card_path = feature.get("active_card")
    active_card: FeatureCard | None = None
    card_path: Path | None = None

    if active_card_path:
        path = (context.root / active_card_path).resolve()
        if path.exists():
            active_card = FeatureCard(
                path=path, slug=active_slug or path.stem, status=""
            )
            card_path = path
    else:
        cards = discover_cards(context=context, statuses=["proposed"])
        active_card = cards[0] if cards else None
        if active_card is not None:
            candidate = card_path_for(context, active_card.slug)
            if candidate.exists():
                card_path = candidate

    if card_path is None and active_slug:
        candidate = card_path_for(context, active_slug)
        if candidate.exists():
            card_path = candidate

    card_hashes = (
        feature.get("card_hashes")
        if isinstance(feature.get("card_hashes"), dict)
        else {}
    )
    stored_hash = (
        card_hashes.get(active_slug) if isinstance(card_hashes, dict) else None
    )
    current_hash = card_content_hash(card_path) if card_path else None
    hash_drift = bool(stored_hash and current_hash and stored_hash != current_hash)

    discriminator_state = data.get("discriminator", {})

    return {
        "active_slug": active_slug or (active_card.slug if active_card else None),
        "active_card": active_card_path
        or (str(active_card.relative_path) if active_card else None),
        "stages": data.get("stages"),
        "llm": data.get("llm"),
        "feature": {
            "active_card": active_card_path,
            "active_slug": active_slug,
            "updated_at": _format_timestamp(feature.get("updated_at")),
            "stored_hash": stored_hash,
            "current_hash": current_hash,
            "hash_drift": hash_drift,
        },
        "discriminator": {
            "last_mode": discriminator_state.get("last_mode"),
            "last_slug": discriminator_state.get("last_slug"),
            "last_green_at": _format_timestamp(
                discriminator_state.get("last_green_at")
            ),
            "last_test_count": discriminator_state.get("last_test_count"),
        },
    }


def render_status(context: RexContext, *, json_output: bool = False) -> None:
    summary = summarize_context(context)
    if json_output:
        print(json.dumps(summary, indent=2, sort_keys=True))
        return
    print("Active Feature:")
    print(f"  slug: {summary.get('active_slug') or 'none'}")
    print(f"  card: {summary.get('active_card') or 'none'}")
    feature = summary.get("feature", {})
    print(f"  updated_at: {feature.get('updated_at')}")
    stored_hash = feature.get("stored_hash")
    current_hash = feature.get("current_hash")
    if stored_hash or current_hash:
        print(f"  stored_hash: {stored_hash or 'none'}")
        print(f"  current_hash: {current_hash or 'none'}")
        drift = "YES" if feature.get("hash_drift") else "no"
        print(f"  hash_drift: {drift}")
    stages = summary.get("stages")
    if isinstance(stages, Iterable):
        print("Configured Stages:")
        for stage in stages:
            print(f"  - {stage}")
    llm = summary.get("llm")
    if isinstance(llm, dict):
        print("LLM Settings:")
        for key, value in llm.items():
            print(f"  {key}: {value}")
    discriminator = summary.get("discriminator")
    if isinstance(discriminator, dict):
        print("Discriminator:")
        print(f"  last_mode: {discriminator.get('last_mode') or 'unknown'}")
        print(f"  last_slug: {discriminator.get('last_slug') or 'none'}")
        print(f"  last_green_at: {discriminator.get('last_green_at')}")
        if discriminator.get("last_test_count") is not None:
            print(f"  last_test_count: {discriminator.get('last_test_count')}")

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_project/utils.py ===
"""Shared utilities for the rex-codex Python CLI."""

from __future__ import annotations

import json
import os
import shlex
import subprocess
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import (
    Dict,
    Iterator,
    List,
    Mapping,
    MutableMapping,
    Optional,
    Sequence,
    Set,
)


class RexError(RuntimeError):
    """Raised when a command should exit with a non-zero status."""


def _env_root() -> Optional[Path]:
    root = os.environ.get("ROOT")
    if root:
        return Path(root).resolve()
    return None


def repo_root() -> Path:
    """Return the repository root, favouring the git toplevel."""
    if cached := _env_root():
        return cached
    try:
        completed = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,
            text=True,
        )
        return Path(completed.stdout.strip()).resolve()
    except subprocess.CalledProcessError:
        return Path.cwd().resolve()


def agent_home(root: Path | None = None) -> Path:
    root = root or repo_root()
    return root / ".rex_agent"


def agent_src(root: Path | None = None) -> Path:
    root = root or repo_root()
    env_src = os.environ.get("REX_SRC")
    if env_src:
        return Path(env_src).resolve()
    return agent_home(root) / "src"


def ensure_dir(path: Path) -> Path:
    path.mkdir(parents=True, exist_ok=True)
    return path


def load_json(path: Path) -> dict:
    if not path.exists():
        return {}
    return json.loads(path.read_text(encoding="utf-8"))


def dump_json(path: Path, data: Mapping) -> None:
    text = json.dumps(data, indent=2, sort_keys=True)
    path.write_text(f"{text}\n", encoding="utf-8")


def which(executable: str) -> Optional[str]:
    from shutil import which as _which

    return _which(executable)


def shlex_join(cmd: Sequence[str]) -> str:
    return shlex.join(cmd)


def run(
    cmd: Sequence[str],
    *,
    cwd: Path | None = None,
    env: Mapping[str, str] | None = None,
    check: bool = True,
    capture_output: bool = False,
    text: bool = True,
) -> subprocess.CompletedProcess:
    """Thin wrapper around subprocess.run with sensible defaults."""
    merged_env: MutableMapping[str, str]
    if env is None:
        merged_env = os.environ.copy()
    else:
        merged_env = {**os.environ, **env}
    kwargs: Dict[str, object] = {"cwd": cwd, "env": merged_env, "check": check}
    if capture_output:
        kwargs["stdout"] = subprocess.PIPE
        kwargs["stderr"] = subprocess.PIPE
    if text:
        kwargs["text"] = True
    return subprocess.run(list(cmd), **kwargs)  # type: ignore[arg-type]


@dataclass(frozen=True)
class RexContext:
    root: Path
    codex_ci_dir: Path
    monitor_log_dir: Path
    rex_agent_file: Path
    venv_dir: Path

    @classmethod
    def discover(cls) -> "RexContext":
        root = repo_root()
        codex_ci = ensure_dir(root / ".codex_ci")
        monitor_logs = ensure_dir(root / ".agent" / "logs")
        return cls(
            root=root,
            codex_ci_dir=codex_ci,
            monitor_log_dir=monitor_logs,
            rex_agent_file=root / "rex-agent.json",
            venv_dir=root / ".venv",
        )

    def relative(self, path: Path) -> str:
        try:
            return str(path.relative_to(self.root))
        except ValueError:
            return str(path)

    def is_agent_repo(self) -> bool:
        """Return True when the current root appears to be the agent source tree."""
        package_sentinels = [
            self.root / "src" / "rex_codex" / "__init__.py",
            self.root / "rex_codex" / "__init__.py",
        ]
        if not any(candidate.exists() for candidate in package_sentinels):
            return False
        other_sentinels = [
            self.root / "scripts" / "selftest_loop.sh",
            self.root / "bin" / "fake-codex",
        ]
        return all(item.exists() for item in other_sentinels)


class FileLock:
    """Simple advisory file lock using fcntl."""

    def __init__(self, lock_path: Path):
        self.lock_path = lock_path
        self._fd: Optional[int] = None

    def acquire(self, blocking: bool = False) -> None:
        import fcntl

        fd = os.open(self.lock_path, os.O_RDWR | os.O_CREAT, 0o666)
        flag = fcntl.LOCK_EX
        if not blocking:
            flag |= fcntl.LOCK_NB
        try:
            fcntl.flock(fd, flag)
        except BlockingIOError as exc:  # pragma: no cover - depends on runtime race
            os.close(fd)
            raise RexError(f"Another rex-codex process holds {self.lock_path}") from exc
        self._fd = fd

    def release(self) -> None:
        import fcntl

        if self._fd is None:
            return
        try:
            fcntl.flock(self._fd, fcntl.LOCK_UN)
        finally:
            os.close(self._fd)
            self._fd = None

    def __enter__(self) -> "FileLock":
        self.acquire()
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        self.release()


@contextmanager
def lock_file(path: Path) -> Iterator[None]:
    lock = FileLock(path)
    lock.acquire()
    try:
        yield
    finally:
        lock.release()


def ensure_python(context: RexContext, *, quiet: bool = False) -> None:
    if which("python3") is None:
        raise RexError("python3 not found on PATH")
    if context.venv_dir.exists():
        if not quiet:
            print("[*] Resetting Python virtual environment (.venv)…")
        import shutil

        shutil.rmtree(context.venv_dir)
    else:
        if not quiet:
            print("[*] Creating Python virtual environment (.venv)…")
    run(["python3", "-m", "venv", str(context.venv_dir)])
    pip = context.venv_dir / "bin" / "pip"
    run(
        [str(pip), "install", "--upgrade", "pip"],
        check=True,
        capture_output=quiet,
        text=True,
    )


def activate_venv(context: RexContext) -> Dict[str, str]:
    env = os.environ.copy()
    env["VIRTUAL_ENV"] = str(context.venv_dir)
    bin_path = context.venv_dir / "bin"
    current_path = env.get("PATH", "")
    env["PATH"] = f"{bin_path}{os.pathsep}{current_path}"
    return env


def read_lines(path: Path) -> List[str]:
    if not path.exists():
        return []
    return [line.rstrip("\n") for line in path.read_text(encoding="utf-8").splitlines()]


def write_text(path: Path, text: str) -> None:
    path.write_text(text, encoding="utf-8")


def print_header(title: str) -> None:
    print(f"=== {title} ===")


def prompt(message: str) -> str:
    try:
        return input(message)
    except EOFError:
        return ""


def ask_confirmation(message: str, *, expected: str) -> bool:
    response = prompt(message)
    return response.strip() == expected


def ensure_requirements_installed(
    context: RexContext,
    requirements_template: Path,
    *,
    quiet: bool = True,
) -> None:
    env = activate_venv(context)
    pip = context.venv_dir / "bin" / "pip"
    base_cmd: List[str] = [str(pip), "install"]
    if quiet:
        base_cmd.append("-q")
    if requirements_template.exists():
        run(base_cmd + ["-r", str(requirements_template)], env=env)
    else:
        baseline = [
            "pytest==8.0.2",
            "pytest-xdist==3.5.0",
            "pytest-cov==4.1.0",
            "black==24.4.2",
            "isort==5.13.2",
            "ruff==0.3.2",
            "flake8==7.0.0",
            "mypy==1.8.0",
        ]
        run(base_cmd + baseline, env=env)


def _audit_candidate_paths(root: Path) -> List[Path]:
    patterns = [
        "*.md",
        "AGENTS.md",
        "README.md",
        ".codex_ci_latest.log",
        ".codex_ci/*.log",
        "documents/**/*.md",
        "bin/**/*.py",
        "bin/**/*.sh",
        "scripts/**/*.py",
        "scripts/**/*.sh",
        "rex_codex/**/*.py",
        "src/rex_codex/**/*.py",
    ]
    seen: Set[Path] = set()
    excluded_root = root / "for_external_GPT5_pro_audit"
    for pattern in patterns:
        for path in root.glob(pattern):
            if not path.is_file():
                continue
            if excluded_root in path.parents:
                continue
            seen.add(path.resolve())
    return sorted(seen)


def _is_gitignored(root: Path, path: Path) -> bool:
    try:
        relative = path.relative_to(root)
    except ValueError:
        return False
    try:
        result = run(
            ["git", "check-ignore", "-q", "--", str(relative)],
            cwd=root,
            capture_output=True,
            check=False,
        )
    except FileNotFoundError:
        return False
    return result.returncode == 0


def _render_directory_listing(root: Path) -> str:
    max_depth = 3
    per_dir_limit = 25
    line_budget = 400
    skip_dir_names = {
        "__pycache__",
        ".pytest_cache",
        ".mypy_cache",
        ".ruff_cache",
        ".nox",
        ".idea",
    }
    skip_contents_dirs = {".git"}
    gitignore_cache: Dict[Path, bool] = {}
    lines: List[str] = []
    truncated = False

    def add_line(text: str) -> bool:
        nonlocal line_budget, truncated
        if truncated:
            return False
        if line_budget <= 0:
            lines.append("  ... (directory listing truncated)")
            truncated = True
            return False
        lines.append(text)
        line_budget -= 1
        return True

    def is_gitignored_cached(path: Path) -> bool:
        resolved = path.resolve()
        if resolved in gitignore_cache:
            return gitignore_cache[resolved]
        ignored = _is_gitignored(root, resolved)
        gitignore_cache[resolved] = ignored
        return ignored

    def walk(path: Path, depth: int) -> None:
        nonlocal truncated
        if truncated:
            return
        try:
            entries = sorted(
                path.iterdir(),
                key=lambda item: (not item.is_dir(), item.name.lower()),
            )
        except OSError as exc:
            add_line(f"{'  ' * (depth + 1)}[Error listing {path.name}: {exc}]")
            return

        filtered: List[Path] = []
        for entry in entries:
            if entry.is_dir() and entry.name in skip_dir_names:
                continue
            filtered.append(entry)

        shown = 0
        total_entries = len(filtered)
        for entry in filtered:
            if truncated:
                break
            if shown >= per_dir_limit:
                break
            rel = entry.relative_to(root)
            indent = "  " * (depth + 1)
            if entry.is_dir():
                ignored = is_gitignored_cached(entry)
                if ignored:
                    add_line(
                        f"{indent}{rel.as_posix()}/ (gitignored; contents omitted)"
                    )
                elif entry.name in skip_contents_dirs:
                    add_line(f"{indent}{rel.as_posix()}/ (contents omitted)")
                elif depth + 1 >= max_depth:
                    add_line(f"{indent}{rel.as_posix()}/ (depth limit)")
                else:
                    add_line(f"{indent}{rel.as_posix()}/")
                    walk(entry, depth + 1)
                shown += 1
            else:
                add_line(f"{indent}{rel.as_posix()}")
                shown += 1

        remaining = total_entries - shown
        if remaining > 0 and not truncated:
            indent = "  " * (depth + 1)
            add_line(f"{indent}... ({remaining} more entries)")

    add_line("./")
    walk(root, 0)
    return "\n".join(lines)


def _write_audit_file(audit_path: Path, root: Path, files: List[Path]) -> None:
    with audit_path.open("w", encoding="utf-8") as fh:
        fh.write("# External GPT5-Pro Audit Snapshot\n")
        fh.write(f"Generated at {datetime.now(UTC).isoformat()}\n\n")
        fh.write("## Repository Layout\n")
        fh.write(_render_directory_listing(root))
        fh.write("\n\n")
        fh.write("## File Snapshots\n\n")
        for file_path in files:
            resolved = file_path.as_posix()
            fh.write(f"=== {resolved} ===\n")
            try:
                contents = file_path.read_text(encoding="utf-8", errors="replace")
            except OSError as exc:  # pragma: no cover - filesystem errors
                fh.write(f"[Error reading file: {exc}]\n\n")
                continue
            fh.write(contents)
            if not contents.endswith("\n"):
                fh.write("\n")
            fh.write("\n")


def _env_flag(name: str) -> bool:
    value = os.environ.get(name, "")
    return value.strip().lower() in {"1", "true", "yes", "on"}


def _auto_commit_and_push(root: Path, audit_path: Path) -> None:
    run(["git", "add", "-A"], cwd=root, check=False)
    status = run(
        ["git", "status", "--porcelain"],
        cwd=root,
        capture_output=True,
        check=False,
    )
    if not (status.stdout or "").strip():
        print("[audit] No changes detected; skipping commit.")
        return
    message = f"chore: external audit snapshot {audit_path.name}"
    commit = run(
        ["git", "commit", "-m", message],
        cwd=root,
        capture_output=True,
        check=False,
    )
    if commit.returncode != 0:
        print(f"[audit] git commit failed: {commit.stderr or commit.stdout}")
        return
    if _env_flag("REX_DISABLE_AUTO_PUSH"):
        print("[audit] Skipping git push (REX_DISABLE_AUTO_PUSH is set).")
        return
    push = run(["git", "push"], cwd=root, capture_output=True, check=False)
    if push.returncode != 0:
        print(f"[audit] git push failed: {push.stderr or push.stdout}")


def create_audit_snapshot(
    context: RexContext,
    *,
    auto_commit: bool = True,
    extra_sections: Optional[List[tuple[str, Sequence[str]]]] = None,
) -> Path:
    root = context.root
    audit_dir = ensure_dir(root / "for_external_GPT5_pro_audit")
    timestamp = datetime.now(UTC).strftime("%Y%m%d%H%M%S")
    audit_path = audit_dir / f"audit_{timestamp}.md"
    files = _audit_candidate_paths(root)
    if not files:
        print("[audit] No candidate files found for snapshot.")
        return audit_path
    _write_audit_file(audit_path, root, files)
    if extra_sections:
        with audit_path.open("a", encoding="utf-8") as fh:
            for title, lines in extra_sections:
                fh.write(f"\n## {title}\n\n")
                for line in lines:
                    fh.write(f"- {line}\n")
    print(f"[audit] Snapshot written to {audit_path}")
    if context.is_agent_repo() and not _env_flag("REX_AGENT_FORCE_BUILD"):
        if auto_commit:
            print(
                "[audit] Detected rex-codex source tree; defaulting to testing mode (auto commit disabled)."
            )
        auto_commit = False
    if _env_flag("REX_DISABLE_AUTO_COMMIT"):
        auto_commit = False
    if auto_commit:
        _auto_commit_and_push(root, audit_path)
    return audit_path

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_sandbox/__init__.py ===
"""Expose the sandbox scope helpers."""

from __future__ import annotations

from .selftest import run_selftest, run_smoke, selftest_script, smoke_script  # noqa: F401

__all__ = ["run_selftest", "run_smoke", "selftest_script", "smoke_script"]

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/scope_sandbox/selftest.py ===
"""Python helpers for invoking the self-test sandbox loops."""

from __future__ import annotations

import os
import subprocess
from pathlib import Path
from typing import Mapping, MutableMapping


def _repo_root() -> Path:
    return Path(__file__).resolve().parents[3]


def selftest_script() -> Path:
    script = _repo_root() / "scripts" / "selftest_loop.sh"
    if not script.exists():
        raise FileNotFoundError(f"Selftest script missing at {script}")
    return script


def smoke_script() -> Path:
    script = _repo_root() / "scripts" / "smoke_e2e.sh"
    if not script.exists():
        raise FileNotFoundError(f"Smoke script missing at {script}")
    return script


def run_selftest(*, keep_workspace: bool = False, extra_env: Mapping[str, str] | None = None) -> subprocess.CompletedProcess:
    env: MutableMapping[str, str] = os.environ.copy()
    if keep_workspace:
        env["SELFTEST_KEEP"] = "1"
    if extra_env:
        env.update(extra_env)
    return subprocess.run(["bash", str(selftest_script())], env=env, check=False)


def run_smoke(*, keep_workspace: bool = False, extra_env: Mapping[str, str] | None = None) -> subprocess.CompletedProcess:
    env: MutableMapping[str, str] = os.environ.copy()
    if keep_workspace:
        env["KEEP"] = "1"
    if extra_env:
        env.update(extra_env)
    return subprocess.run(["bash", str(smoke_script())], env=env, check=False)

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/self_update.py ===
"""Compatibility shim exposing global self-update helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_global.self_update", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/status.py ===
"""Compatibility shim for project runtime status helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.status", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/uninstall.py ===
"""Compatibility shim exposing global uninstall helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_global.uninstall", globals())

=== /media/skynet3/8tb_a1/rex_codex_agent/src/rex_codex/utils.py ===
"""Compatibility shim for project runtime utility helpers."""

from __future__ import annotations

from ._compat import reexport

reexport("rex_codex.scope_project.utils", globals())

